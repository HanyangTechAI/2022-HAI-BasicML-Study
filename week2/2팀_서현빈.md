# Ch3 [회귀 알고리즘과 모델규제] 스터디

###3-1. K-최근접 이웃 회귀
* K-최근접 이웃 회귀

 회귀 : 분류처럼 샘플을 클래스 중 하나로 분류하는 것이 아닌, 임의의 어떤 숫자를 예측하는 문제
> - 예측하려는 샘플에 가장 가까운 샘플 k개를 선택하여, 수치의 평균을 구한다. 
> - 넘파이 배열의 reshape() 메서드를 이용하면 배열의 크기를 편리하게 바꿀 수 있다.

* 결정계수 R^2
> - 회귀를 평가하는 점수
> - R^2 = 1 - (타깃-예측)^2의 합/(타깃-평균)^2의 합
> - 타깃의 평균 정도를 예측하는 수준이면 0에 가까워지고, 예측이 타깃에 아주 가까워지면 1에 가까운 값이 된다.
> - 이웃의 개수 k를 줄이면 모델을 더 복잡하게 만들 수 있다.

* 과대적합과 과소적합
> - 과대적합 : 훈련 세트에서 점수가 좋지만 테스트 세트에서 점수가 나쁜 경우. 훈련 세트에만 잘 맞는 모델.
> - 과소적합 : 훈련 세트보다 테스트 세트의 점수가 높거나 두 점수 모두 너무 낮은 경우. 훈련 세트에 적절히 훈련되지 않은 모델.

###3-2. 선형 회귀
K-최근접 이웃 회귀는 새로운 샘플이 훈련 세트의 범위를 벗어나면, 엉뚱한 값을 예측할 수 있다.
* 선형 회귀 : 특성이 하나인 경우 어떤 직선을 학습하는 알고리즘이다.
> - LinearRegression 클래스를 사용한다.
> - 직선의 방정식에 대해 기울기와 절편은 coed_와 intercept 속성에 저장되어 있다.
* 다항 회귀 : 다항식을 사용한 선형 회귀이다. 그래프를 그리면 곡선의 형태로 나타난다.

###3-3. 특성 공학과 규제
* 특성 공학 : 기존의 특성을 사용해 새로운 특성을 뽑아내는 작업 
* 데이터 준비 

 판다스 :  데이터프레임이라는 데이터 구조를 이용한 데이터 분석 라이브러리.
* 사이킷런의 변환기
> - 변환기 : 특성을 만들거나 전처리기 위한 클래스 ex) fir(), transform()

* 규제 

 사용하는 특성의 개수를 늘리면 훈련 세트에 대해 완벽하게 학습할 수 있지만, 과대적합되어 테스트 세트에서 좋은 점수를 얻지 못한다.
 
 머신러닝 모델이 훈련 세트를 과도하게 학습하지 못하도록 방해하는 것을 규제라고 하며, 선형 회귀 모델에 규제를 추가한 것으로 릿지와 라쏘가 있다.

* 릿지, 라쏘 회귀
> - alpha : 규제의 강도를 조절하는 매개변수. 값이 클수록 규제 강도가 커진다.
> - 적절한 alpha 값은, R^2의 그래프를 그려가며 훈련 세트와 테스트 세트의 점수가 가까운 지점을 찾는 것으로 정한다.
