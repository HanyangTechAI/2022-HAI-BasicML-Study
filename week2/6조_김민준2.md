#Ch. 3

+ 회귀
    주어진 샘플을 몇 개의 범주 중 하나로 판단하는 분류와는 다르게, 회귀는 특정 값을 숫자로써 예측하는 방법이다.

+ K-최근접 이웃 분류
  1. 값을 구하려는 데이터와 가장 가까운 샘플 K개를 선택한다. 여기서 숫자 K는 반드시 홀수로 설정해야 한다.
  2. 선택된 K개의 샘플 중 다수를 차지하는 클래스를 데이터의 범주로 예측한다.

+ K-최근접 이웃 회귀
  1. 값을 구하려는 데이터와 가장 가까운 샘플 K개를 선택한다. 단, 분류와 다르게 클래스가 아니라 수치를 이용한다.
  2. 이웃한 샘플들의 평균값을 도출하여 데이터의 타깃값을 구한다.

+ 결정계수
  R^2이라고도 불리는 결정계수는 각 샘플들의 타깃값과 예측값의 차이를 제곱하여 더한 후, 타깃과 평균의 차이를 제곱한 값들의 총합으로 나누어 도출되는 값이다. 예측값이 타깃값과 가까울수록 결정계수는 1에 가까워진다.

+ 과대적합 (overfitting)
  훈련 세트에서만 점수가 높고 테스트 세트에서는 점수가 낮은 경우, 즉 훈련 세트에 대해서만 과도하게 학습된 경우를 뜻한다.

+ 과소적합 (underfitting)
  훈련 세트보다 테스트 세트의 점수가 높은 경우, 혹은 두 점수 모두 낮은 경우, 즉 모델이 너무 단순하게 학습되어 훈련 세트에서조차 잘 학습되지 않은 경우를 뜻한다.

+ K-최근접 이웃 알고리즘의 한계
  K-최근접 이웃 회귀는 가장 가까운 샘플들의 평균을 값으로 도출하는데, 새로 입력된 샘플이 훈련 데이터의 범위를 벗어나게 된다면 적절치 않은 값을 도출할 수 있다.

+ 선형 회귀 (linear regression)
  전체 데이터의 특성을 가장 잘 나타낼 수 있는 하나의 직선, 혹은 선형적인 그래프를 표현하는 알고리즘이다.
  