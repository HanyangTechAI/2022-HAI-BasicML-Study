## 회귀 알고리즘

### 1\. k - 최근접 이웃 회귀

#### 회귀란?

특정 클래스로 예측하는 분류와는 다르게, 임의의 수치를 예측하는 문제이다. 

#### k-최근접 이웃 회귀란?

knn 알고리즘을 사용한 회귀방법으로 가장 가까운 이웃 샘플을 찾고 이 샘플들의 타깃값을 평균으로 예측한다.

#### Data

```
# 농어 길이
perch_length = np.array(
    [8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 
     21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 
     22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 
     27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 
     36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 
     40.0, 42.0, 43.0, 43.0, 43.5, 44.0]
     )
# 농어 무게
perch_weight = np.array(
    [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 
     110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 
     130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 
     197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 
     514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 
     820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 
     1000.0, 1000.0]
     )
```

```
plt.scatter(perch_length, perch_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

---

#### 데이터 시각화

---

#### 1차원 배열 -> 2차원 배열

numpy의 reshpae() 메서드를 사용하여 데이터를 원하는 배열의 크기로 변경가능하다.

```
# 1차원 배열 -> 2차원 배열
train_input = train_input.reshape(-1,1)
test_input = test_input.reshape(-1,1)
```

---

#### 결정 계수

분류에서는 정확도를 평가할 수 있지만 회귀에서는 수치로 반환하기 때문에 정확도를 구하기 어렵다. 그래서 회귀는 다른 값으로 평가하는데 이 점수를 결정계수라 한다. 결정계수는 다음과 같다.

> R^2 = 1 - (타깃 - 예측)^2의 합 / (타깃 - 평균)^2의 합

만약 타깃의 평균 정도를 예측하는 수준이면  R^2는 0에 가까워지고 예측이 타킷에 아주 가까워지면 1에 가까운 값이 된다.

```
from sklearn.neighbors import KNeighborsRegressor
# k 최근접 이웃 회귀 모델
knr = KNeighborsRegressor()
# 훈련 - train_input 값은 2차원 배열만 가능
knr.fit(train_input, train_label)
knr.score(test_input, test_label)
```

---

#### MAE

사이킷런에서 제공하는 mae (타깃과 예측값의 절대값 오차 평균)을 통해 예측값이 평균적으로 얼마 차이나는지 확인해보자.

```
from sklearn.metrics import mean_absolute_error
# 예측
test_prediction = knr.predict(test_input)
# mae 계산
mae = mean_absolute_error(test_label, test_prediction)
print(mae)
```

대략 19 정도 평균적으로 차이나는 것을 확인할 수 있다.

---

#### 과대적합 / 과소적합

과대적합 : 훈련 세트에서는 점수가 굉장히 좋지만, 테스트 세트와 실전에서는 예측이 제대로 작동하지 않는 경우

과소적합 : 모델이 너무 단순하여 훈련 세트에 적절히 훈련되지 않는 경우

#### 과대적합 / 과소적합 테스트

```
knr = KNeighborsRegressor()
# 5~45
x = np.arange(5, 45).reshape(-1, 1)
for n in [1, 5, 10]:
  knr.n_neighbors = n
  knr.fit(train_input, train_label)
  prediction = knr.predict(x)
  plt.scatter(train_input, train_label)
  plt.plot(x, prediction)
  plt.title('n_neighbors ={}'.format(n))
  plt.xlabel('length')
  plt.ylabel('weight')
  plt.show()
```

---

### 2\. 선형 회귀

#### k - 최근접 이웃 한계

예측하는 데이터가 훈련 세트의 범위를 벗어나면 엉뚱한 값을 예측한다.

```
print(knr.predict([[50]]))
"""
[[912.5]]
"""
```

이러한 문제점을 해결하기 위해 선형 회귀를 이용한다.

---

#### 선형 회귀란?

특성과 타깃 사이의 관계를 가장 잘 나타내는 선형 방정식을 찾는 방법이다. 선형 회귀가 찾은 특성과 타깃 사이의 관계는 선형 방정식의 계수 또는 가중치(기울기, 절편)에 저장된다. 

---

#### 예시

```
from sklearn.linear_model import LinearRegression
# 선형 회귀
lr = LinearRegression()
lr.fit(train_input, train_label)

# 시각화
plt.scatter(train_input, train_label)
# 15~50 1차 방정식 그래프 그리기
plt.plot([15, 50], [15*lr.coef_[0]+lr.intercept_[0], 50*lr.coef_[0]+lr.intercept_[0]])
plt.scatter(50, 1241.8, marker='D')
plt.show()
```


```
print(lr.score(train_input, train_label))
print(lr.score(test_input, test_label))
```



---

### 3\. 다항 회귀

#### 다항 회귀란?

다항 회귀는 다항식을 사용하여 특성과 타깃 사이의 관계를 나타내는 방법이다. 이 함수는 비선형, 선형 두 가지 모두 가능하다.

---

#### 예시

```
train_poly = np.column_stack((train_input ** 2, train_input))
test_poly = np.column_stack((test_input ** 2, test_input))
lr = LinearRegression()
lr.fit(train_poly, train_label)

# 시각화
# 15 ~ 49
x = np.arange(15,50)
plt.scatter(train_input, train_label)
plt.plot(x, 1.01*x**2 - 21.6*x + 116.05)
plt.scatter([50],[1574], marker='^')
plt.show()
```


---
## 특성 공학과 규제
​
### 1\. 다중 회귀
​
#### 다중 회귀란?
​
여러 개의 특성을 사용한 선형 회귀를 다중 회귀라 한다.
​
#### 특성 공학이란?
​
주어진 특성을 조합하여 새로운 특성을 만드는 일련의 작업 과정이다.
​
---
​
#### Pandas 로 데이터 불러오기
​
```
import pandas as pd
df = pd.read_csv('https://bit.ly/perch_csv_data')
perch_full = df.to_numpy()
```
​
pandas 로 csv 데이터를 불러온 후 to\_numpy() 메서드를 통해서 배열로 변환한다.
​
```
perch_weight = np.array(
    [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 
     110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 
     130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 
     197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 
     514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 
     820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 
     1000.0, 1000.0]
     )
```
​
```
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state=42)
```
​
---
​
### 2\. 사이킷런 변환기
​
사이킷런은 특성을 만들거나 전처리하기 위한 다양한 클레스를 제공하는데 이를 변환기라 한다.
​
#### PolynomialFeatures()
​
```
from sklearn.preprocessing import PolynomialFeatures
# poly 
poly = PolynomialFeatures()
poly.fit([[2, 3]])
print(poly.transform([[2, 3]])) 
​
# include_bias(절편) 포함 x
poly = PolynomialFeatures(include_bias=False)
​
poly.fit(train_input)
train_poly = poly.transform(train_input)
```
​
#### 특성 확인
​
```
poly.get_feature_names_out()
```
여기서는 총 9가지의 특성이 있다.
​
---
​
### 3\. 다중 회귀 모델 훈련하기
​
```
from sklearn.linear_model import LinearRegression
​
lr = LinearRegression()
lr.fit(train_poly, train_target)
#degree = 5 -> 5제곱 까지 특성을 만듦
poly = PolynomialFeatures(degree=5, include_bias=False)
​
poly.fit(train_input)
train_poly = poly.transform(train_input)
test_poly = poly.transform(test_input)
lr.fit(train_poly, train_target)
print(lr.score(train_poly, train_target))
```

---
​
### 4\. 규제
​
규제는 머신러닝 모델이 훈련 세트를 너무 과도하게 학습하지 못하도록 훼방하는 것을 말한다. 즉, 과대적합되지 않도록 만드는 작업이다. 선형 회귀 모델에서의 규제는 특성에 곱해지는 계수를 작게 만드는 것이다.
​
```
# 사이킷런의 변환기 중 하나
from sklearn.preprocessing import StandardScaler
​
ss = StandardScaler()
ss.fit(train_poly)
​
train_scaled = ss.transform(train_poly)
test_scaled = ss.transform(test_poly)
```
​
---
​
#### 릿지란?
​
규제가 있는 선형 회귀 모델 중 하나이며 선형 모델의 계수를 제곱한 값을 기준으로 규제를 한다.
​
```
from sklearn.linear_model import Ridge
​
ridge = Ridge()
ridge.fit(train_scaled, train_target)
print(ridge.score(train_scaled, train_target))
```
​
#### 최적의 alpha 값 찾기
​
```
import matplotlib.pyplot as plt
​
train_score = []
​
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list:
    # 릿지 모델을 만듭니다
    ridge = Ridge(alpha=alpha)
    # 릿지 모델을 훈련합니다
    ridge.fit(train_scaled, train_target)
    # 훈련 점수와 테스트 점수를 저장합니다
    train_score.append(ridge.score(train_scaled, train_target))
    test_score.append(ridge.score(test_scaled, test_target))
test_score = []
​
# 시각화
plt.plot(np.log10(alpha_list), train_score)
plt.plot(np.log10(alpha_list), test_score)
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.show()
```

---
​
#### 라쏘란?
​
또 다른 규제가 있는 선형 모델 회귀모델이다. 계수의 절댓값을 기준으로 규제를 한다. 릿지와 달리 계수 값을 아예 0으로 만들 수 있다.
​
```
from sklearn.linear_model import Lasso
​
lasso = Lasso()
lasso.fit(train_scaled, train_target)
​
train_score = []
test_score = []
​
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list:
    # 라쏘 모델을 만듭니다
    lasso = Lasso(alpha=alpha, max_iter=10000)
    # 라쏘 모델을 훈련합니다
    lasso.fit(train_scaled, train_target)
    # 훈련 점수와 테스트 점수를 저장합니다
    train_score.append(lasso.score(train_scaled, train_target))
    test_score.append(lasso.score(test_scaled, test_target))
​
# 시각화
plt.plot(np.log10(alpha_list), train_score)
plt.plot(np.log10(alpha_list), test_score)
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.show()