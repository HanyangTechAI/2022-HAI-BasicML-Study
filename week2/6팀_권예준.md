# 로지스틱 회귀
## -> 이름에 회귀가 들어가지만 회귀가 아님!(확률을 찾는 알고리즘)

## 분류 알고리즘에는 무엇이 있을까?
---
> ## K-NeighborsClassifier
* ### 최근접 이웃 몇개를 뽑아서, 각각의 클래스가 차지하는 비율이 곧 확률
* ### 구현방법
  ### 1. 데이터 불러오기 (pd.read_csv())
  ### 2. 넘파이로 변환하기 (.to_numpy())
  ### 3. 데이터 분리하기 (train_test_split())
  ### 4. 데이터 표준화 (StandardScaler())
  ### 5. 모델 만들기 (KNeighborsClassifier())

> ## 로지스틱 회귀(Logistic Classifier)
![](https://mblogthumb-phinf.pstatic.net/MjAyMDA3MDdfMTgx/MDAxNTk0MTMwODg2NzAw.Bgt42rm3pV0xTPfuVjN1UbXw9HchDcAdLdvnsrAQvJ0g.ILAv2yJkoMXNiWHKAUe0QswJWyr84GwwlRbXwxCogKUg.PNG.zzoyou_/sigmoid.png?type=w800)
* ### 치역이 0(0%)과 1(100%) 사이
* ### 이중분류 (True/False)에서 사용
* ### 0.5보다 크면 양성 클래스, 0.5보다 작으면 음성 클래스  
# 
* ### 구현방법
    ### 1. 데이터 가공 (pd.read_csv(), .to_numpy())
    ### 2. 불리언 인덱싱 (array['True'/'False'])
    ### 3. 모델 만들기 (LogisticRegression())
    ### 4. 모델 학습하기 (lr.fit())
    ### 5. 모델 평가하기 (lr.predict(), predict_proba())

---
# 소프트맥스 함수
![](https://lh6.googleusercontent.com/3vcfJ5hJhsMZAMFIbQOEycfVW1t6rh1CXt62DeMk8RPPXVzV4vCcURNm_z_F7618uAeSHT7qT7wE_UiK5Ic0b-Eeuunn6iTGeHWbpAaUAP6-G2ePubeGWCb4_TmSapeaimZqvuUs)
* ### 모든 i에 대한 출력을 더하면 1(100%)
* ### 다중 분류에서 사용

---
# 경사 하강법
## 확률적 경사 하강법(Stochastic Gradient Descent, SGD)
  * ### 훈련 세트에서 랜덤하게 샘플 하나를 골라 학습
  * ### 이 과정을 훈련 세트의 수 만큼 사용하는 과정->에포크
## 미니배치 경사 하강법(Mini-Batch Stochastic Gradient Descent, MSGD)
  * ### 전체 데이터를 batch_size 개씩 나누어 학습
## 배치 경사 하강법 (Batch Gradient Descent, BGD)
  * ### 전체 데이터를 하나의 배치로 묶어 학습

---
# 손실 함수
## 로지스틱 손실 함수
  * ### 양성 클래스인 경우: -log(p)
  * ### 음성 클래스인 경우: -log(1 - p)
  * ### 이진 분류에서는 '이진' 크로스엔트로피 손실 함수
  * ### 다중 분류에서는 크로스엔트로피 손실 함수
## 평균 절댓값 오차
  * ### 타깃에서 예측을 뺀 절댓값을 모든 샘플에 평균한 값
## 평균 제곱 오차
  * ### 타깃에서 예측을 뺸 값을 제곱한 다음 모든 샘플에 평균한 값