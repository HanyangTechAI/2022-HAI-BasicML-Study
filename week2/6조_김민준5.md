#Ch. 3

+ 회귀
    주어진 샘플을 몇 개의 범주 중 하나로 판단하는 분류와는 다르게, 회귀는 특정 값을 숫자로써 예측하는 방법이다.

+ K-최근접 이웃 분류
  1. 값을 구하려는 데이터와 가장 가까운 샘플 K개를 선택한다. 여기서 숫자 K는 반드시 홀수로 설정해야 한다.
  2. 선택된 K개의 샘플 중 다수를 차지하는 클래스를 데이터의 범주로 예측한다.

+ K-최근접 이웃 회귀
  1. 값을 구하려는 데이터와 가장 가까운 샘플 K개를 선택한다. 단, 분류와 다르게 클래스가 아니라 수치를 이용한다.
  2. 이웃한 샘플들의 평균값을 도출하여 데이터의 타깃값을 구한다.

+ 결정계수
  R^2이라고도 불리는 결정계수는 각 샘플들의 타깃값과 예측값의 차이를 제곱하여 더한 후, 타깃과 평균의 차이를 제곱한 값들의 총합으로 나누어 도출되는 값이다. 예측값이 타깃값과 가까울수록 결정계수는 1에 가까워진다.

+ 과대적합 (overfitting)
  훈련 세트에서만 점수가 높고 테스트 세트에서는 점수가 낮은 경우, 즉 훈련 세트에 대해서만 과도하게 학습된 경우를 뜻한다.

+ 과소적합 (underfitting)
  훈련 세트보다 테스트 세트의 점수가 높은 경우, 혹은 두 점수 모두 낮은 경우, 즉 모델이 너무 단순하게 학습되어 훈련 세트에서조차 잘 학습되지 않은 경우를 뜻한다.

+ K-최근접 이웃 알고리즘의 한계
  K-최근접 이웃 회귀는 가장 가까운 샘플들의 평균을 값으로 도출하는데, 새로 입력된 샘플이 훈련 데이터의 범위를 벗어나게 된다면 적절치 않은 값을 도출할 수 있다.

+ 선형 회귀 (linear regression)
  전체 데이터의 특성을 가장 잘 나타낼 수 있는 하나의 직선, 혹은 선형적인 그래프를 표현하는 알고리즘이다.
  
+ 다항 회귀
  다항식을 활용한 선형회귀. 차원이 늘어날수록 그래프(함수)의 계수 또한 늘어난다.

+ 다중 회귀 (multiple regression)
  여러 개의 특성을 이용한 선형 회귀 알고리즘. 
  - 특성 공학
    기존의 feature값에서 새로운 특성을 생성하는 작업. 기존의 feature 값 간의 곱이나 차이를 주로 사용한다.
  - 규제
    조정 학습과 비슷한 방법. 훈련 세트에 대해 너무 과도하게 학습되는 것을 방지한다.
    - 릿지 회귀(Ridge Regression)
      릿지 회귀식: 잔자제곱의 합 + 패널티 항의 합
      제약 조건에 가장 가까이 접근하는 RSS 값을 선택
    - 라쏘 회귀
      기존의 선형회귀에서 추가적인 제약 조건을 부여한 알고리즘. MSE가 최소가 되도록 하는 가중치 및 편향을 탐색하고, 가중치들의 크기가 최소가 되도록 함.
      * MSE (평균 제곱 오차)
        데이터와 모델의 함수가 얼마나 평균적으로 떨어져 있는지 보여주는 지표.