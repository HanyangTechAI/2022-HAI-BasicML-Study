# 트리 알고리즘

## 결정트리
로지스틱 회귀 모델 같은 머신러닝 모델은 왜 그런 결과가 나왔는지 설명하기 어려운 암상자와 같은 모델이다. 반면 결정 트리 모델은 다른 모델에 비해 비교적 이유를 설명하기 쉽다는 장점이 있다.

결정트리 모델은 스무고개와 유사하게 데이터를 이용해 가지치기를 하는 방식으로 데이터를 분류한다. 사이킷런에선 `DecisionTreeClassifier`에 정의되어있다.

```py
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
```

또한 사이킷런에선 트리의 내용과 구조를 살펴볼 수 있는 모듈을 지원한다. 이를 통해 테스트 조건, 불순도, 총 샘플 수, 클래스별 샘플 수를 확인할 수 있다.

- **지니 불순도**: 1 - (음성 클래스 비율 ^ 2)
- **정보 이득**: 부모와 자식 노드 사이의 불순도 차이

결정트리는 불순도 기준을 통해 정보 이득이 최대가 되도록 노드를 분할하고, 노드를 순수하게 나눌수록 정보 이득이 커진다. 또한 새로운 샘플에 대해 예측할 때 노드의 질문에 따라 트리를 이동시키고, 마지막에 도달한 노드의 클래스의 비율을 보고 예측을 만든다.

결정트리 알고리즘에도 가지치기가 필요하고, 최대 깊이를 지정함으로써 모델의 과대적합을 예방할 수 있다.

## 교차 검증과 그리드 서치

모델의 과대적합을 막고, 일반화 성능을 높이기 위해 우리는 **검증 세트**를 만들어 테스트하곤 한다. 일반적으로 훈련세트의 20%를 떼어내어 검증 세트로 만든다. `train_test_split()` 함수를 2번 적용해주면 쉽게 나눠줄 수 있다.

또한 교차 검증을 통해 안정적인 검증 점수를 얻고 더 많은 데이터를 훈련에 적용할 수 있는데, 훈련세트를 n개로 나누어 하나씩 검증 세트로 이용해가며 모델을 검증하는 것이다.

**그리드 서치**는 모델의 하이퍼파라미터 탐색과 교차 검증을 한 번에 수행해주는 도구로, 사이킷런의 `GridSearchCV`에 정의되어있다. 탐색할 매개변수를 나열하면 교차 검증을 수행해 가장 좋은 검증 점수의 매개변수 조합을 선택하고, 마지막으로 그 매개변수 조합으로 최종 모델을 훈련한다.

**랜덤 서치**는 연속된 매개변수 값을 탐색할 때 유용한 도구로, 매개변수를 샘플링할 수 있는 확률 분포 객체를 전달하여 수행할 수 있다.

## 트리의 앙상블

**앙상블 학습**은 더 좋은 예측 결과를 만들기 위해 여러 개의 모델을 훈련하는 알고리즘이다.

- **랜덤 포레스트**: 결정 트리 기반의 앙상블 학습 방법으로, 부트스트랩 샘플을 사용하고 랜덤하게 일부 특성을 선택하여 트리를 만드는 것이 특징이다.

- **엑스트라 트리**: 랜덤 포레스트와 유사하나 부트스트랩 샘플을 사용하지 않고, 노드를 무작위로 분할하여 과대적합을 줄인다.

- **그레이디언트 부스팅**: 결정 트리를 연속적으로 추가하여 손실함수를 최소화하는 방법으로, 훈련 속도는 느리지만 좋은 성능을 내며, 속도를 개선한 **히스토그램 기반 그레이디언트 부스팅**도 있다.