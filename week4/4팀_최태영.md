import pandas as pd

wine = pd.read_csv('https://bit.ly/wine_csv_data')
wine.head()#head()는 처음 5개 뎅터를 보여줌.() 안에 넣은 수 만큼 볼 수도 있. -1 넣으면 전체.
wine.info()#데이터의 누락 확인하기 좋음 각 특징의 숫자 출력
wine.describe()# 데이터의 특징 출력. 언제 쓰지? 실전에서는 numpy면 될 듯??
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size=0.2, random_state=42)#데이터 분류 작업
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)

train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)#데이터 가공
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_scaled, train_target)

print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))
print(lr.coef_, lr.intercept_)#로지스틱 회귀/ 


from sklearn.tree import DecisionTreeClassifier#보고서를 만들기 위해 쓴다고 하지만 뒤에 나오는 앙상블이 최종 목표임

dt = DecisionTreeClassifier(random_state=42) #근데 random이 왜 필요한거임. 정보 이득이 최대가 되는 값은 하나일텐데 그냥 찾으면 안되는 건가?
dt.fit(train_scaled, train_target)

print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))

import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

plt.figure(figsize=(10,7))
plot_tree(dt)
plt.show()

plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])#filled=True유용할 듯.plot은 그냥 그림 그리는 라이브러리임 그래서 특징이 순서대로 뭔지 알려줌.
plt.show()

dt = DecisionTreeClassifier(max_depth=3, random_state=42)#애초에 tree깊이를 3까지만 학습하도록 정함
dt.fit(train_scaled, train_target)

print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))

plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()



dt = DecisionTreeClassifier(min_impurity_decrease=0.0005, random_state=42)#정보이득의 최솟값 정의, 사용자가 지정해야하는 파라미터임. 근데 이걸 또 기계한테 시킬 수도 있음.

dt.fit(train_input, train_target)

print(dt.score(train_input, train_target))
print(dt.score(test_input, test_target))

plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
#ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ

import pandas as pd

wine = pd.read_csv('https://bit.ly/wine_csv_data')
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size=0.2, random_state=42)

sub_input, val_input, sub_target, val_target = train_test_split(
    train_input, train_target, test_size=0.2, random_state=42)#기존에는 학습 방식(하이퍼 파라미터)를 개발자가 지정하고 그 안에서 최고의 계수를 찾는 형식이었음. 
                                                                          #근데 하이퍼 파라미터 조차도 기계로 최고를 찾으면 어떨까? 하는 발상임.하이퍼 파라미터의 값을 조금씩 바꾸며 여러가지 모델을 만들고 
                                                                          #그중 가장 좋은 모델의 하이퍼 파라미터를 이용해 모델을 학습 시킴. 이때 "그중 가장 좋은 모델"을 찾기 위해 검증 데이터 셋을 만듦
print(sub_input.shape, val_input.shape)

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)
dt.fit(sub_input, sub_target)

print(dt.score(sub_input, sub_target))
print(dt.score(val_input, val_target))

from sklearn.model_selection import cross_validate#검증 세트를 쓰느라 훈련 세트가 줄어드는 것을 방지하기 위해 사용
                                                               #말 그대로 (훈련 세트 + 검증 세트)에서 검증 세트를 돌려가면서 쓰는 것임
                                                               #여러번 훈련하여 모델 평가 점수를 평균 낸 후, 다른 모델 들과 비교. 근데 n-폴드 교차 검증이면 n배 훈련이 오래 걸릴 듯

scores = cross_validate(dt, train_input, train_target)
print(scores)
import numpy as np

print(np.mean(scores['test_score']))

from sklearn.model_selection import StratifiedKFold#데이터를 섞음. 만약 train_test_split()을 사용했다면 필요x.

scores = cross_validate(dt, train_input, train_target, cv=StratifiedKFold())
print(np.mean(scores['test_score']))

splitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)#아니 왜 StratifiedKFold에서 n_split매개변수를 지정? cross_validation에서 안하고?
scores = cross_validate(dt, train_input, train_target, cv=splitter)
print(np.mean(scores['test_score']))

from sklearn.model_selection import GridSearchCV #서로 다른 2개의 하이퍼 파라미터의 변화에 따라 최고의 조합을 찾으려면 a*b개의 모델을 훈련
                                                                #그리드 서치 사용. 자동으로 cross_val,stratifiedkfold쓰는 듯

params = {'min_impurity_decrease': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}#예시로 한개만 해봄

gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)#n_jobs은 코어 쓰는 수, 여기선 몇 폴든지 어떻게 정의하지?
gs.fit(train_input, train_target)
dt = gs.best_estimator_#여기에 가장 결과가 좋은 하_파 모델이 들어있음
print(dt.score(train_input, train_target))
print(gs.best_params_)
print(gs.cv_results_['mean_test_score'])
best_index = np.argmax(gs.cv_results_['mean_test_score'])
print(gs.cv_results_['params'][best_index])
params = {'min_impurity_decrease': np.arange(0.0001, 0.001, 0.0001),
          'max_depth': range(5, 20, 1),
          'min_samples_split': range(2, 100, 10)
          }#이번엔 여러 개의 파라미터를 자동으로 찾게함. a개에서a*b*c개의 모델을 만들기 때문에 오래 걸림.


gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)
gs.fit(train_input, train_target)
print(gs.best_params_)
print(np.max(gs.cv_results_['mean_test_score']))

from scipy.stats import uniform, randint#이번에는 하이퍼 파라미터를 차근차근 넣는게 아니라 무작위로 골라 넣음. 랜덤 수 만드는 라이브러리
                                                 #근데 하이퍼 파라미터도 가중치 맹키로 조금씩 좋은 걸 찾는게 좋지 않을까? 
                                                 #예를 들어 max_dept가 20일 때 보다, 7일때 결과가 좋았으면 다음엔 8이나 6 정도를 넣어 보는게 합리적이지 않을까.....
rgen = randint(0, 10)
rgen.rvs(10)
np.unique(rgen.rvs(1000), return_counts=True)
ugen = uniform(0, 1)
ugen.rvs(10)
params = {'min_impurity_decrease': uniform(0.0001, 0.001),
          'max_depth': randint(20, 50),
          'min_samples_split': randint(2, 25),
          'min_samples_leaf': randint(1, 25),
          }
from sklearn.model_selection import RandomizedSearchCV

gs = RandomizedSearchCV(DecisionTreeClassifier(random_state=42), params, 
                        n_iter=100, n_jobs=-1, random_state=42)# n_iter은 몇번 랜덤하게 파라미터를 뽑을 것인가 하는것임, 많을 수 록 좋지만 오래 걸릴 듯

gs.fit(train_input, train_target)
print(gs.best_params_)
print(np.max(gs.cv_results_['mean_test_score']))
dt = gs.best_estimator_

print(dt.score(test_input, test_target))

#ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

wine = pd.read_csv('https://bit.ly/wine_csv_data')

data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()

train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)
from sklearn.model_selection import cross_validate


from sklearn.ensemble import RandomForestClassifier
#음...그러니까 이건 굳이 힘들게 여러가지 조합의 파라미터로 모델을 많이 만들었는데 왜 가장 좋은거 하나만 써?
#그냥 다 굴린 다음에 평균 낸 결과를 쓰자!!하는 거로 이해함 .일단 여러가지 모델은 만든다는 것에서부터 당연히 검증 데이터는 필요함
#아마도 여러 모델을 사용하기 때문에 속도는 좀 더 느릴 듯(gpu 사용)
#그리고 이상하게 각각의 모델에서 각각의 노드를 만들 때 마다 루트 특성의 수 만큼 특성을 골라 사용. 왜그러는 지는 모르겠음. 모든 특성을 균등하게 쓰고자 하는 의지인가
#그리고 훈령 데이터를 복원 추출하여 각각의 모델을 훈련시킴. 이것도 왜 그러는지 더 생각해 봐야할 듯
rf = RandomForestClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
rf.fit(train_input, train_target)
print(rf.feature_importances_)
#ㅇㅇ. 더 균등하게 특성을 활용함.근데 이게 과대 적합하고 무슨 상관이지? 
#만약 레드,화이트 와인을 분류하는데 도움이 전혀 안되는 특성이 있으면 (예, 온도) 자동으로 배제되면 좋은거 아닌가...
rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)
#oob_score는 남는 샘플로 훈련한 결정 트리를 평가. 검증 세트의 수가 늘어나는 건가?

rf.fit(train_input, train_target)
print(rf.oob_score_)
from sklearn.ensemble import ExtraTreesClassifier
#전체 데이터 세트로 100개의 모델을 만드는데 노드를 무작위로 분할 함.-->이게 말이 되나...??
et = ExtraTreesClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))

et.fit(train_input, train_target)
print(et.feature_importances_)

from sklearn.ensemble import GradientBoostingClassifier
#경사 하강법을 사용하여 트리를 앙상블에 추가합니다.가 뭔소릴까...
#단순하게 모델들의 평균을 예측으로 사용하는게 아니라 가중치를 곱하여 사용한다는 의미인가?
#맞는 듯. 깊이 3짜리 각각의 모델들을 마치 로지스틱 회귀에서 input과 target처럼 쓰는 듯
gb = GradientBoostingClassifier(random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)
#과대 적합에 아주 강함...중간에 로지스틱(1차항 이용)을 써서 과대 적합되기가 아주 힘든거겠지?
#모델을 하나씩 꼭 추가하는 이유가 뭘까. gpu로 모델 여러개 만들어서 선형 회귀 때리면 되는거 아닌가.
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))

gb.fit(train_input, train_target)
print(gb.feature_importances_)

from sklearn.ensemble import HistGradientBoostingClassifier
#특성을 256개로 나누는게 어떻게 최적의 노드 분할을 매우 빠르게 찾는 것과 연결되는가...?

hgb = HistGradientBoostingClassifier(random_state=42)
scores = cross_validate(hgb, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
from sklearn.inspection import permutation_importance

hgb.fit(train_input, train_target)
result = permutation_importance(hgb, train_input, train_target, n_repeats=10,
                                random_state=42, n_jobs=-1)
print(result.importances_mean)

result = permutation_importance(hgb, test_input, test_target, n_repeats=10,
                                random_state=42, n_jobs=-1)
print(result.importances_mean)

hgb.score(test_input, test_target)