# Ch5 [트리 알고리즘] 스터디

###5-1. 결정 트리
* 로지스틱 회귀로 와인 분류하기
> - 알코올 도수, 당도, PH값으로 레드 와인과 화이트 와인을 분류하는 문제
> - 로지스틱 회귀 모델이 가지는 계수의 이유를 이해하기 어렵다,
* 결정 트리 : 이유를 설명하기 쉬운 모델
>- 사이킷런의 DecisionTreeClassifier 클래스를 사용한다. 
>- plot_tree() 함수를 이용하면 그림으로 출력해준다. (매개변수 : max_depth, filled, feature_names)
> - 하나의 노드당, 테스트 조건, 불순도, 총 샘플 수, 클래스별 샘플 수가 표기되어 있다.
> - 표준화 전처리를 할 필요가 없다.
* 불순도 : 지니 불순도를 의미하며, criterion 매개변수의 기본값은 gini이다.
> - 지니 불순도 = 1- 음성 클래스 비율^2 + 양성 클래스 비율^2
> - 노드에서 데이터를 분할할 기준을 정해준다,
> - 결정 트리 모델은 부모 노드와 자식 노드의 불순도 차이(정보 이득)가 가능한 크도록 트리를 성장시킨다.
> - criterion='entropy'로 지정할 시 엔트로피 불순도를 사용할 수 있으며, 결과의 차이는 크지 않다.
* 가지치기 : max_depth 매개변수를 조정, 트리의 최대 깊이를 지정하여 과대적합을 막는다. 

###5-2. 교차 검증과 그리드 서치
* 검증 세트 : 훈련 세트와 테스트 세트 이외에도 테스트 세트를 하나 더 만들어 사용한다.

 훈련 세트로 모델을 훈련, 검증 세트로 모델을 평가한 후 매개변수를 결정, 그 매개변수로 전체 훈련 데이터에서 모델을 다시 훈련한 후 마지막에 테스트 세트를 활용, 최종 점수를 평가한다.

* 교차 검증 : 검증 세트를 떼어 내어 평가하는 과정을 여러 번 반복하여, 안정적인 검증 점수를 얻을 수 있다.
> - 훈련세트를 몇 부분으로 나누냐에 따라 3-폴드, 5-폴드 10-폴드등으로 나눈다.
> - cross_validate() : 교차 검증 함수
> - cross_validate()는 회귀모델일 경우 기본적으로 KFold, 분류모델일 경우 StratifiedKFold 분할기를 사용한다.

* 하이퍼파라미터 튜닝 : 모델이 학습할 수 없어 사용자가 지정해야 하는 파라미터
> - 검증 세트의 점수나 교차 검증을 통해 매개변수를 바꿔 가며 최적의 모델을 찾는다.
> - 파라미터가 여러 개일 경우 튜닝도 같이 이루어져야 하기 때문에 그리드 서치를 사용한다. (GridSearchCV클래스 사용)
> - 그리드 서치가 끝나면, 검증 점수가 가장 높은 조합으로 자동으로 모델을 다시 훈련해 놓는다.

* 랜덤 서치 : 탐색할 매개변수 값의 목록을 전달하는 것이 아니라, 확률 분포 객체를 전달한다. uniform 과 randint를 이용, 주어진 범위에서 고르게 값을 뽑는다.


###5-3. 트리의 앙상블
* 정형 데이터 : 정해진 규칙에 맞게 구조적으로 이루어진 데이터. 반대되는 데이터를 비정형 데이터라고 부른다.
 
 앙상블 학습은 정형 데이터를 다루는 데 뛰어난 성과를 낸다.

* 랜덤 포레스트 : 앙상블 학습중 하나로 결정 트리를 랜덤하게 만들어 결정 트리의 숲을 구성하고, 각 결정 트리의 예측을 통해 최종 예측을 만든다.
> - 이 때, 트리를 훈련하기 위한 데이터는 데이터를 중복해서 뽑을 수 있게 만든 부트스트랩 샘플을 사용한다.
> - 각 노드를 분할할 때, 특성 중에 일부 특성을 무작위로 골라 최선의 분할을 찾는다.
> - 결정 트리를 훈련한 후, 분류일 때는 트리의 클래스별 확률을 평균, 가장 높은 확률을 가진 클래스를 예측으로 삼고 회귀일 때는 각 트리의 예측을 평균한다.
> - 훈련 세트에 과대적합되는 것을 막고, 검증 세트와 테스트 세트에서 안정적인 성능을 얻을 수 있다.
> - 부트스트랩 샘플에 포함되지 않는 남는 샘플인 OOB샘플을 이용, 검증 세트의 역할을 할 수 있다.

* 엑스트라 트리 : 랜덤 포레스트와 비슷한 앙상블 학습이다.
> - 부트스트랩 샘플을 사용하지 않고, 전체 훈련 세트를 사용한다.
> - 노드를 분할할 때 무작위로 분할한다.

* 그레이던트 부스팅 : 깊이가 얕은 결정 트리를 사용, 이전 트리의 오차를 보완하는 방식의 앙상블이다.
> - 경사 하강법을 사용하여 트리를 앙상블에 추가한다. 분류는 로지스틱 손실 함수, 회귀는 평균 제곱 오차 함수를 사용한다.
> - learning_rate : 학습 속도를 조절한다.
> - subsample : 트리 훈련에 사용할 ㅜㄴ련 세트의 비율을 정하는데, 1보다 작으면 일부를 사용한다.

* 히스토그램 기반 그레디언트 부스팅 : 그래디언트 부스팅의 속도와 성능을 개선한 알고리즘이다.
> - 입력 특성을 256개의 구간으로 나누며, 하나를 뗴어 놓고 누락된 값을 위해 사용한다.
> - max_iter : 부스팅 반복 횟수를 지정한다.
> - permutation_importance() : 특성 중요도를 확인할 수 있다.
