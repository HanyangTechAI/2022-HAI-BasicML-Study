# 🖥️ 혼공머신 스터디 : 3장 요약
#### 스터디 3조 이재흠 (@rethinking21, rethinking21@gmail.com)

***
## 챕터 5 트리 알고리즘 🌳<br><br>

### 05-1. 결정 트리 🍷

###### 결정 트리 (Decision Tree)

**결정 트리 (Decision Tree, 의사결정나무)** 각 노드마다 예/아니오 질문을 반복해가며 데이터를 정리하는 알고리즘입니다.
이 모델은 분류와 회귀가 모두 가능하며 널리 사용되고 있는 모델 중 하나입니다. 또한 결정 트리는 데이터의 전처리(표준화)가 필요하지 않습니다.

<br>![결정트리에 대한 대략적인 설명](.\images\3팀_이재흠\05-1 Decision Tree example1.png)<br>
▲[🖼️ 이미지 출처][1]<br>
여기서 **노드(Node)** 는 질문이나 정답을 담은 상자를 뜻합니다. 
노드 안에 있는 질문에 의해 데이터가 흘러내려가게 되고, 흘러 내려간 데이터가 다른 노드에 도착하는 방법을 반복하며 데이터를 분류합니다.
<br>![한 노드에 표시되는 이미지](.\images\3팀_이재흠\05-1 Decision Tree Node.png)<br>
▲첫번째 줄은 노드의 조건을 의미합니다. 이 조건에 따라 데이터들이 다른 노드로 이동합니다.<br>
두번째 줄은 불순도(여기서는 지니불순도)를 의미합니다.<br>
세번째 줄은 노드를 지나간 샘플의 숫자를 의미합니다.<br>
마지막 줄은 노드를 지나간 데이터의 분포를 보여줍니다.<br>

Scikit-learn 에서는 **DecisionTreeClassifier** 클래스를 통해 결정 트리 분류 모델을 사용할 수 있고,
**plot_tree** 함수를 통해 결정 트리 모델을 시각화할 수 있습니다.
DecisionTreeRegressor 클래스는 결정 트리 회귀 모델입니다.
```python
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_scaled, train_target)

plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```
DecisionTreeClassifier의 매개변수 max_depth는 트리의 최대 깊이를 설정해줍니다. 
plot_tree의 매개변수 max_depth는 표시할 트리의 최대 깊이를 설정해주고, feature_names는 해당 요소의 이름을 표시해줍니다.

이전에 배웠던 모델들은 '머신러낭 모델'을 모르는 사람에게 설명하기 어렵다는 단점이 있었습니다. 
하지만 **결정 트리** 는 그런 사람들에게 모델을 쉽게 설명할 수 있다는 장점을 지니고 있습니다.

###### 불순도 (impurity) 와 정보이득 (Information Gain)

**불순도(impurity)** 는 다양한 개체들이 얼마나 포함되어 있는가를 의미합니다. 불순도를 표시하는 방법에는 여러가지 방법이 있습니다.<br>
<br>![지니 불순도를 계산하는 공식](.\images\3팀_이재흠\05-1 Gini impurity form.png) <br>
**지니 불순도(gini impurity)** 는 1에다가 각 분류 정도를 제곱한 것을 뺴는 방식으로 불순도를 측정합니다.
값이 0이 될수록 순도가 높다는 의미입니다.

<br>![엔트로피 불순도를 계산하는 공식](.\images\3팀_이재흠\05-1 Entropy impurity form.png) <br>
**엔트로피 불순도(Entropy impurity)** 는 제곱이 아닌 로그를 사용하여 곱하는 방식을 사용합니다.

이런 부모와 자식 노드의 불순도의 차이를 **정보 이득 (Information Gain)** 이라고 합니다. 
정보이득이 클수록 더 좋은 모델을 가집니다.

[❕ 불순도와 정보이득에 대한 더 자세한 내용][2]

### 05-2. 교차 검증과 그리드 서치 🔍

###### 검증 세트 (validation set)

우리는 데이터 세트를 훈련 세트와 테스트 세트로 나누는 작을 통해 모델의 과대적합 문제를 방지했습니다.
하지만 이 방법은 모델이 테스트 세트에만 집중된다는 문제를 지니고 있습니다.
그 결과 모델이 오히려 테스트 세트에 과대적합 될 수 있습니다.

<br>![검증세트에 대한 설명 이미지](.\images\3팀_이재흠\05-2 Training set.png)<br>
<br>이를 방지하기 위한 방법중에는 훈련세트 중 일부를 떼어낸 **검증 세트 (validation set)** 를 두어 모델을 평가하는 방법이 있습니다.
<br>훈련 세트에서 모델을 훈련하고, 검증세트에서 모델을 평가하는 방식을 사용합니다. 테스트 세트는 이렇게 만들어진 모델을 마지막에 평가할 떄 사용합니다.

###### 교차 검증 (cross validation)

<br>![교차 검증에 대해 설명하는 이미지 (4-폴드 교차 검증)](.\images\3팀_이재흠\05-2 Cross Validation.jpg)<br>
**교차 검증 (cross validation)** 은 훈련 세트를 몇개로 나누어서 그중 하나를 검증 세트로 두어 모델을 평가하는 방식입니다.
이 방법을 이용하면 더 안정적인 검증 점수를 얻고 훈련에 더 많은 데이터를 사용 할 수 있게 됩니다.
보통 5-폴드 교차 검증 이나 10-폴드 교차 검증을 사용합니다

<br>Scikit-learn에서는 cross_validate으로 모델의 교차 검증을 수행할 수 있습니다.
```python
from sklearn.model_selection import cross_validate
import numpy as np

scores = cross_validate(model, train_input, train_target, cv=5) #cv는 데이터를 몇개 쪼갤건지를 의미합니다(기본값:5)
print(scores)
print(np.mean(scores['test_score']))
```

###### StratifiedKFold

<br>![KFold,StratifiedKFold 방식의 차이](.\images\3팀_이재흠\05-2 Stratified cross validation.png)<br>
▲[🖼️ 이미지 출처][3]<br>
**KFold** 는 데이터 세트를 일정한 간격으로 나누어 사용하는 방식입니다.
가장 간단한 방법이지만, 데이터 세트가 불균형할 경우, 모델 평가가 떨어지는 문제가 발생할 수 있습니다.

<br>**StratifiedKFold** 는 이를 해결하기 위한 분할기로, 레이블의 데이터 분포도에 따라 데이터를 분할합니다.

Scikit-learn에서는 StratifiedKFold를 통해 데이터 세트의 분할을 수행 할 수 있습니다.
```python
from sklearn.model_selection import StratifiedKFold
import numpy as np

splitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
scores = cross_validate(model, train_input, train_target, cv=splitter)
print(np.mean(scores['test_score']))
```
###### 하이퍼파라미터 튜닝

모델이 학습 할 수 없어서 사용자가 직접 지정해줘야만 하는 파라미터를 **하이퍼마라미터** 라고 합니다.
파라미터의 값이 두개일 경우, 한 파라미터에서 최적의 값을 찾아도, 다른 파라미터를 조정할때 달라지는 문제가 생깁니다.

<br>![그리드 서치와 랜덤서치에 대한 설명](.\images\3팀_이재흠\05-2 Grid Search Random Search.png)<br>
▲[🖼️ 이미지 출처][4]<br>
**그리드 서치(Grid Search)** 는 파라미터의 가능한 모든 경우의 수를 다 적용한 다음, 최적의 모델을 찾는 방법입니다.
정말로 모든 경우의 수를 다 고려하기 때문에 시간이 오래 걸리며, 자원이 많이 필요합니다.
<br>Scikit-learn 에서의 GridSearchCV를 이용해 그리드 서치를 사용하실 수 있습니다.
```python
from sklearn.model_selection import GridSearchCV
import numpy as np

params = {'min_impurity_decrease': np.arange(0.0001, 0.001, 0.0001),
          'max_depth': range(5, 20, 1),
          'min_samples_split': range(2, 100, 10)
          } #파리미터를 여러개 넣을 수 있습니다.
gs = GridSearchCV(model, params, n_jobs=-1) #n_jobs는 사용할 CPU코어 개수입니다.
gs.fit(train_input, train_target)
dt = gs.best_estimator_ # 그리드 서치를 통해 나온 결과중 가장 좋은 결과를 가져옵니다.
print(dt.score(train_input, train_target))
best_index = np.argmax(gs.cv_results_['mean_test_score']) # 가장 큰 값의 익덱스를 불러옵니다.
print(gs.cv_results_['params'][best_index])
```
**랜덤 서치(Random Search)** 는 주어잔 파라미터 범위에 파라미터 값을 랜덤으로 주어 최적을 모델을 찾는 방법입니다.
<br>Scikit-learn 에서의 RandomizedSearchCV를 이용해 랜덤 서치를 사용하실 수 있습니다.
```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

params = {'min_impurity_decrease': uniform(0.0001, 0.001),
          'max_depth': randint(20, 50),
          'min_samples_split': randint(2, 25),
          'min_samples_leaf': randint(1, 25),
          } #파라미터의 범위를 넣을 수 있습니다.
gs = RandomizedSearchCV(model, params, n_iter=100, n_jobs=-1, random_state=42)
gs.fit(train_input, train_target)
print(gs.best_params_)
dt = gs.best_estimator_ # 랜덤 서치를 통해 나온 결과중 가장 좋은 결과를 가져옵니다.
print(dt.score(test_input, test_target))
```

[❕ 더 깊게 들어가고 싶다면..(링크)][5]<br>
[❕ 더어어 깊게 들어가고 싶다면..(링크)][6]

### 05-3. 트리의 앙상블 🎄

###### 정형데이터, 비정형데이터

**정형 데이터** 는 정해진 규칙에 맞게 들어간 데이터 중에 수치 만으로 의미 파악이 쉬운 데이터들을 보통 말합니다.
반면, 규칙적이지 않은 데이터는 **비정형 데이터** 라고 합니다.

###### 앙상블 학습 (ensemble learning)
**앙상블 학습(ensemble learning)** 은 여러 개의 분류기를 생성하고 그 분류기들의 예측을 결합시켜 더 정확한 예측을 내놓는 기법입니다.
약한 모델을 여러개 두는 기법을 이용해 모델의 과대적합 문제를 해결할 수 있습니다.

앙상블 학습은 보팅(Voting), 배깅(Bagging), 부스팅(Boosting)으로 유형을 나눌 수 있습니다.<br>
[❕ 앙상블 학습의 세가지 유형][7]

###### 랜덤 포레스트 (Random Forest)

<br>![랜덤포레스트를 설명하는 이미지](.\images\3팀_이재흠\05-3 Random Forest.png)<br>
▲[🖼️ 이미지 출처][8]<br>
**랜덤 포레스트(Random Forest)** 는 훈련을 통해 여러개의 결정 나무들을 생성한 다음, 그 나무들로 부터 예측값을 도출해내는 방법입니다.
회귀의 경우 나무들의 평균값으로 계산하며, 분류의 경우 가장 많이 투표한 예측값으로 결론을 냅니다.
<br>랜덤 포레스트는 샘플을 뽑을 때 샘플을 뽑고 다시 넣는 식으로 중복이 가능하게 샘플을 모읍니다. 
이를 **부트 스트랩 샘플(bootstrap sample)** 이라고 합니다.

<br>Scikit-learn에서는 RandomForestClassifier를 통해 결정 트리 분류 모델을 사용할 수 있습니다.
```python
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)
rf.fit(train_input, train_target)
```

샘플중 부트스트랩 샘플에 포함되지 않고 남는 샘플이 있습니다.
이 샘플을 **OBB(out of bag) 샘플** 이라고 합니다. 
이 샘플을 이용하여 부트스트랩으로 훈련한 결정 트리를 평가할 수 있습니다.

```python
rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)
rf.fit(train_input, train_target)
print(rf.oob_score_)
```

###### 엑스트라 트리 (Extra Trees)

**엑스트라 트리(Extra Trees)** 는 랜덤 포레스트와 달리 부트스트랩 샘플을 사용하지 않습니다.
대신 노드를 분할 할 때 무작위로 분할합니다.
<br>Scikit-learn에서는 RandomForestClassifier를 통해 엑스트라 트리 분류 모델을 사용할 수 있습니다.
```python
from sklearn.ensemble import ExtraTreesClassifier

et = ExtraTreesClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)
et.fit(train_input, train_target)
```

###### 부스팅 (Boost)

![부스팅에 대한 설명 이미지](.\images\3팀_이재흠\05-3 Boosting.png)
**부스팅(Boost)** 은 가중치를 활용하여 분류기를 강화하며 만드는 모델입니다. 
처음 모델이 예측을 하면 그 예측 결과에 따라 데이터에 가중치가 부여되고, 부여된 가중치가 다음 모델에 영향을 줍니다.

**그래디언트 부스팅(Gradient Boosting)** 은 경사하강법을 사용하여 트리를 앙상블에 추가합니다.

```python
from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42) #n_estimators는 결정트리의 개수를 의미합니다.
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)
```

**Histogram-based Gradient Boosting** 은 입력 특성을 256개의 구간 (255개의 구간 + 누락구간 1개) 으로 나누는 방식을 사용합니다.

특성값 중 누락된 값이 있어도 전처리를 안해도 된다는 장점이 있습니다.
이런 장점 때문에, 정형 데이터를 다루는 머신러닝 알고리즘 중에 가장 높은 인기를 가지고 있는 알고리즘입니다.

```python
from sklearn.ensemble import HistGradientBoostingClassifier

hgb = HistGradientBoostingClassifier(random_state=42)
scores = cross_validate(hgb, train_input, train_target, return_train_score=True, n_jobs=-1)
```

**XGBoost (Extreme Gradient Boosting)** 은 Gradient Boosting이 병렬로 처리될 수 있게 만든 모델입니다. 
학습 속도가 빠르고 정확도가 높아 사람들이 많이 쓰는 모델 중 하나입니다.

공식문서의 xgboost 라이브러리에서 XGBClassifier를 통해 모델을 사용할 수 있습니다.
<br>[❕ XGBoost 공식 사이트][9]
<br>[❕ XGBoost 공식 문서][10]
```python
from xgboost import XGBClassifier

xgb = XGBClassifier(tree_method='hist', random_state=42)
scores = cross_validate(xgb, train_input, train_target, return_train_score=True, n_jobs=-1)
```

**LightGBM** 은 학습속도가 빠르고 다른 부스팅 모델과 성능 차이가 별로 안나는 모델입니다.

공식 문서에 따르면 일반적으로 10,000 건 이하의 데이터 세트를 다루는 경우 과적합 문제가 발생하기 쉽다는 단점이 있습니다.

공식문서의 lightgbm 라이브러리에서 LGBMClassifier를 통해 모델을 사용할 수 있습니다.
<br>[❕ LightGBM 공식 문서][11]
```python
from lightgbm import LGBMClassifier

lgb = LGBMClassifier(random_state=42)
scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)
```

<br>![XGBoost와 LightGBM에 대한 간략한 설명](.\images\3팀_이재흠\05-3 XGBoost and LightGBM.png)<br>


[1]: https://tensorflow.blog/파이썬-머신러닝/2-3-5-결정-트리/
[2]: https://process-mining.tistory.com/106
[3]: https://jhryu1208.github.io/data/2021/01/24/ML_cross_validation/
[4]: https://link.springer.com/chapter/10.1007/978-3-030-05318-5_1
[5]: https://carpe08.tistory.com/92
[6]: https://daheekwon.github.io/bayesian-optimization/
[7]: http://www.dinnopartners.com/__trashed-4/
[8]: https://hleecaster.com/ml-random-forest-concept/
[9]: https://xgboost.ai/
[10]: https://xgboost.readthedocs.io/en/stable/
[11]: https://lightgbm.readthedocs.io/en/latest/