결정 트리
======

>알코올 도수, 당도, pH 값에 로지스틱 회귀 모델을 적용하여 와인 종류 구별하기

```python
import pandas as pd
wine = pd.read_csv('http://bit.ly/wine_csv_data')

wine.head()
```
![](2022-04-06-14-38-40.png)

와인 데이터셋을 ```head()``` 메서드로 처음 5개 샘플을 확인하였습니다.

네 번째 열(class)은 타깃값으로 0이면 레드 와인, 1이면 화이트 와인입니다. 이진 분류 문제, 양성클래스는 화이트 와인입니다.

모델 훈련 전에 유용한 pandas 메서드를 알아봅시다.

>```wine.info()```

![](2022-04-06-14-41-17.png)

데이터프레임의 각 열의 데이터 타입과 누락된 데이터가 있는지 확인하는데 유용한 메서드입니다.

6,497개 샘플, 4개 열이 모두 실숫값, None-Null Count가 모두 6,497이므로 누락된 값이 없음을 알 수 있습니다.

>```wine.describe()```

![](2022-04-06-14-44-37.png)

데이터에 대한 간단한 통계를 보여주는 메서드입니다.

여기서 데이터의 스케일이 다름을 확인했으니 특성을 표준화하겠습니다.

```python
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()

from sklearn.model_selection import train_test split
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_sclaed, train_target)
print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))
```
```
0.7808350971714451
0.7776923076923077
```

```train_test_split()```는 테스트 세트를 25%로 지정하는게 기본값입니다. 여기서는 20%로 설정하였습니다.

```StandardScaler``` 클래스를 사용해 훈련 세트를 전처리했습니다.

표준화된 ```train_scaled```와 ```test_scaled```를 사용해 로지스틱 회귀 모델을 훈련하였습니다. 점수를 확인해보니 둘다 낮은게 과소적합인 것 같습니다.

---------------

결정 트리
-------

>결정 트리 모델은 스무 고개와 같습니다.

데이터를 잘 나눌 수 있는 질문을 찾는다면 계속 질문을 추가해서 분류 정확도를 높일 수 있습니다.

```python
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target) 
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))
```
```
0.996921300750433
0.8592307692307692
```

사이킷런에서 제공하는 결정 트리 모델을 학습시켜 score를 출력시켜보았습니다. 과대적합이라고 볼 수 있겠습니다.

```python
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
plt.figure(figsize=(10,7))
plot_tree(dt)
plt.show()
```
![](2022-04-06-16-44-26.png)

사이킷런의 ``plot_tree()`` 함수를 사용해 결정트리를 이해하기 쉬운 트리 그림으로 출력해 줍니다. 결정 트리는 위에서부터 아래로 자라납니다. 맨 위의 노드(node)를 **루트 노드(root node)** 라 부르고 맨 아래 끝에 달린 노드를 **리프 노트(leaf node)** 라고 합니다.

```python
plt.figure(figsize(10,7))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```
![](2022-04-06-16-49-15.png)

`plot_tree` 함수에서 트리의 깊이를 제한하여 출력하였습니다. `max_depth=1`은 루트 노드를 제외하고 하나의 노드를 더 확장하여 그립니다(사진 참고). `feature_names` 매개변수에는 특성의 이름을 전달할 수 있습니다. 사진이 담고있는 정보는 다음과 같습니다.

![](2022-04-06-16-52-35.png)

![](2022-04-06-16-54-45.png)

즉 가장 위 노드(**루트 노드**)는 당도가 -0.239 이하인지 질문합니다. `True`면 왼쪽 branch를 타고, `False`면 오른쪽 branch를 탑니다. 루트 노드의 총 샘플 수는 5,197개입니다. 이 중에서 음성 클래스(레드 와인)는 1,258개이고, 양성 클래스(화이트 와인)은 3,939개입니다. 이 값이 `value`에 나타나 있습니다.

![](2022-04-06-16-56-45.png)

>왼쪽 노드는 양성클래스의 샘플 개수가 크게 줄어들었고, 오른쪽 노드는 양성 클래스의 샘플 개수가 음성 클래스보다 상대적으로 큰 값을 가지게 되었습니다.

 `plot_tree()` 함수에서 `filled=True`로 지정하면 클래스마다 색깔을 부여하고, 어떤 클래스의 비율이 높아지면 점점 진한 색으로 표시합니다.

 >결정 트리는 리프 노드에서 가장 많은 클래스가 예측 클래스가 됩니다.

 따라서 앞의 두 노드에서 양성 클래스가 더 많으므로 두 노드 모두 양성 클래스로 예측합니다.

 --------------
 
 불순도
 ---
 노드 상자 안의 `gini` 값도 있었습니다. 이것은 **지니 불순도** 를 의미합니다. `DecisionTreeClassifier` 클래스의 `criterion` 매개변수의 기본값이 `gini` 입니다. `criterion` 매개변수는 데이터를 분할할 기준을 정해줍니다. 앞의 트리에서 루트 노드는 당도 -0.239를 기준으로 왼쪽과 오른쪽 노드로 나누었는데, 이 과정을 살펴보도록 합시다.

 >_지니 불순도 = 1 - (음성 클래스 비율^2 + 양성 클래스 비율^2)_

 음성 클래스 비율과 양성 클래스 비율이 같아 지니 불순도가 0.5가 되는 상황이 가장 불순도가 높습니다.

 노드에 하나의 클래스만 있다면 지니 불순도는 0이 되어(한 클래스의 비율이 0, 나머지 한 클래스의 비율이 1) 가장 작습니다. 이런 노드를 **순수 노드** 라고도 부릅니다.

 결정 트리 모델은 부모 노드와 자식 노드의 불순도 차이가 가능한 크도록 트리를 성장시킵니다. 불순도 차이는 다음과 같이 계산합니다.

 >_부모의 불순도 - (왼쪽 노드 샘플 수 / 부모의 샘플 수) * 왼쪽 노드 불순도 - (오른쪽 노드 샘플 수 / 부모의 샘플 수) * 오른쪽 노드 불순도_

 이런 부모와 자식 노드 사이의 불순도 차이를 **정보 이득**이라고 부릅니다.

 ------------
 
 가지치기
 ----

 ```pyhon
 dt = DecisionTreeClassifier(max_depth=3, random_state=42)
 print(dt.score(train_scaled, train_target))
 print(dt. score(test_scaled, test_target))
 ```
 ```
 0.8454877814123533
0.8415384615384616
```
가지치기를 하는 가장 간단한 방법으로 트리의 최대 깊이를 max_depth 매개변수로 3까지 지정하였습니다.

```python
plt.figure(figsize=(20, 15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'plt'])
plt.show()
```
![](2022-04-07-14-10-38.png)

트리 그래프로 모델을 그려보았습니다. 당도가 -인 노드도 있고 해서 해석이 쉽지 않습니다. **데이터 전처리**를 거치면 더 이해하기 쉬운 트리를 그릴 수 있습니다.

-----------

교차 검증과 그리드 서치
====

>"이런저런 값으로 모델을 많이 만들어서 테스트 세트로 평가하면 결국 테스트 세트에 잘 맞는 모델이 만들어지는 것 아닌가요?"

테스트 세트를 자꾸 사용하면 테스트 세트에 맞는 모델이 만들어집니다 이 문제를 해결하기 위해 **검증 세트**를 사용합니다.

```python
import pandas as pd
wine = pd.read_csv('http://bit.ly/wine_csv_data')

data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()

from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_stat=42)

su_input, val_input, sub_target, val_target = train_test_split(train_input, train_target, test_size=0.2, random_state=42)

#훈련 세트 4,157개, 검증 세트 1,040개

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(sub_input, sub_target)
```

교차 검증
-----

![](2022-04-07-14-36-02.png)
**3-폴도 교차 검증**입니다. 훈련 세트를 통해 모델을 훈련하지만 훈련 세트로 모델을 평가할 때 같이 이용됩니다. n-폴드 교차 검증을 할 때 해야될 평가는 n번이라고 추측할 수 있겠네요.

>```cross_validate()```

사이킷런의 교차 검증 함수입니다.

```python
from sklearn.model_selection import cross_validate
scores = cross_validate(dt, train_input, train_target)
print(scores)
```
![](2022-04-07-15-45-21.png)

이 함수는 fit_time, score_time, test_score 키를 가진 딕셔너리를 반환합니다. 이번 소입설 실습에서 딕셔너리를 배워서 다행입니다 ㅎㅎ. 처음 2개의 키는 각각 모델을 훈련하는 시간과 검증하는 시간을 의미합니다. 각 키마다 5개의 숫자가 담겨있는데, 이는 `cross_validate()` 함수가 기본적으로 5-폴드 교차 검증을 시행하기 때문입니다. cv 매개변수에서 폴드 수를 바꿀 수 있습니다.

```python
import numpy as np
print(np.mean(scores['test_score']))
```
```
0.855300214703487
```
교차 검증의 최종 점수를 `test_score`(검증 폴드의 점수) 키에 담긴 5개의 점수를 평균하여 구하였습니다.

--------------

하이퍼 파라미터 튜닝
---
>매개변수가 많아지면 최적의 하이퍼 파라미터를 찾기 힘들어진다.

가령 x, y의 매개변수가 있다고 하면 최적의 x값을 찾고 최적의 y값을 찾는게 아닌, 최적의 x, y 값을 동시에 찾아야 합니다. 사이킷런에서 제공하는 **그리드 서치**를 사용하면 됩니다.

```python
from sklearn.model_selection import GridSearchCV
params = {'min_impurity_decrease': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]}

gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)
#gs 클래스에 탐색 대상 모델, params 변수 전달
#n_job은 사용 CPU 코어 수 결정(-1: 모든 코어 사용)

gs.fit(train_input, train_target)
```
사이킷런의 그리드 서치는 훈련이 끝나면 25개 모델 중에서 검증 점수가 가장 높은 모델의 매개변수 조합으로 전체 훈련 세트에서 다시 모델을 훈련합니다.
>`dt = gs.best_estimator_`

위 모델은 일반 결정 트리와 똑같이 사용할 수 있습니다.
>`gs.best_params`

위 속성에 그리드 서치로 찾은 최적의 매개변수가 저장되어 있습니다.

>`gs.cv_results_['mean_test_score']`

각 매개변수에서 수행한 교차 검증의 평균 점수가 위 딕셔너리에 저장되어 있습니다.

>`argmax()`

넘파이의 함수입니다. 가장 큰 교차검증 점수의 인덱스를 추출할 수 있습니다.

```python
best_index = np.argmax(gs.cv_results_['mean_test_score'])
print(gs.cv_results_['params'][best_index])
```
```
{'min_impurity_decrease': 0.0001}
```
`gs.best_params_`와 동일한 값을 출력할 수 있습니다.

**과정 정리**
>1. 먼저 탐색할 매개변수를 지정합니다.
>2. 그 다음 훈련세트에서 **그리드 서치**를 수행하여 최상의 평균 검증 점수가 나온느 매개변수 조합을 찾습니다. 이 조합은 그리드 서치 객체에 저장됩니다.
>3. 그리드 서치는 최상의 매개변수에서 (교차 검증에 사용한 훈련 세트가 아니라) 전체 훈련 세트를 사용해 최종 모델을 훈련합니다. 이 모델도 그리드 서치 객체에 저장됩니다.

--------------

랜덤 서치
----
랜덤 서치에는 매개변수를 샘플링할 수 있는 확률 분포 객체를 전달합니다.
```python
from scipy.stats import uniform, randint

rgen = randint(0, 10)
rgen.rvs(10)

```
```
array([6, 4, 2, 2, 7, 7, 0, 0, 5, 4])
```
>randint: 정숫값 / uniform: 실숫값

난수 발생기랑 유사하게 생각하면 된다고 합니다. 

```python
params = {'min_imputiry_decrease' : uniform(0.0001, 0.001), 'max_depth': randint(20, 50), 'min_samples_split': randint(2, 25), 'min_samples_leaf': randint(1, 25)}

from sklearn.model_selection import RandomizedSearchCV
gs = RandomizedSearchCB(DecisionTreeClassifier(random_state=42), params, n_iter=100, n_jobs=-1, random_state=42)
gs.fit(train_input, train_target)

print(gs.best_params_)
print(np.max(gs.cv_results_['mean_test_score']))
```
```
{'max_depth': 14, 'min_impurity_decrease': 0.0004, 'min_samples_split': 12}

0.8683865773302731
```
`min_imputiry_decrease`는 0.0001에서 0.001 사이의 실숫값을 샘플링합니다. `max_depth`는 20에서 50 사이의 정수, `min_samples_split`은 2에서 25 사이의 정수, `min_samples_leaf`는 1에서 25 사이의 정수를 샘플링합니다. 샘플링 횟수는 사이킷런의 랜덤 서치 클래스인 `RandomizedSearchCV`의 `n_iter` 매개변수에 지정합니다.

그리고 params에 정의된 매개변수 범위에서 총 100번 샘플링하여 교차 검증을 수행하고 최적의 매개변수 조합을 찾습니다.

최적의 매개변수 조합(첫 번째 출력)과 최고의 교차 검증 점수(두 번째 출력)를 출력하였습니다.

```python
dt = gs.best_estimator_
print(dt.score(test_input, test_target))
```
```
0.86
```
최적의 모델은 이미 전체 훈련 세트 `(train_input, train_target)`로 훈련되어 `best_estimator_` 속성에 저장되어 있습니다. 이 모델의 테스트 세트 점수를 출력하였습니다.

---------------

트리의 앙상블
======

정형 데이터와 비정형 데이터
----
>4장까지는 생선의 길이, 높이, 무게 등을 데이터로 사용했습니다. 이 데이터는 csv 파일에 가지런히 정리되어 있었죠.

이런 정리된 데이터를 **정형 데이터**라고 합니다. 

이와 반대로 **비정형 데이터**는 데이터베이스(csv 파일)나 엑셀로 표현하기 어려운 것들입니다. 텍스트, 사진, 음악 등이 있습니다.

**앙상블 학습**은 정형 데이터를 다루는 데 가장 뛰어난 성과를 내는 알고리즘입니다.

----------

랜덤 포레스트
---
**랜덤 포레스트**는 앙상블 학습의 대표 주자 중 하나로 안정적인 성능 덕분에 널리 사용되고 있습니다.

![](2022-04-09-16-58-13.png)

랜덤 포레스트는 각 트리를 훈련하기 위한 데이터를 랜덤하게 만드는데, 우리가 입력한 훈련 데이터에서 랜덤하게 샘플을 추출하여 훈련 데이터를 만듭니다.

가령 훈련 데이터 1000개가 있고 1000개의 샘플을 뽑는다면, 먼저 1개를 뽑고 뽑은걸 다시 집어넣고 다시 1000개 중에서 하나를 뽑습니다. 그래서 중복된 것이 존재하는 1000개의 샘플이 만들어집니다. 이를 **부트스트랩 샘플**이라고 합니다.

>**부트스트랩**: 데이터 세트에서 중복을 허용하여 데이터를 샘플링하는 방식

**각 노드를 분할하는 방식**
![](2022-04-09-17-26-15.png)
분류 모델: `RandomForestClassifier`
회귀 모델: `RandomForestRegressor`

분류 모델은 전체 특성 개수의 제곱근만큼의 특성을 사용하지만, 회귀 모델은 전체 특성을 사용합니다.

사이킷런의 랜덤 포레스트는 기본적으로 100개의 결정 트리를 이런 방식으로 훈련합니다. 분류는 각 트리의 클래스별 확률을 평균하여 **가장 높은 확률**의 클래스를 예측으로 삼고, 회귀는 단순히 각 트리의 예측을 평균합니다.

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
wine = pd.read_csv('http://bit.ly/wine_csv_data')
data = wine[['alcohol', 'sugar' 'pH']].to_numpy()
target = wine['class'].to_numpy()
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)

from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_jobs=-1, random_state=42)
#모든 CPU사용
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)
#검증 점수와 훈련 세트 점수 모두 반환, 병렬 교차 검증
print(np.mean(score['train_score'], np.mean(score['test_score'])))
```
```
0.9973541965122431 0.8905151032797809
```
`RandomForestClassifier` 클래스를 화이트 와인을 분류하는 문제에 적용하였습니다. 

```python
rf.fit(train_input, train_target)
print(rf.feature_importance_)
```
위 과정을 통해 랜덤 포레스트 모델의 특성 중요도를 확인할 수 있는데, 두번째 특성의 당도의 중요도가 감소하고 나머지가 증가하였습니다. 이는 하나의 특성에 과도하게 집중을 하지 않고 다른 특성들을 더 사용하여 일반화 성능을 높이는 데 도움이 됩니다.

```python
rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)
rf.fit(train_input, train_target)
print(rf.obb_score_)
```
```
0.8934000384837406
```

`RandomForestClassifier`에서 부스트트랩 샘플을 만들 때 사용하지 않은 샘플(**OOB(Out Of Bag**))을 이용하여 결정 트리를 평가할 수 있습니다. 교차 검증에서 얻은 점수와 비슷한 결과를 보입니다.

--------
엑스트라 트리
---

```python
from sklearn.ensemble import ExtraTreesClassifier
et = ExtraTreesClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(score['train_score']), np.mean(scores['test_score']))
```
```
0.9974503966084433 0.8887848893166506
```
특성이 적어서 랜덤 포레스트와 결과가 비슷합니다.노드를 랜덤하게 분할하기 때문에 계산 속도가 빠른 것이 엑스트라 트리의 장점입니다.
>엑스트라 트리의 회귀 버전은 `ExtraTreesRegressor` 클래스입니다.

---
그레디언트 부스팅
---
>경사 하강법을 이용하여 트리를 앙상블에 추가

```python
from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier(random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_scores']), np.mean(scores['test_score']))
```
```
0.8881086892152563 0.8720430147331015
```
이때까지 했던 다른 앙상블 학습과 달리 과대적합이 일어나지 않습니다. 하지만 순서대로 트리를 추가하기 떄문에 훈련 속도가 느립니다(`n_jobs` 매개변수가 없습니다).

회귀 버전은 `GradientBoostingRegressor` 입니다.

---

히스토그램 기반 그레이디언트 부스팅
---
**히스토그램 기반 그레디언트 부스팅**은 우선 입력 특성을 256개의 구간으로 나눕니다. 따라서 노드를 분할할 때 최적의 분할을 매우 빠르게 찾을 수 있습니다(???).

```python
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier
hgb = HistGradientBoostingClassifier(random_state=42)
scores = cross_validate(hgb, train_input, train_target, return_train_score=True)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```
```
0.9321723946453317 0.8801241948619236
```
와인 데이터셋에 사이킷런의 히스토그램 기반 그레이디언트 부스팅 클래스 `HistGradientBoostingClassifier`입니다
(사이킷런 1.0에서부터는 enable_hist_gradient_boosting 모듈을 임포트할 필요가 없습니다).

```python
from sklearn.inspection import permutation_importance

hgb.fit(train_input, train_target)
result = permutation_importance(hgb, train_input, train_target, n_repeats=10, random_state=42, n_jobs=-1)
#n_repeats: 랜덤하게 섞을 횟수
print(result.importances_mean)
```
```
[0.08876275 0.23438522 0.08027708]
```
히스토그램 기반 그레이디언트 부스팅의 특성 중요도를 `permutation_importance()` 함수를 사용하였습니다. 이 함수가 반환하는 객체는 반복하여 얻은 특성 중요도(importances), 평균(importances_mean), 표준 편차(importances_std)를 담고 있습니다.

회귀 버전은 `HistGradientBoostingRegressor` 클래스에 구현되어 있습니다.

사이킷런 말고도 그레이디언트 부스팅 알고리즘을 구현한 라이브러리가 여럿 있는데, 대표적으로 `XGBoost`입니다. 이 라이브러리는 사이킷런의 `cross_validate()` 함수와 함께 사용할 수 있습니다.

```python
from xgboost import XGBClassifier
xgb = XGBClassifier(tree_method='hist', random_state=42)
scores = cross_validate(xgb, train_input, train_target, return_train_score=True)
print(np.mean(scores['train_socre']), np.mean(scores['test_score']))
```
```
0.9555033709953124 0.8799326275264677
```
`XGBoost`를 사용해 와인 데이터의 교차 검증 점수를 확인하였습니다.

다음은 `LightGBM`입니다.

```python
from lightgbm iport LGBMClassifier
lgb = LGBMClassifier(random_state=42)
scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_job=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```
```
0.935828414851749 0.8801251203079884
```
`LightGBM`은 마이크로소프트 사에서 만들었고, 빠르고 최신 기술을 많이 적용하고 있습니다.