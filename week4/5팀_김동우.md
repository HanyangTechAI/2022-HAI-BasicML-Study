# Chap 5
## 결정 트리
결정 트리는 데이터를 질문을 통해 분류하는 알고리즘이다.

결정 트리 호출
```py
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
```
트리 그림 생성
```py
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
plt.figure(figsize=(?,?))
plot_tree(dt, max_depth=?, filled=True)
plt.show()
```
노드: 트리 구성의 핵심 요소, 훈련 데이터의 특성에 대한 테스트를 표시  
가지: 테스트의 결과를 나타냄
지니 불순도: 1-(음성 클래스 비율^2+양성 클래스 비율^2)  
정보 이득: 부모 자식 노트 사이의 불순도 차이, 결정 트리는 이 값이 크도록 데이터를 분류함

가지치기
```py
dt = DecisionTreeClassifier(max_depth=3)
dt.fit(train_scaled, train_target)
```
결정 트리는 표준화 전처리를 할 필요가 없다.

## 교차 검증과 그리드 서치
검증 세트: 훈련 세트(train)를 훈련 세트(sub)와 검증 세트(val)로 한 번 더 나눔
교차 검증: 검증 세트를 떼어 내어 평가하는 과정을 여러 번 반복함
```py
from sklearn.model_selection import cross_validate
scores = cross_validate(dt, train_input, train_target)
```
교차 검증의 최종 점수
```py
np.mean(scores['test_score'])
``` 
분할기를 지정하면 교차 검증을 할 때 훈련 세트를 섞음  
회귀: KFold 분류: StratifiedKFold  
StratifiedKFold 분할기 + 10-폴드 교차 검증
```py
scores = cross_validate(dt, train_input, train_target, cv = StratifiedKFold(n_splits=10))
```
그리드 서치: 하이퍼파라미터 탐색 + 교차검증
```py
from sklearn.model_selection import GridSearchCV
params = {'min_impurity_decrease': [0.001, 0.002, ...]}
gs = GridSearchCV(DecisionTreeClassifier(), params, n_jobs=-1)
gs.fit(train_input, train_target)
```
그리드 서치에서 찾은 최적의 매개변수 조합으로 다시 모델을 훈련한 결과
```py
dt = gs.best_estimator_
dt.score(train_input, train_target)
```
gs.best_params_ 은 gs.cv_results_['params'][best_index] 와 결과가 같다  
(best_index = np.argmax(gs.cv_results_['mean_test_score']))

랜덤 서치: 매개변수를 샘플링할 수 있는 확률 분포 객체를 전달
```py
from scipy.stats import uniform, randint
params = {'min_impurity_decrease': uniform(0.0001, 0.001), 'max_depth': randint(20, 50), 'min_samples_split': randint(2, 25), 'min_samples_leaf': randint(1, 25)}
from sklearn.model_selection import FandomizedSearchCV
gs = RandomizedSearchCV(DecisionTreeClassifier(), params, niter=100, n_jobs=-1)
gs.fit(train_input, train_target)
print(gs.best_params_)
```
앙상블 학습: 정형 데이터를 다루는 데 가장 뛰어난 성과를 내는 알고리즘  
랜덤 포레스트: 앙상블 학습의 대표 주자, 결정 트리를 랜덤하게 만듦  
부트스트랩 샘플: 훈련 데이터에서 랜덤하게 샘플을 추출하여 훈련 데이터를 만드는 랜덤 포레스트의 데이터 생성 방식
```py
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_jobs=-1)
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```
엑스트리 트리: 랜덤 포레스트에서 부트스트랩 샘플을 사용하지 않고 전체 훈련 세트를 사용
```py
from sklearn.ensemble import ExtraTreesClassifier
et = ExtraTreesClassifier(n_jobs=-1)
```
그래이디언트 부스팅: 깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완
```py
from sklearn.ensemble import GradientBooostingClassifier
gb = GradientBoostingClassifier()
```
히스토그램 기반 그레이디언트 부스팅: 입력 특성을 256개 구간으로 나누어 노드를 분할할 때 최적의 분할을 매우 빠르게 찾음, 256개의 구간 중에서 하나를 떼어 놓고 누락된 값을 위해서 사용
```py
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier
hgb = HistGradientBoostingClasifier()
```
