!wget https://bit.ly/fruits_300_data -O fruits_300.npy
#과일 데이터 불러오기
import numpy as np
import matplotlib.pyplot as plt
fruits = np.load('fruits_300.npy')
plt.imshow(fruits[0], cmap='gray_r')
#검은색과 흰색을 반전시켜 보여 줌
plt.show()

fig, axs = plt.subplots(1, 2)
axs[0].imshow(fruits[100], cmap='gray_r')
axs[1].imshow(fruits[200], cmap='gray_r')
plt.show()
apple = fruits[0:100].reshape(-1, 100*100)
#1차원 배열로 변경
pineapple = fruits[100:200].reshape(-1, 100*100)
banana = fruits[200:300].reshape(-1, 100*100)

plt.hist(np.mean(apple, axis=1), alpha=0.8)
plt.hist(np.mean(pineapple, axis=1), alpha=0.8)
plt.hist(np.mean(banana, axis=1), alpha=0.8)
plt.legend(['apple', 'pineapple', 'banana'])
plt.show()
#픽셀값을 평균화하여 히스토그램화하여 표현
fig, axs = plt.subplots(1, 3, figsize=(20, 5))
axs[0].bar(range(10000), np.mean(apple, axis=0))
axs[1].bar(range(10000), np.mean(pineapple, axis=0))
axs[2].bar(range(10000), np.mean(banana, axis=0))
plt.show()
#각각의 픽셀값의 평균을 표현
apple_mean = np.mean(apple, axis=0).reshape(100, 100)
pineapple_mean = np.mean(pineapple, axis=0).reshape(100, 100)
banana_mean = np.mean(banana, axis=0).reshape(100, 100)

fig, axs = plt.subplots(1, 3, figsize=(20, 5))
axs[0].imshow(apple_mean, cmap='gray_r')
axs[1].imshow(pineapple_mean, cmap='gray_r')
axs[2].imshow(banana_mean, cmap='gray_r')
plt.show()
#평균 픽셀을 그림으로 표현
#각각의 과일의 평균 모습으로 생각할 수 있음
abs_diff = np.abs(fruits - apple_mean)
abs_mean = np.mean(abs_diff, axis=(1,2))
print(abs_mean.shape)
apple_index = np.argsort(abs_mean)[:100]
fig, axs = plt.subplots(10, 10, figsize=(10,10))
for i in range(10):
    for j in range(10):
        axs[i, j].imshow(fruits[apple_index[i*10 + j]], cmap='gray_r')
        axs[i, j].axis('off')
plt.show()
#각각의 이미지와 가장 가까운 사진을 100개씩 고름

#이번 챕터 정리 
#사진 픽셀의 평균 값을 이용하여 사진 분류


from sklearn.cluster import KMeans
#k평균 알고리즘
#신박....
#직관적인 이해를 위해 데이터의 특성을 좌표축위에 표시한다고 할 때, 각각의 같이 분류되어야 하는 데이터는 비교적 가까이 있을 것임#가까이 있는 애들끼리 컴퓨터로 묶는 방식
#가까이 있는 애들의 평균 위치로 이동, 묶음 새로 정의,위 과정 반복
km = KMeans(n_clusters=3, random_state=42)
km.fit(fruits_2d)


import matplotlib.pyplot as plt

def draw_fruits(arr, ratio=1):
    n = len(arr)    # n은 샘플 개수입니다
    # 한 줄에 10개씩 이미지를 그립니다. 샘플 개수를 10으로 나누어 전체 행 개수를 계산합니다. 
    rows = int(np.ceil(n/10))
    # 행이 1개 이면 열 개수는 샘플 개수입니다. 그렇지 않으면 10개입니다.
    cols = n if rows < 2 else 10
    fig, axs = plt.subplots(rows, cols, 
                            figsize=(cols*ratio, rows*ratio), squeeze=False)
    for i in range(rows):
        for j in range(cols):
            if i*10 + j < n:    # n 개까지만 그립니다.
                axs[i, j].imshow(arr[i*10 + j], cmap='gray_r')
            axs[i, j].axis('off')
    plt.show()

#그림 그리는 함수

draw_fruits(fruits[km.labels_==0])
draw_fruits(fruits[km.labels_==1])
draw_fruits(fruits[km.labels_==2])

draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio=3)

draw_fruits(fruits[100:101])

inertia = []
#최적의 k를 구하는 방법
for k in range(2, 7):
    km = KMeans(n_clusters=k, random_state=42)
    km.fit(fruits_2d)
    inertia.append(km.inertia_)
#적당히 묶일 수 록 중심의 수 대비 중심과 객체의 거리가 가까울 것
plt.plot(range(2, 7), inertia)
plt.xlabel('k')
plt.ylabel('inertia')
plt.show()
#엘보우 포인트 이용


from sklearn.decomposition import PCA
#주성분 분석 --> 데이터에서 액가스만 빼내는 방식/ 약간 convolutional이 생각남

pca = PCA(n_components=50)
#기존에 10000개였던 축을 무려 50개로 줄임
pca.fit(fruits_2d)


draw_fruits(pca.components_.reshape(-1, 100, 100))
fruit_pca = pca.transform(fruit_2d)
#주성분으로 데이터 변환
fruits_inverse = pca.inverse_transform(fruits_pca)
#자시 복원
print(fruits_inverse.shape)

fruits_reconstruct = fruits_inverse.reshape(-1, 100, 100)

for start in [0, 100, 200]:
    draw_fruits(fruits_reconstruct[start:start+100])
    print("\n")
#복원한 데이터를 프린트 함으로써 데이터가 얼마나 손실되었는지 확인 가능
