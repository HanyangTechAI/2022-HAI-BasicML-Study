# 2022년 3월 17일 HAI 혼공머신 스터디 1조 발표
### 발표자: 임정섭

## __Contents__
### Chapter 1
- 인공지능 / 머신러닝 / 딥러닝 이란?
- 왜 머신러닝인가?
- 지도학습과 비지도 학습

### Chapter 2
- 머신러닝 예제(KNN 알고리즘을 이용한 생선 분류)

<hr>

## Chapter 1
### 인공지능 / 머신러닝 / 딥러닝 이란?
- 인간의 지능을 모방하는 기술 분야인 인공지능과 기계가 학습하는 과정을 통해 이를 구현하는 머신 러닝, 그리고 인공 신경망을 이용한 머신러닝 기법인 딥러닝이 있습니다.
### 왜 머신러닝인가?
- 결정에 필요한 로직이 한 분야, 작업에 한정되고, 규칙을 설계하려면 그 분야의 전문가들이 내리는 결정 방식에 대해 잘 알아야 합니다.
### 지도학습과 비지도 학습
- 지도 학습의 종류: KNN, 로지스틱 회귀, 신경망, 선형 회귀, SVM, 결정 트리
- 비지도 학습의 종류: K-Mean, 주성분 분석
- 이 외에도 강화 학습 등 다양한 종류가 있습니다.

<hr>

## Chapter 2
### 머신러닝 예제(KNN 알고리즘을 이용한 생선 분류)

#### 이 챕터에서는 사이킷런을 이용한 첫 머신 러닝 프로그램을 구현하는 과정을 설명합니다.

<br>

### 1. 개발 환경 구축
---
- 개발 환경으로는 손쉽게 머신 러닝 프레임워크를 구동하기 위해 Google Colab을 사용했습니다.
- 프로젝트에 사용될 언어는 Python이며 Numpy, Matplotlib, Scikits-learn 라이브러리가 사용되었습니다.
  
### 2. KNN 알고리즘 소개
---
- KNN 알고리즘은 주어진 데이터가 있을 때 새로운 데이터의 라벨을 예측하는 회귀 알고리즘 중 하나입니다.
- 새로운 데이터가 주어지면, 거리가 가장 가까운 몇 개의 데이터를 살펴본 뒤 가장 많은 데이터가 포함된 라벨이 새로운 데이터의 라벨이 됩니다.
- 따로 훈련이 필요하지 않고, 데이터를 저장하는 과정이 전부라는 특징이 있습니다.

### 3. 데이터 다루기 및 시각화
---
- 이 프로젝트에서는 도미와 빙어라는 두 종류의 생선들의 데이터를 입력받아 새로 추가된 생선이 도미인지 빙어인지 구분하는 프로그램을 작성합니다.
- 각 생선들은 길이와 무게라는 두 개의 특성을 지니고 있습니다.
- matplotlib.pyplot을 이용하여 그래프 형태료 표현해볼 수 있습니다.
- 도미를 찾는것을 1로, 빙어를 0으로 정의하여 라벨 데이터도 생성합니다.

### 4. sklearn을 이용한 KNN 알고리즘
---
- skleanr.neighbors에 포함된 KNeighborsClassifier 클래스를 활용합니다.
- fit() 함수를 활용해 주어진 데이터로 알고리즘을 훈련시킵니다.
- n_neighbors 파라미터를 조정하는 것을 통해 얼마나 많은 이웃들을 고려할지 정할 수 있습니다.

### 5. 훈련 데이터와 테스트 데이터
---
- 어떤 머신러닝 모델을 훈련할 때, 모든 데이터로 훈련시키는 것 보다는 전체 데이터에서 일부분을 테스트 데이터로 구분하여 훈련 과정에서 완전히 배제시키고 훈련이 끝난 모델에서 모델의 성능을 평가하는 방식으로 사용됩니다.
- 이 프로젝트에서는 전체 데이터의 약 25%를 균일하게 뽑아서 테스트 데이터로 사용했습니다.

### 6. 데이터 전처리
---
- 도미와 빙어의 성질인 길이와 무게는 단위와 데이터 분포가 다르기 때문에 상대적으로 숫자가 더 큰 무게가 KNN 알고리즘에서 더 중요한 성질처럼 학습될 수 있습니다.
- 그래서 무게와 길이 데이터를 각각 평균을 빼고 표준편차로 나누어 정규화하면 각각의 데이터가 편중되지 않게 반영될 수 있습니다.
- 여기서 주의해야 할 점은 테스트 데이터의 성질이 훈련 과정에 반영되면 안 되기 때문에, 테스트 데이터를 전처리할 때에도 훈련 데이터의 성질(평균, 표준편차)를 사용해야 합니다.

### __아래는 이번주 발표자분이 작성한 프로그램 실행 과정입니다.__

---

<br>

# 혼공머신 1.3장 예제 - KNN 알고리즘을 이용한 생선 분류


## 예제 데이터 준비


```python
# 도미 Data
#특성1. 길이
bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 
                31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 
                35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]
#특성2. 무게
bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 
                500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 
                700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0]
# 빙어 Data
smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]
```

## matplotlib 을 이용한 데이터 그래프 표현


```python
import matplotlib.pyplot as plt
# 산점도 그래프 생성
plt.scatter(bream_length, bream_weight)
plt.scatter(smelt_length, smelt_weight)
# x축
plt.xlabel('length')
# y축
plt.ylabel('weight')
plt.show()

```


## KNN 알고리즘을 이용한 도미 빙어 데이터 구분


```python
# 데이터 합치기
length = bream_length + smelt_length
weight = bream_weight + smelt_weight

fish_data = [[l, w] for l,w in zip(length,weight)]
print(fish_data)
```

    [[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0], [29.7, 450.0], [29.7, 500.0], [30.0, 390.0], [30.0, 450.0], [30.7, 500.0], [31.0, 475.0], [31.0, 500.0], [31.5, 500.0], [32.0, 340.0], [32.0, 600.0], [32.0, 600.0], [33.0, 700.0], [33.0, 700.0], [33.5, 610.0], [33.5, 650.0], [34.0, 575.0], [34.0, 685.0], [34.5, 620.0], [35.0, 680.0], [35.0, 700.0], [35.0, 725.0], [35.0, 720.0], [36.0, 714.0], [36.0, 850.0], [37.0, 1000.0], [38.5, 920.0], [38.5, 955.0], [39.5, 925.0], [41.0, 975.0], [41.0, 950.0], [9.8, 6.7], [10.5, 7.5], [10.6, 7.0], [11.0, 9.7], [11.2, 9.8], [11.3, 8.7], [11.8, 10.0], [11.8, 9.9], [12.0, 9.8], [12.2, 12.2], [12.4, 13.4], [13.0, 12.2], [14.3, 19.7], [15.0, 19.9]]
    


```python
# 1 : 도미 / 2: 빙어
fish_target = [1]*35 + [0]*14
print(fish_target)
```

    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    


```python
from sklearn.neighbors import KNeighborsClassifier
```


```python
# KNN 객체 생성
kn = KNeighborsClassifier()
# fit() : 주어진 데이터로 알고리즘을 훈련 
kn.fit(fish_data, fish_target)
# score() : 0~1 사이의 값 반환
kn.score(fish_data, fish_target)
```




    1.0




```python
# predict
kn.predict([[30,600]]) , kn.predict([[10,100]]) # (array([1]), array([0]))
```




    (array([1]), array([0]))




```python
# n_neighbors
kn49 = KNeighborsClassifier(n_neighbors=49)
kn49.fit(fish_data, fish_target)
print(kn49.score(fish_data, fish_target))
print(35/49)
```

    0.7142857142857143
    0.7142857142857143
    


```python
kn49.predict([[10, 100]])
```




    array([1])




```python
plt.scatter(25, 500, marker='^')
# 산점도 그래프 생성
plt.scatter(bream_length, bream_weight)
plt.scatter(smelt_length, smelt_weight)
# x축
plt.xlabel('length')
# y축
plt.ylabel('weight')
plt.show()
```


    


## 훈련 세트 & 테스트 세트


```python
import numpy as np
```

#### Numpy vs List 속도 비교


```python
my_arr = np.arange(1000000)
my_list = list(range(1000000))
%time for _ in range(10): my_arr2 = my_arr * 2
%time for _ in range(10): my_list2 = [x * 2 for x in my_list]
```

    CPU times: user 14.1 ms, sys: 5.32 ms, total: 19.4 ms
    Wall time: 22.6 ms
    CPU times: user 763 ms, sys: 190 ms, total: 953 ms
    Wall time: 957 ms
    

#### Numpy를 이용한 데이터 표현 


```python
input_arr = np.array(fish_data)
target_arr = np.array(fish_target)
print(input_arr)
print(target_arr)
```

    [[  25.4  242. ]
     [  26.3  290. ]
     [  26.5  340. ]
     [  29.   363. ]
     [  29.   430. ]
     [  29.7  450. ]
     [  29.7  500. ]
     [  30.   390. ]
     [  30.   450. ]
     [  30.7  500. ]
     [  31.   475. ]
     [  31.   500. ]
     [  31.5  500. ]
     [  32.   340. ]
     [  32.   600. ]
     [  32.   600. ]
     [  33.   700. ]
     [  33.   700. ]
     [  33.5  610. ]
     [  33.5  650. ]
     [  34.   575. ]
     [  34.   685. ]
     [  34.5  620. ]
     [  35.   680. ]
     [  35.   700. ]
     [  35.   725. ]
     [  35.   720. ]
     [  36.   714. ]
     [  36.   850. ]
     [  37.  1000. ]
     [  38.5  920. ]
     [  38.5  955. ]
     [  39.5  925. ]
     [  41.   975. ]
     [  41.   950. ]
     [   9.8    6.7]
     [  10.5    7.5]
     [  10.6    7. ]
     [  11.     9.7]
     [  11.2    9.8]
     [  11.3    8.7]
     [  11.8   10. ]
     [  11.8    9.9]
     [  12.     9.8]
     [  12.2   12.2]
     [  12.4   13.4]
     [  13.    12.2]
     [  14.3   19.7]
     [  15.    19.9]]
    [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
     0 0 0 0 0 0 0 0 0 0 0 0]
    


```python
# random 으로 data 뽑기
# seed 값 설정 시 일정한 random 값 생성
np.random.seed(3)
index = np.arange(49)
np.random.shuffle(index)
print(index)
```

    [12 39  9 46 31 28 13 47 44  6 36 23 37 18  4 25 16 15 30 11  7 40 27 34
     45  5 48  1 35  2 22 33 17 26 14 29 20 32 38 43 41 10 19 21  0  8  3 24
     42]
    


```python
# train_data 생성하기
train_input = input_arr[index[:35]]
train_target = target_arr[index[:35]]

# test_data 생성하기
test_input = input_arr[index[35:]]
test_target = target_arr[index[35:]]
```


```python
plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(test_input[:,0], test_input[:,1])
plt.show()
```


    


```python
# KNN 알고리즘으로 test_data 대입해보기
kn = kn.fit(train_input, train_target)
kn.score(test_input,test_target)
```




    1.0




```python
kn.predict([[20, 250]])
```




    array([1])



## 데이터 전처리


```python
# np.column_stack 을 이용한 fish_data 생성

fish_data = np.column_stack((length, weight))
print(fish_data[:10])
```

    [[ 25.4 242. ]
     [ 26.3 290. ]
     [ 26.5 340. ]
     [ 29.  363. ]
     [ 29.  430. ]
     [ 29.7 450. ]
     [ 29.7 500. ]
     [ 30.  390. ]
     [ 30.  450. ]
     [ 30.7 500. ]]
    


```python
# np.zeros(), np.ones() : 0, 1 행렬 생성
# fish_target
fish_target = np.concatenate((np.ones(35), np.zeros(14)))
print(fish_target)
```

    [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
     1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
     0.]
    

### 사이킷런으로 훈련 데이터와 테스트 데이터 나누기 


```python
from sklearn.model_selection import train_test_split
```


```python
train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, random_state = 42)
print(train_input, test_input, train_target, test_target)
```

    [[  30.   450. ]
     [  29.   363. ]
     [  29.7  500. ]
     [  11.3    8.7]
     [  11.8   10. ]
     [  13.    12.2]
     [  32.   600. ]
     [  30.7  500. ]
     [  33.   700. ]
     [  35.   700. ]
     [  41.   975. ]
     [  38.5  920. ]
     [  25.4  242. ]
     [  12.     9.8]
     [  39.5  925. ]
     [  29.7  450. ]
     [  37.  1000. ]
     [  31.   500. ]
     [  10.5    7.5]
     [  26.3  290. ]
     [  34.   685. ]
     [  26.5  340. ]
     [  10.6    7. ]
     [   9.8    6.7]
     [  35.   680. ]
     [  11.2    9.8]
     [  31.   475. ]
     [  34.5  620. ]
     [  33.5  610. ]
     [  15.    19.9]
     [  34.   575. ]
     [  30.   390. ]
     [  11.8    9.9]
     [  32.   600. ]
     [  36.   850. ]
     [  11.     9.7]] [[ 32.  340. ]
     [ 12.4  13.4]
     [ 14.3  19.7]
     [ 12.2  12.2]
     [ 33.  700. ]
     [ 36.  714. ]
     [ 35.  720. ]
     [ 35.  725. ]
     [ 38.5 955. ]
     [ 33.5 650. ]
     [ 31.5 500. ]
     [ 29.  430. ]
     [ 41.  950. ]] [1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0.
     1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0.] [1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
    

### 수상한 도미 한마리


```python
kn = KNeighborsClassifier()
# train
kn.fit(train_input, train_target)
# test
kn.score(test_input, test_target)
```




    1.0




```python
# sample predict
print(kn.predict([[25,150]]))
```

    [0.]
    


```python
plt.scatter(train_input[:, 0], train_input[:,1])
plt.scatter(25, 150, marker='^')
plt.show()
```


    



```python
# 최근접 이웃의 거리 및 index
distances, indexes = kn.kneighbors([[25, 150]])
print(indexes)
print(distances)
```

    [[12 29  5 19  4]]
    [[ 92.00086956 130.48375378 138.32150953 140.00603558 140.62090883]]
    


```python
plt.scatter(train_input[:, 0], train_input[:,1])
plt.scatter(25, 150, marker='^')
# 최근접 이웃 표시
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')
plt.show()
```


    


## 데이터 전처리 : x축, y축 기준 맞추기


```python
#평균, 표준편차
mean = np.mean(train_input, axis=0)
std = np.std(train_input, axis=0)

print(mean, std)
```

    [ 26.175      418.08888889] [ 10.21073441 321.67847023]
    


```python
# 전처리 데이터
train_scaled  = (train_input - mean) / std
```

### 전처리 데이터로 모델 훈련하기



```python
plt.scatter(train_scaled[:,0], train_scaled[:,1])
# 전처리 데이터와 마찬가지로 테스트 데이터도 전처리 해야됨.
plt.scatter(25, 150,marker='^') 
plt.show()
```


    



```python
new_sample_scaled = ([25, 150] - mean) / std
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new_sample_scaled[0], new_sample_scaled[1])
plt.show()
```


    



```python
test_scaled = (test_input - mean) / std
kn.fit(train_scaled, train_target)
print(kn.score(test_scaled, test_target))
print(kn.predict([new_sample_scaled]))
```

    1.0
    [1.]
    


```python
distances, indexes = kn.kneighbors([new_sample_scaled])
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new_sample_scaled[0], new_sample_scaled[1], marker='^')
plt.scatter(train_scaled[indexes, 0], train_scaled[indexes, 1], marker='D')
plt.show()
```


    


## 결정경계

### 외부 라이브러리 불러오기


```python
import os, sys
from google.colab import drive
drive.mount('/content/drive')

my_path = '/content/notebooks'
# Colab Notebooks 안에 my_env 폴더에 패키지 저장
#os.symlink('/content/drive/My Drive/Colab Notebooks/my_env', my_path)
sys.path.insert(0, my_path)
```

    Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).
    

### mglearn 라이브러리를 사용하여 결정경계 확인 
  


```python
import mglearn
```


```python
mglearn.plots.plot_2d_separator(kn, train_scaled, fill=True, eps=0.5, alpha=0.5)
mglearn.discrete_scatter(train_scaled[:,0], train_scaled[:,1], train_target)
plt.scatter(new_sample_scaled[0], new_sample_scaled[1], marker='D')
plt.show()
```


    


## 최적의 K 찾기


```python
# 훈련 정확도와 테스트 정확도를 저장할 리스트 선언
training_accuracy = []
testing_accuracy = []

# neighbor 1~36 세팅
neighbor_settings = range(1,37)
for n_neighbor in neighbor_settings:
  kn = KNeighborsClassifier(n_neighbors=n_neighbor)
  kn.fit(train_input, train_target)
  training_accuracy.append(kn.score(train_input, train_target))
  testing_accuracy.append(kn.score(test_input, test_target))
plt.plot(neighbor_settings, training_accuracy, label = '훈련 정확도')
plt.plot(neighbor_settings, testing_accuracy, label = '테스트 정확도')
plt.show()
print(f'훈련 데이터에서 최적의 k 는 {training_accuracy.index(max(training_accuracy)) + 1}')
print(f'테스트 데이터에서 최적의 k 는 {testing_accuracy.index(max(training_accuracy)) + 1}')


```




    훈련 데이터에서 최적의 k 는 1
    테스트 데이터에서 최적의 k 는 1
    
