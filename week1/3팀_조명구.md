 1주차에는 구글 코랩의 기본적인 사용법과 k-최근접 이웃 알고리즘을 공부했습니다.
교재에서 배운 내용을 학습하는 것을 기본적으로 구글 코랩이라는 곳에서 하는데
코드를 구글 클라우드에서 실행하기 때문에 본인이 사용하는 pc의 영향을 받지 않지만
할당받은 메모리 안에서 코드 실행이 이루어져야하기 때문에 조금의 제한사항이 있습
니다. 이 책의 커리큘럼을 따라가는데에는 무리가 없다고 합니다. 
 첫 머신러닝은 k-최근접 알고리즘을 학습하기 위해 샤이킷런에서 KNeighborsClassifier
을 import하여 사용합니다. k-최근접 알고리즘을 이용하게 해주는 머신러닝 관련 class인
걸로 이해하고 있습니다. KNeighborsClassifier(이하 kn)는 지도 학습에 해당하는데 이 
지도학습은 input과 target을 필요로 합니다. input에 대응하는 target을 학습한 후
test set로 kn이 학습이 잘 되었는지 결정계수(Ch. 3)를 점수로 매깁니다. fit 명령어를
이용하여 train하고 score 명령어로 점수를 매깁니다. 점수가 1에 가까울수록 훈련이
잘 되었다고 볼 수 있습니다.
 kn은 제시된 데이터가 도미냐 빙어냐를 판별하기 위해 가장 가까운 이웃 5개를 그래프
상 거리를 기준으로 선발하여 그 중 많은 쪽으로 판별합니다. 하지만 여기서 무게와
너비 단위가 다르다는 이유로 산점도 그래프상으로 빙어인 데이터를 도미로 오판정하게
됩니다. 이를 계기로 데이터 전처리라는 걸 공부하게 됩니다.
 데이터 전처리는 모든 데이터를 평등하게 만들어줍니다. 여기서 배운 데이터 전처리는
표준 점수라고 하는 것으로, {(데이터)-(평균)}/(표준편차)로 계산합니다. 이 표준 점수는
train_input을 기준으로 구한 평균과 표준편차를 이용하여 train_input, test_input 둘 다를
전처리해주어야 합니다. 이 과정을 통해 x축과 y축의 scale 문제가 해소될 수 있습니다.