# 🖥️ 혼공머신 스터디 : 1장, 2장 요약
#### 스터디 3조 이재흠 (@rethinking21, rethinking21@gmail.com)

***
## 챕터 1 :  나의 첫 머신러닝 <br><br>
### 01-1. 인공지능, 머신러닝, 딥러닝

알파고의 등장 이후, 인공지능이란 단어는 일상 속에 깊숙이 파고 들었습나다. 그 결과, 일상생활 속에서 인공지능이라는 단어를 심심치 않게 볼 수 있습니다.<br>
**인공지능, 머신러닝, 딥러닝**은 이쪽 분야에 대해서 설명할 때 주로 쓰이는 단어들입니다. 사람들은 이 세 단어를 혼용해서 사용하곤 하지만 이들은 서로 다릅니다.

- 인공지능 : 사람처럼 학습하고 추론 할 수 있는 지능을 가진 컴퓨터 시스템을 만드는 기술
- 머신러닝 : 자동으로 데이터에서 규칙을 학습하는 알고리즘을 연구하는 분야, 통계학과 연관이 깊다.
- 딥러닝 : 머신러닝 알고리즘 중 **인공신경망**을 기반으로 학습한 방법

정확히 말하면 인공지능 안에 머신러닝이라는 개념이 있고, 머신러닝 안에 딥러닝이라는 개념이 있는 것입니다.

    인공지능 ⊃ 머신러닝 ⊃ 딥러닝


###### 인공지능, 머신러닝, 딥러닝의 역사
인공지능에 대한 역사는 의외로 짧지 않습니다(80년 정도). 머신러닝이라는 기법 또한 또한 1949년에 최초로 발표되었고, 인공신경망이라는 개념도 1943년에 발표되었습니다. 또한 인공신경망이라는 개념도 하지만 컴퓨터 성능의 한계로 인해 인공지능에 대한 연구와 투자는 크게 줄어들었습니다.
인공지능에 대한 연구가 활발히 일어나기 시작한건 비교적 최근입니다. 검색 엔진의 발달로 이전과는 비교도 할 수 없을 정도로 방대한 데이터를 수집할 수 있게 되었고, 컴퓨터 또한 성능이 꾸준히 발전해 딥 블루, 왓슨과 같은 인간의 수준을 뛰어넘은 인공지능들이 등장하게 되었습니다.
<br>

###### Scikit-learn, TensorFlow
컴퓨터 과학 분야의 대표적인 **머신러닝** 라이브러리입니다.<br>
사이킷 런과 같은 오픈소스 라이브러리가 있기 전까지 머신러닝 기술들은 폐쇄적인 코드와 라이브러리로 통용되어왔습니다. 하지만 누구나 무료로 이용할 수 있는 오픈소스 라이브러리의 등장 덕분에 사람들의 접근성의 쉬워져 머신러닝 분야는 폭발적으로 성장했습니다.
<br><br>또한 구글은 2015년 딥러닝 라이브러리인 **텐서플로우**를 오픈소스로 공개하고, 페이스북은 2018년 딥러닝 라이브러리인 **파이토치**를 오픈소스로 발표했습니다.



### 01-2. 코랩과 주피터 노트북
###### 노트북, 주피터 노트북
**노트북**은 프로그램 안에서 텍스트와 프로그램 코드를 자유롭게 작성할 수 있는 에디터 입니다.

**셀**은 노트북에서 실행할 수 있는 최소 단위입니다. 셀 안에 있는 내용을 한번에 실행하고 그 결과를 노트북으로 나타냅니다. 기존에 실행된 셀 내용이 다른 셀에서 실행할때 영향을 끼친다는 것이 노트북의 큰 특징입니다.


###### 구글 코랩

**구글 코랩**은 주피터 노트북을 커스터마이징 웹 브라우저에서 무료로 파이썬 프로그램을 테스트하고 저장할 수 있는 서비스 입니다. 클라우드 기반의 주피터 노트북 개발 환경을 가지고 있어 자신의 컴퓨터의 사양이 좋지 않아도 프로그램을 돌릴 수 있어 유용합니다. 

<br><br>
### 01-3. 마켓과 머신러닝
###### 개념 관련
머신러닝에서 여러 개의 종류(또는 클래스)에서 한 곳으로 구별해주는 것을 **분류** 라고 합니다. 생선분류 예제에서는 2개의 클래스로 나뉘는데, 이러한 경우를 **이진분류**라고 하기도 합니다.
**특성**은 데이터의 특징으로, 머신러닝은 이 특성을 이용해 분류를 할 수 있습니다. 그래프(산점도 그래프)가 일직선에 가까운 형태로 나타나는 경우를 **선형**적이라고 부릅니다.
<br><br>
###### matplotlib
**matplotlib**는 파이썬에서 과학계산용 그래프를 그리는 대표적인 패키지입니다. 
<br>다음과 같은 방식으로 산점도를 그릴 수 있습니다
```python
import matplotlib.pyplot as plt

plt.scatter(bream_length, bream_weight) #점의 분포를 산점도로 그려줍니다.
plt.scatter(smelt_length, smelt_weight) #한번 더 명령을 할 경우 그래프에서는 각각 다른 색깔로 산점도를 표시해줍니다.
plt.xlabel('length') #x축에 해당 문자를 출력합니다.
plt.ylabel('weight') #y축에 해당 문자를 출력합니다.
plt.show() #설정한 그래프를 보여줍니다. 일반 파이썬에 경우 새창이 띄워지지만 노트북의 경우 셀 밑에 그래프가 표시됩니다.
```
<br><br>
###### k 최근접 아웃(k-Nearest Neighbors) 알고리즘
k-최근접 이웃 알고리즘(k-Nearest Neighbors)은 어떤 데이터에 대한 답을 구할 때 주위의 다른 데이터를 보고 **다수를 차지하는** 것을 정답으로 채택하는 알고리즘입니다. 
매우 단순하고, 훈련 단계가 빠르다는 장점이 있지만, 데이터가 많아질 경우에는 메모리가 많이 필요해지고, 거리 계산량이 많아서 속도가 느려지기에 적합하지 않습니다. 주로 수치 기반 데이터 분류 작업에서 쓰입니다.
<br>사이킷런(Scikit-learn) 패키지에서는 k-최근접 이웃 알고리즘을 구현한 클래스인 **KNeighborsClassifier**가 있습니다.
<br>이 클래스에는 **n_neighbors**라는 매개변수가 있으며(기본값 5) 이 매개변수는 데이터 예측을 할 때 가장 가까운 거리의 데이터들을 몇개까지 참고하는지 나타내는 매개변수 입니다.

```python
from sklearn.neighbors import KNeighborsClassifier
kn - KNeighborsClassifier()

kn.fit(data,target) #훈련
kn.score(data,target) #정확도

kn.predict(data) #예측
```
모델에 데이터를 전달하여 규칙을 학습하는 과정을 **훈련**이라고 합니다. 여기에서는 fit( ) 메서드로 알고리즘을 훈련합니다.
score( ) 메서드는 사이킷런에서 모델을 평가하는 메서드로 정확도((예측이 맞은 데이터 개수/총 데이터 개수)*100)를 출력합니다.
predict( ) 메서드는 새로운 데이터의 정답을 예측하는 메소드입니다.

***
## 챕터 2 데이터 다루기  (생선분류예제🐟)
###### 지도학습 비지도학습 강화학습
**지도학습**은 정답이 존재하는 데이터를 활용하여 데이터를 학습시키는 걸 말합니다. 
<br>반면, **비지도학습**은 정답이 존재하지 않는 데이터를 비슷한 집단끼리 군집화하여 새로운 데이터에 대한 예측을 이끌어내는 걸 말합니다.
<br>**강화학습**은 분류할 수 있는 데이터도 없고 정답도 존재하지 않는 상황에서 자기가 행동한 상황에 대해 보상을 받는 식으로 학습하는걸 말합니다.
<br><br>지도학습에서는 데이터와 정답을 각각 **입력(input)** 과 **타깃(target)** 이라 부르고 , 이 둘을 합쳐 **훈련 데이터** 라고 부릅니다.

### 02-1. 훈련세트와 테스트세트
데이터가 주어질때 우리는 이 데이터를 이용하여 모델을 만들고, 테스트와 훈련을 하여야 합니다. 
하지만 모든 데이터를 훈련을 넣고 모든 데이터를 넣고 테스트 해보는 것은 학습이 되어있는 데이터를 다시 처리하는 것 뿐이므로 성능을 제대로 평가 할 수 없습니다. 
<br>그래서 우리는 이 데이터를 훈련하는 데이터(train set)와 테스트하는 데이터(test set)로 나누어 모델 평가를 해야 합니다.


###### 샘플링 편향

훈련 데이터와 테스트 데이터에는 다른 특징을 가지는 데이터들이 골고루 섞여져 있어야 합니다. 
골고루 섞여져 있지 않은 경우, 샘플링이 한쪽으로 치우치게 되어 훈련을 할 때 한쪽에 편향된 결과가 만들어져 정확도에 영향을 끼칠수 있습니다. 이를 **샘플링 편향**이라고 부릅니다.

###### numpy
**numpy**는 파이썬의 대표적인 배열 라이브러리로, 파이썬의 리스트와 달리 속도가 빠르고 고차원의 배열도 손쉽게 조작할 수 있어 머신러닝을 할때 많이 쓰이는 라이브러리입니다.

```python
import  numpy as np #보통 np로 많이 바꿈

test_array = np.array(some_list) #리스트 값을 바로 넘파이 배열로 손쉽게 바꿀 수 있습니다.
print(test_array.shape) #배열의 크기를 알려줍니다
```
numpy는 1개의 인덱스가 아닌 여래개의 인덱스로 한번에 선택 할 수 있는 **배열 인덱싱**을 제공합니다.

```python
print(test_array[[1,3]]) #1,3번째 값이 출력됩니다.
print(test_array[index_array[:35]]) #인덱스 안에 배열을 넣어서 한꺼번에 추출 할 수 있습니다.
print(test_array[:,0]) #2차원 배열은 행과 열 인덱스를 콤마(,)로 나누어 지정합니다.
# 콤마(,)는 슬라이싱 연산자로서 처음부터 마지막 원소까지 모두 선택하는 경우 시작, 종료 인덱스를 생략 할 수 있습니다.
```

### 02-2. 데이터 전처리 
###### 사이킷런으로 데이터 나누기 
위 단원에서는 numpy를 이용해 훈련세트와 테스트 세트를 나누었습니다. Scikit-learn을 이용하면 더 편리하게 훈련세트와 테스트세트를 나눌 수 있습니다.

```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(some_data, some_target, random_state=42)
```
이 함수는 기본적으로 25%정도를 테스트 세트로 떼어냅니다. strattify 매개변수를 이용하여 클래스 비율에 맞게 데이터를 나누는 기능도 있습니다.

###### 표준점수를 이용한 데이터 전처리

우리가 실습한 k 최근접 아웃(k-Nearest Neighbors)알고리즘은 말 그래도 각 측정값들의 거리를 측정해서 거리 순서대로 판단을 하는 것이기에 거리에 민감하다.
하지만 각 특성값들의 사이즈간의 차이가 큰 경우(예를 들면 한 특성은 변화가 1~2내외인데 다른 특성값은 변화가 100단위가 되는 경우), 알고리즘에서 한 특성값을 아예 무시하고 판단하는 일까지 일어난다.
이러기 위해서는 각 특성값들이 같은 영향을 끼치도록 하는 **데이터 전처리**가 필요하다.
<br>여기서 **데이터 전처리**는 머신러닝 모델에 훈련 데이터를 주입하기 전애 가공하는 단꼐를 말한다.
<br><br>각 특성값의 차이 변화를 가장 널리 사용하는 전처리 방법 중 하나인 **표준점수**를 이용하여 같은 영향을 끼치게 만들 수 있다.
<br>Numpy는 표준편차는 구하는 함수를 모두 제공합니다.
````python
mean = np.mean(train_input, axis=0) #행을 따라(axis=0) 각 열의 평균값을 계산합니다.
std = np.std(trian_input, axis=0) #행을 따라(axis=0) 각 열의 표준편차를 계산합니다.

````

