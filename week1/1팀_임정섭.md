## 혼공머신 1장, 2장 정리

### 1\. 인공지능 / 머신러닝 / 딥러닝 이란?

#### 인공지능

인공지능은 사람처럼 학습하고 추론할 수 있는 지능을 가진 컴퓨터 시스템을 만드는 기술이다. 인공지능은 과거 암흑기를 거쳐 최근 컴퓨터 하드웨어의 발전으로 다시 한 번 각광받기 시작한다. 인공지능에는 강인공지능(인공일반지능)과 약인공지능으로 구분할 수 있다. 강인공지능은 사람과 구별하기 어려운 높은 레벨의 시스템이고, 약인공지능은 특정 분야에 특화되어 보조 역할을 하는 시스템이다.

#### 머신러닝

머신러닝은 규칙을 사람이 일일이 정하지 않고 자동으로 데이터에서 규칙을 학습하는 알고리즘을 연구하는 분야이다. 인공 지능의 하위 분야인 지능을 구현하기 위한 핵심 분야이다.

#### 딥러닝

딥 러닝은 학습하는 많은 방법 중 하나(머신 러닝의 하위 개념)이다. 딥 러닝은 신경망 학습으로, 인간이 결론을 내리는 방식과 유사한 논리 구조를 사용하여 데이터를 지속적으로 분석한다.

---

### 2\. 왜 머신러닝인가?

그렇다면 왜 머신러닝을 이용하여 규칙을 찾아야할까? '사람이 직접 규칙을 입력해서 프로그래밍을 만들면 되는 것 아닌가'라는 생각을 할 수 있지만 이런 경우 두 가지 단점이 생긴다.

- 결정에 필요한 로직이 한 분야 or 작업에 국한됨.
- 규칙을 설계하려면 그 분야 전문가들이 내리는 결정 방식에 대해 잘 알아야 한다.

얼굴 인식을 예시로 들 수 있다. 컴퓨터가 인식하는 픽셀 방식이 사람이 얼굴을 인식하는 방법과 매우 다르기 때문에 사람이 직접 디지털 이미지에서 얼굴을 구성하는 것이 무엇인지를 일련의 규칙으로 표현하기가 근본적으로 불가능하다. 그러나 머신 러닝을 사용하여 알고리즘에 많은 얼굴 이미지를 제공하면 얼굴을 특정 요소(규칙)가 무엇 인지를 찾아낼 수 있다.

---

### 3\. 지도학습 과 비지도 학습

#### 지도학습

지도학습이란 정답을 알려주면서 진행하는 학습이다. 학습 시 데이터와 함께 레이블(정답)이 항상 제공되어야 한다. 지도 학습의 예로는 분류와 회귀가 대표적이다. 대표적인 알고리즘으로 KNN , SVM, 선형 회귀, 로지스터 회귀, 신경망, 결정 트리가 있다.

**장점**

테스트할 때 데이터와 함께 제공하기 때문에 손쉽게 모델의 성능을 평가할 수 있다.

**단점**

데이터마다 레이블을 달기 위해 많은 시간을 투자해야한다.

#### 비지도학습

비지도학습이란 레이블 없이 데이터만 입력하여 진행하는 학습이다. 보통 데이터 자체에서 패턴을 찾아낼 때 사용한다. 비지도학습의 대표적인 예로는 군집화와 차원축소가 있다. 대표적인 알고리즘으로는 K-Mean, 주성분 분석이 있다.

**장점**

레이블을 제공할 필요가 없다.

**단점**

레이블이 없기 때문에 모델 성능을 평가하는 데에 어려움이 있다.

---

### 4\. 머신러닝 예제 (KNN을 이용한 생선 분류)

#### Colab 시작하기

Colab은 구글에서 제공하는 서비스로, 웹브라우저에서 무료로 파이썬 프로그램을 테스트하고 저장할 수 있다. 이는 대화식 프로그래밍 환경인 주피터 노트북을 커스터마이징한 것이다. 구글의 클라우드와 연결되어 있기 때문에 초기 세팅(라이브러리 설치 등)이 필요 없고 컴퓨터 성능과 상관없이 프로그램을 실습할 수 있다.

Colab 에는 코드 셀과 텍스트 셀을 지원한다. 코드 셀은 말 그대로 코드를 입력해서 바로 실행할 수 있다. (셀은 코랩에서 실행할 수 있는 최소 단위) 그리고 텍스트 셀은 마크다운 형식으로 주석처럼 사용가능하다.

#### KNN이란?

KNN 알고리즘은 가장 간단한 머신러닝 알고리즘으로, 훈련 데이터셋을 저장하여 모델을 만드는 과정이 전부이다. 새로운 입력 데이터에 대해 예측할 때, 알고리즘이 훈련 데이터셋에서 가장 가까운 데이터 포인터, '최근접 이웃'을 찾는다.

#### 예제 데이터 준비하기

```
# 도미 Data
#특성1. 길이
bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0,
                31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0,
                35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]
#특성2. 무게
bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0,
                500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0,
                700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0]
# 빙어 Data
smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]
```

\* [데이터 출처](http://bit.ly/bream_list)

예제의 데이터는 도미와 빙어 2 클래스(품종)로 이루어졌고 특성으로는 길이와 무게이다. (도미 sample 35개, 빙어 sample 14개)

#### Matplotlib 으로 데이터 시각화하기

맷플롯립 라이브러리를 이용하여 산점도 그래프를 그려보자.

```
import matplotlib.pyplot as plt
# 산점도 그래프 생성
plt.scatter(bream_length, bream_weight)
plt.scatter(smelt_length, smelt_weight)
# 예측 데이터 추가
plt.scatter(25, 500, marker='^')
# x축
plt.xlabel('length')
# y축
plt.ylabel('weight')
plt.show()
```

#### 데이터 다루기

```
length = bream_length + smelt_length
weight = bream_weight + smelt_weight

fish_data = [[l, w] for l,w in zip(length,weight)]
print(fish_data)
```

파이썬의 리스트를 이용하여 도미와 빙어 길이와 무게 데이터를 각각 합친다. zip() 메서드는 나열된 리스트에서 원소를 하나씩 꺼내준다.

#### 레이블 만들기

```
# 1 : 도미 / 2: 빙어
fish_target = [1]*35 + [0]*14
print(fish_target)
```

---

이제 본격적으로 sklearn 의 knn 알고리즘을 이용하여 데이터를 훈련시키고 예측해보자.

#### 모델 생성 / 데이터 훈련 / 성능 측정 / 데이터 예측

```
from sklearn.neighbors import KNeighborsClassifier
# KNN 객체 생성
kn = KNeighborsClassifier()
# fit() : 주어진 데이터로 알고리즘을 훈련
kn.fit(fish_data, fish_target)
# score() : 0~1 사이의 값 반환
kn.score(fish_data, fish_target)
# predict
kn.predict([[30,600]]) , kn.predict([[10,100]]) # (array([1]), array([0]))
```

KNeighborsClassifier 객체는 KNN 알고리즘을 사용하기 위한 다양한 메서드를 포함하고 있다. 대표적으로 fit(), score(), predict() 등이 있다.

#### n_neighbors

KNN은 최근접 이웃을 찾을 때 n_neighbors를 통해 이웃을 몇 개나 찾을지 정할 수 있다. default 값은 5이다. n_neighbors 가 49(전체 데이터 수) 인 모델을 만들어보자.

```
# n_neighbors
kn49 = KNeighborsClassifier(n_neighbors=49)
kn49.fit(fish_data, fish_target)
print(kn49.score(fish_data, fish_target))
print(35/49)
```

n-neighbors=49 로 정하면 전체 데이터 수와 같기 때문에 데이터를 예측할 때 항상 도미로 분류한다.

---

#### 훈련 세트 & 테스트 세트

위 예제에서는 데이터 셋 구분 없이 전체 데이터를 학습시키고 정확도를 평가했다. 여기서 문제가 발생하는데, 훈련시킨 데이터를 테스트 데이터로 사용했기 때문에 정확성을 신뢰할 수 없다. (마치 정답을 미리 알려주고 시험을 보는 것과 같다.) 또한, 전체 데이터로 훈련시키면 정확한 평가를 위해 전체 데이터 외에 추가적인 데이터가 필요하다.

그래서 데이터 셋이 있을 때 훈련 세트(데이터)와 테스트 세트(데이터)로 나눠서 모델을 생성하고 평가한다. 일반적으로 훈련 세트는 데이터 셋의 75%, 테스트 세트는 데이터 셋의 25% 로 설정하여 진행한다.

#### Numpy 을 이용한 데이터 다루기

이번에는 Numpy의 ndarray 를 이용해 데이터를 만들어보자.

```
import numpy as np
input_arr = np.array(fish_data)
target_arr = np.array(fish_target)
print(input_arr)
print(target_arr)
```

#### 샘플링 편향

위 데이터는 index 0

~34 까지는 도미 데이터, 35~

48 까지는 빙어 데이터로 되어있다. 이처럼 훈련 세트와 테스트 세트에 샘플이 골고루 섞여 있지 않으면 샘플링이 한쪽으로 치우친 것을 샘플링 편향이라고 한다. 위 데이터는 샘플링 편향이 있기 때문에 앞 데이터를 가지고 훈련하고 뒤에 있는 데이터로 테스트하면 결과값이 도미로만 출력되는 문제가 있다. 이러한 문제를 해결하기 위해 무작위로 데이터를 섞을 필요가 있다. numpy 의 random 클래스를 이용해서 도미와 빙어가 골고루 섞인 데이터를 만들어보자.

```
# random 으로 data 뽑기
# seed 값 설정 시 일정한 random 값 생성
np.random.seed(3)
index = np.arange(49)
np.random.shuffle(index)
print(index)
```

#### 훈련 데이터 / 테스트 데이터 생성 및 성능 평가

우선, 훈련 데이터 개수를 35개, 테스트 데이터를 14개로 설정하겠습니다.

```
# train_data 생성하기
train_input = input_arr[index[:35]]
train_target = target_arr[index[:35]]

# test_data 생성하기
test_input = input_arr[index[35:]]
test_target = target_arr[index[35:]]
```

다음으로 훈련 데이터와 테스트 데이터를 시각화 해보자.

```
plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(test_input[:,0], test_input[:,1])
plt.show()
```

이제 모델 성능 테스트를 해보자.

```
# KNN 알고리즘으로 test_data 대입해보기
kn = kn.fit(train_input, train_target)
kn.score(test_input,test_target)
```

1.0 이라는 100% 정확성 결과를 얻었다.

---

### 5\. 데이터 전처리

#### 사이킷런으로 훈련 데이터와 테스트 데이터 나누기

위에서 random 함수를 이용하여 훈련 데이터와 테스트 데이터를 나눴다. 이 기능을 사이킷런에서 train_test_split() 함수를 이용하면 간단하게 훈련 데이터와 테스트 데이터를 나눌 수 있다.

```
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, random_state = 42)
print(train_input, test_input, train_target, test_target)
```

return 순서로는 train_input(훈련 데이터), test_input(테스트 데이터), train_target(훈련 라벨), test_target(테스트 라벨) 순이다.

#### Kneighbors 확인하기

데이터 시각화

```
kn = KNeighborsClassifier()
# train
kn.fit(train_input, train_target)
# test
kn.score(test_input, test_target)
# sample predict
print(kn.predict([[25,150]]))
plt.scatter(train_input[:, 0], train_input[:,1])
plt.scatter(25, 150, marker='^')
plt.show()
```

```
# 최근접 이웃의 거리 및 index
distances, indexes = kn.kneighbors([[25, 150]])
print(indexes)
print(distances)
```

kneighbors 를 이용하면 해당 샘플의 최근접 이웃과 그 거리 값을 얻을 수 있다.

```
plt.scatter(train_input[:, 0], train_input[:,1])
plt.scatter(25, 150, marker='^')
# 최근접 이웃 표시
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')
plt.show()
```

최근접 이웃을 확인했을 때 이상한 점을 확인할 수 있을 것이다. knn은 기본적으로 거리를 계산하는데 x축과 y축의 단위(스케일)가 다르기 때문에 이러한 문제가 발생했다. 그렇기 때문에 우리는 두 특성의 값을 일정한 기준으로 맞춰야 한다. 이를 데이터 전처리라 한다.

데이터 전처리에서 많이 사용하는 방법 중 하나는 표준점수이다.

> 표준 점수 = 데이터 값 - 평균 / 표준 편차

표준 점수를 이용하여 데이터 전처리를 진행해보자.

Numpy 의 평균, 표준편차 함수를 이용하여 값을 구한다.

```
#평균, 표준편차
mean = np.mean(train_input, axis=0)
std = np.std(train_input, axis=0)

print(mean, std)
```

```
# 전처리 데이터
train_scaled  = (train_input - mean) / std
```

Numpy의 브로드 캐스팅으로 기존 데이터를 표준 점수로 바꿔준다.

이를 시각화 해보자.

```
new_sample_scaled = ([25, 150] - mean) / std
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new_sample_scaled[0], new_sample_scaled[1])
plt.show()
```

x축과 y축의 스케일이 변경된 것을 확인할 수 있다.

이제 최근접 이웃을 확인해보자.

```
distances, indexes = kn.kneighbors([new_sample_scaled])
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new_sample_scaled[0], new_sample_scaled[1], marker='^')
plt.scatter(train_scaled[indexes, 0], train_scaled[indexes, 1], marker='D')
plt.show()
```

이제 우리가 생각했던 것처럼 최근접 이웃을 선택한 것을 확인할 수 있다.

---

### 6\. 결정경계

가능한 모든 테스트 포인트의 예측을 xy 평면에 나타내어 각 포인트가 속한 클래스에 따라 평면에 색을 칠한다. 이를 결정 경계라 한다.

```
import mglearn
mglearn.plots.plot_2d_separator(kn, train_scaled, fill=True, eps=0.5, alpha=0.5)
mglearn.discrete_scatter(train_scaled[:,0], train_scaled[:,1], train_target)
plt.scatter(new_sample_scaled[0], new_sample_scaled[1], marker='D')
plt.show()
```

\* colab 에서 mglearn 을 import 하는 방법이 궁금하면 다음 [글](https://noapps-code.tistory.com/202)을 참고하길 바란다.

---

### 7\. 최적의 k 찾기

```
# 훈련 정확도와 테스트 정확도를 저장할 리스트 선언
training_accuracy = []
testing_accuracy = []

# neighbor 1~37 세팅
neighbor_settings = range(1,37)
for n_neighbor in neighbor_settings:
  kn = KNeighborsClassifier(n_neighbors=n_neighbor)
  kn.fit(train_input, train_target)
  training_accuracy.append(kn.score(train_input, train_target))
  testing_accuracy.append(kn.score(test_input, test_target))
plt.plot(neighbor_settings, training_accuracy, label = '훈련 정확도')
plt.plot(neighbor_settings, testing_accuracy, label = '테스트 정확도')
plt.show()
print(f'훈련 데이터에서 최적의 k 는 {training_accuracy.index(max(training_accuracy)) + 1}')
print(f'테스트 데이터에서 최적의 k 는 {testing_accuracy.index(max(training_accuracy)) + 1}')
```
