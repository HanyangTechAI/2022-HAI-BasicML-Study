## 인공 신경망

### 1\. 인공 신경망 이론

#### 인공 신경망이란?

인공 신경망은 생물학적 뉴런에서 영감을 받아 만든 머신러닝 알고리즘이다. 신경망은 기존의 머신러닝 알고리즘으로 다루기 어려운 이미지, 음성, 텍스트 분야에서 뛰어난 성능을 발휘한다. 최근에는 인공 신경망 알고리즘을 종종 딥러닝이라고 부른다.

#### 텐서플로란?

텐서플로는 구글이 만든 딥러닝 라이브러리이다. CPU 와 GPU를 사용해 인공 신경망 모델을 효율적으로 훈련하며 모델 구축과 서비스에 필요한 다양한 도구를 제공한다. 신경망 모델을 빠르게 구성할 수 있는 케라스를 핵심 API로 채택하여 간단한 모델부터 복잡한 모델까지 손쉽게 만들 수 있다.

#### 밀집층이란?

가장 간단한 인공 신경망의 층이다. 밀집층에서는 뉴런들이 모두 연결되어 있기 때문에 완전 연결층이라고도 한다. 특별히 출력층에 밀집층을 사용할 때는 분류하려는 클래스와 동일한 개수의 뉴런을 사용한다.

#### 원-핫 인코딩이란?

원-핫 인코딩은 정숫값 배열에서 해당 정수 위치의 원소만 1이고 나머지는 모두 0으로 변환한다. 이 같은 변환이 필요한 이유는 다중 분류에서 출력층에서 만든 확률과 크로스 엔트로피 손실을 계산하기 위해서이다. 

---

### 2\. 텐서플로를 이용한 인공 신경망 예제

#### 데이터 셋

데이터 셋으로 패션 MNIST 데이터셋을 사용한다. 

```
import numpy as np
from tensorflow import keras

(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()
print(train_input.shape, train_target.shape) # (60000, 28, 28) (60000,)
print(test_input.shape, test_target.shape) # (10000, 28, 28) (10000,)
print(np.unique(train_target, return_counts=True))
'''
(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000]))
'''
```

\*load\_data() 메서드는 훈련 데이터 와 테스트 데이터를 분리하여 반환한다. 훈련, 테스트 데이터는 28x28 size로 각각 60000, 10000 개의 데이터로 이루어져 있다. 데이터 종류는 10가지이다.

#### 로지스틱 회귀로 패션 아이템 분류하기

```
from sklearn.model_selection import cross_validate
from sklearn.linear_model import SGDClassifier

# 픽셀 0 ~ 255 -> 0 ~ 1 사이 값으로 변환
train_scaled = train_input / 255.0
# SGDClassifier는 1차원 배열을 매개변수로 받기 때문에 2차원 배열 -> 1차원 배열로 변환
train_scaled = train_scaled.reshape(-1, 28*28)

sc = SGDClassifier(loss='log_loss', max_iter=5, random_state=42)

scores = cross_validate(sc, train_scaled, train_target, n_jobs=-1)
print(np.mean(scores['test_score']))
'''
0.8196000000000001
'''
```

SGDClassifier 모델은 10개(클래스 개수)의 방정식에 대한 모델 파라미터(가중치와 절편)을 찾는다. 즉, 총 784개(픽셀 개수)의 가중치와 절편값을 구한다. 

#### 인공 신경망으로 모델 만들기

이제 본격적으로 텐서플로를 이용해 인공 신경망 모델을 만들어보자.

```
from sklearn.model_selection import train_test_split

train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size=0.2, random_state=42)
```

\* 훈련 세트에서 20%를 검증세트로 분리했다.

```
dense = keras.layers.Dense(10, activation='softmax', input_shape=(784,))
```

첫 번째 매개변수로는 뉴런 개수를 지정한다. 패션 데이터셋의 클래스가 10개이기 때문에 뉴런 개수를 10으로 설정했다. 

두 번째 매개변수는 활성화 함수를 지정한다. 다중 분류에 사용할 소프트 맥스 함수로 설정했다. (이진 분류에서는 sigmod 로 설정하면 된다.)

세 번째 매개변수는 입력값의 크기를 지정한다. 데이터의 size가 28x28 = 784 이기 때문에 (784,)로 설정했다.

```
model = keras.Sequential(dense)
```

keras.Sequential 클래스로 dense 객체를 넘겨주면 신경망 모델이 생성된다.

#### 인공 신경망으로 패션 아이템 분류하기

```
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')
```

케라스 모델은 훈련하기 전에 손실함수 종류와 계산하고 싶은 측정값을 지정해야 한다.

```
model.fit(train_scaled, train_target, epochs=5)
'''
Epoch 1/5
1500/1500 [==============================] - 3s 1ms/step - loss: 0.6058 - accuracy: 0.7932
Epoch 2/5
1500/1500 [==============================] - 2s 1ms/step - loss: 0.4785 - accuracy: 0.8385
Epoch 3/5
1500/1500 [==============================] - 2s 1ms/step - loss: 0.4564 - accuracy: 0.8471
Epoch 4/5
1500/1500 [==============================] - 2s 1ms/step - loss: 0.4435 - accuracy: 0.8539
Epoch 5/5
1500/1500 [==============================] - 2s 2ms/step - loss: 0.4358 - accuracy: 0.8551
<keras.callbacks.History at 0x7f329a2c34c0>
'''
```

 에포크마다 걸린 시간과 손실, 정확도를 출력한다. 

```
model.evaluate(val_scaled, val_target)
'''
375/375 [==============================] - 1s 1ms/step - loss: 0.4579 - accuracy: 0.8483
[0.45794257521629333, 0.8483333587646484]
'''
```

검증 세트로 모델을 평가해본 결과 훈련 세트의 점수보다 조금 낮은 84%의 정확도를 보여준다.

## 심층 신경망

### 1\. 심층 신경망 이론

#### 심층 신경망이란?

2개 이상의 층을 포함한 신경망이다. 다중 인공 신경망, 심층 신경망. 딥러닝은 같은 의미이다.

#### 렐루 함수

이미지 분류 모델의 은닉층에 많이 사용하는 활성화 함수이다. 시그모이드 함수는 층이 많을수록 활성화 함수의 양쪽 끝에서 변화가 작기 떄문에 학습이 어렵다. 렐루 함수는 계산이 간단하며 양쪽 끝 변화의 문제가 없다.

#### 옵티마이저

신경망의 가중치와 절편을 학습하기 위한 알고리즘 또는 방법을 말한다. 케라스에는 다양한 경사 하강법 알고리즘이 구현되어 있다. 대표적으로 SGD, Adam, 네스테로프 모멘텀, Adagard, RMSprop 등이 있다.

---

### 2\. 심층 신경망 예제

#### 데이터 셋

인공 신경망에서 사용한 패션 데이터셋과 동일하다.

```
from tensorflow import keras
from sklearn.model_selection import train_test_split


(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()

# 데이터 전처리
train_scaled = train_input / 255.0
train_scaled = train_scaled.reshape(-1, 28*28)

train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size=0.2, random_state=42)
```

#### 2개의 층

```
dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(784,))
dense2 = keras.layers.Dense(10, activation='softmax')
```

dense1은 활성화 함수를 sigmoid 함수로 설정한 출력층이다.

dense2은 활성화함수를 softmax 함수로 설정한 은닉층이다.

#### 심층 신경망 만들기

```
model = keras.Sequential([dense1, dense2])
# summary() : 모델의 정보를 요약해서 출력
model.summary()
'''
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 100)               78500     
                                                                 
 dense_1 (Dense)             (None, 10)                1010      
                                                                 
=================================================================
Total params: 79,510
Trainable params: 79,510
Non-trainable params: 0
_________________________________________________________________
'''
```

첫 번째 층인 출력층에서는 100개의 노드, 은닉층에서는 10개의 노드로 이루어져 있다. 총 파라미터 개수는 79510 이다.

훈련 결과는 다음과 같다. 

```
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')
model.fit(train_scaled, train_target, epochs=5)
'''
Epoch 1/5
1500/1500 [==============================] - 4s 2ms/step - loss: 0.5628 - accuracy: 0.8073
Epoch 2/5
1500/1500 [==============================] - 3s 2ms/step - loss: 0.4075 - accuracy: 0.8522
Epoch 3/5
1500/1500 [==============================] - 3s 2ms/step - loss: 0.3741 - accuracy: 0.8652
Epoch 4/5
1500/1500 [==============================] - 4s 2ms/step - loss: 0.3509 - accuracy: 0.8732
Epoch 5/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.3335 - accuracy: 0.8784
<keras.callbacks.History at 0x7fa888270250>
'''
```

#### 렐루 활성화 함수

```
# add() : 모델에 층을 추가하는 메서드
model = keras.Sequential()
# Flatten : 입력 데이터의 차원을 일렬로 변환하는 층
model.add(keras.layers.Flatten(input_shape=(28, 28)))
model.add(keras.layers.Dense(100, activation='relu'))
model.add(keras.layers.Dense(10, activation='softmax'))
```

활성화 함수를 relu 로 설정했다. 

훈련 결과는 다음과 같다.

```
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')

model.fit(train_scaled, train_target, epochs=5)
'''
Epoch 1/5
1500/1500 [==============================] - 5s 3ms/step - loss: 0.5336 - accuracy: 0.8090
Epoch 2/5
1500/1500 [==============================] - 3s 2ms/step - loss: 0.3934 - accuracy: 0.8579
Epoch 3/5
1500/1500 [==============================] - 3s 2ms/step - loss: 0.3554 - accuracy: 0.8717
Epoch 4/5
1500/1500 [==============================] - 4s 2ms/step - loss: 0.3337 - accuracy: 0.8799
Epoch 5/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.3181 - accuracy: 0.8859
<keras.callbacks.History at 0x7fa8801e1d30>
'''
```

앞선 입력층과 출력층만으로 이루어진 인공 신경망보다 더 높은 정확성을 보인다. 

---

### 3\. 옵티마이저 예제

#### SGD

```
sgd = keras.optimizers.SGD()
# 적응적 학습률 적용
sgd = keras.optimizers.SGD(learning_rate=0.1)
# 네스테로프 모멘텀 : 모멘텀 최적화 2번 진행
sgd = keras.optimizers.SGD(momentum=0.9, nesterov=True)
model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics='accuracy')
```

#### Adagrad

```
adagrad = keras.optimizers.Adagrad()
model.compile(optimizer=adagrad, loss='sparse_categorical_crossentropy', metrics='accuracy')
```

#### Rmsprop

```
rmsprop = keras.optimizers.RMSprop()
model.compile(optimizer=rmsprop, loss='sparse_categorical_crossentropy', metrics='accuracy')
```

## 신경망 모델 훈련

### 1\. 신경망 모델 훈련 이론

#### 드롭아웃 

은닉층에 있는 뉴런의 출력을 랜덤하게 껴서 과대적합을 막는 기법이다. 드롭아웃은 훈련 중에 적용되며 평가나 예측에서는 적용하지 않는다. 텐서플로는 이를 자동적으로 처리한다.

#### 콜백 

케라스 모델을 훈련하는 도중에 어떤 작업을 수행할 수 있도록 도와주는 도구이다. 대표적으로 최상의 모델을 자동으로 저장해 주거나 검증 점수가 더 이상 향상되지 않으면 일찍 종료할 수 있다.

#### 조기 종료

검증 점수가 더 이상 감소하지 않고 상승하여 과대적합이 일어나면 훈련을 계속 진행하지 않고 멈추는 기법이다.

---

### 2\. 신경망 모델 훈련 예시

#### 손실 곡선

```
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')

history = model.fit(train_scaled, train_target, epochs=5, verbose=0)
print(history.history.keys())
'''
dict_keys(['loss', 'accuracy'])
'''
```

```
import matplotlib.pyplot as plt

plt.plot(history.history['loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
```

[##_Image|kage@sGlPZ/btrCxHjDvtV/AqNYpe8KYGTYAb7TqLjiQ0/img.png|CDM|1.3|{"originWidth":392,"originHeight":262,"style":"alignCenter"}_##]

```
plt.plot(history.history['accuracy'])
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()
```

[##_Image|kage@o0jlR/btrCyYMDs1v/PoZdp3XiTmaaqJUaKAP1kK/img.png|CDM|1.3|{"originWidth":392,"originHeight":266,"style":"alignCenter"}_##]

#### 검증 손실

```
model = model_fn()
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')

history = model.fit(train_scaled, train_target, epochs=20, verbose=0, 
                    validation_data=(val_scaled, val_target))
print(history.history.keys())
'''
dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])
'''
```

```
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()
```

[##_Image|kage@b2u8nT/btrCCE6J2dj/WkiuD7UnSxuKz2sVEQV2cK/img.png|CDM|1.3|{"originWidth":392,"originHeight":262,"style":"alignCenter"}_##]

#### 드롭아웃

```
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', 
              metrics='accuracy')

history = model.fit(train_scaled, train_target, epochs=20, verbose=0, 
                    validation_data=(val_scaled, val_target))
```

```
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()
```

---

### **참고 자료**

**\[혼공머신 - 7.1 인공신경망.ipynb\] : [https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/7-1.ipynb#scrollTo=fMTyGYIMqUE9](https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/7-1.ipynb#scrollTo=fMTyGYIMqUE9)**

**[**\[혼공머신 - 7.2 심층 신경망.ipynb\] :**](https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/7-1.ipynb#scrollTo=fMTyGYIMqUE9) **[https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/7-2.ipynb#scrollTo=KkpbSMXWtakr](https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/7-2.ipynb#scrollTo=KkpbSMXWtakr)****

**[**\[혼공머신 - 7.3 신경망 모델 훈련.ipynb\] :**](https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/7-1.ipynb#scrollTo=fMTyGYIMqUE9) **[https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/7-3.ipynb#scrollTo=KkpbSMXWtakr](https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/7-2.ipynb#scrollTo=KkpbSMXWtakr)****