# 퍼셉트론(Perceptron)

- 로지스틱 회귀 : 가중치와 편향을 이용하여 input에 대해 특정 확률을 계산. 
  
- McCulloch-Pitts Neuron : 로지스틱 회귀와 유사하게 선형 방정식을 통해 Yes / No 를 판별.

- Perceptron : McCulloch-Pitts Neuron 과 유사하지만, input이 boolean이 아니어도 동작한다. 퍼셉트론의 약점은 XOR 문제를 해결하지 못한다는 점이며 이는 multi-layer perceptron 으로 해결할 수 있다.
<br/></br>

# Multi-Layer Perceptron(MLP)

- 퍼셉트론을 여러 개 쌓은 것
- 입력층 + 은닉층 + 출력층 으로 구성되어 있다.



## MLP (1) : 데이터로드 및 처리

```python
from tensorflow import keras
from sklearn.model_selection import train_test_split

(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()
train_scaled = train_input / 255.0
train_scaled = train_scaled.reshape(-1, 28*28)

train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size=0.2, random_state=42)
```


## MLP (2) : 모델 생성

```python
# 28*28 크기, grayscale image data 사용

model = keras.Sequential()
model.add(keras.layers.Flatten(input_shape=(28,28)))
model.add(keras.layers.Dense(100, activation='relu'))
model.add(keras.layers.Dense(10, activation='softmax'))
```


## MLP (3.1) : 컴파일 및 훈련

```python
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')
history = model.fit(train_scaled, train_target, epochs=20, verbose=0, 
                    validation_data=(val_scaled, val_target))
```


## MLP (3.2) : callback 추가, 컴파일 및 훈련

```python
# 컴파일은 그대로
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', 
              metrics='accuracy')

# ModelCheckpoint : 가장 낮은 검증 점수의 모델을 자동으로 저장
checkpoint_cb = keras.callbacks.ModelCheckpoint('best-model.h5',
                                                save_best_only=True)

# EarlyStopping : patience 만큼 연속으로 검증 점수 향상되지 않으면 조기 종료.
early_stopping_cb = keras.callbacks.EarlyStopping(patience=2,
                                                  restore_best_weights=True)

history = model.fit(train_scaled, train_target, epochs=20, verbose=0, 
                    validation_data=(val_scaled, val_target),
                    callbacks=[checkpoint_cb, early_stopping_cb])                                                 
```


## MLP (4) : 모델의 저장과 복원

```python
# 모델 저장
# 모델 파라미터만 저장.
model.save_weights('model-weights.h5')

# 모델 구조와 모델 파라미터를 함께 저장.
model.save('model-whole.h5')

# 모델 복원
model_re = keras.models.load_model('model-whole.h5')
```

ModelCheckpoint 에서 이미 가장 좋은 성적을 내는 모델을 저장했기에 그대로 불러올 수 있다.

```python
model_re_best = keras.models.load_model('best-model.h5')
```

# 최적화 문제

## 손실 함수(Loss Function)

- 모델 성능이 얼마나 나쁜지 보여주는 함수이다.

- 이진분류의 경우 binary_crossentropy, 회귀 모델의 경우 MSE 등을 사용한다.

- DNN 에서는 손실값을 줄이는 방향으로 모델의 update가 진행되며 이를 최적화 라고 한다.

## optimizer

- 케라스는 다양한 경사 하강법 알고리즘을 제공하며 이들을 optimizer 라고 한다.

- 모델 최적화를 위해 optimizer 종류를 선정하고 learning rate 를 조정하는 작업이 필요하다.

- 기본 경사 하강법 optimizer : SGD / momentum
- 적응적 학습률 optimizer : RMSprop / Adam / Adagrad (모델이 최적점에 가까이 갈수록 학습률을 낮출 수 있다.)