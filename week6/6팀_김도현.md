# Chapter 07 딥러닝을 시작합니다

## Chapter 07-1 인공 신경망

- 생물학적 뉴런에서 영감을 얻어 만들어진 알고리즘
	-> 인공 뉴런은 생물학적 뉴런의 모양을 본뜬 수학적 모델로, 생물학적 뉴런이 하는 일을 실제로 구현한 것이 아니다.

- 용어정리
	- **밀집층(완전 연결층):** 뉴런들이 모두 연결되어 있는 가장 간단한 인공 신경망의 층
	- **활성화 함수:** 뉴런의 선형 방정식 계산 결과에 적용되는 함수 

- **원-핫 인코딩**
	- 크로스 엔트로피 손실 함수를 사용하기 위해서 타깃값을 해당 클래스는 1로, 나머지 클래스는 0으로 만들어주는 것
	- 텐서플로에서 손실 함수로 'sparse_categorical_entropy'를 사용하면 굳이 해줄 필요는 없다.
---

## Chapter 07-2 심층 신경망

- 용어정리
	- **은닉층:** 입력층과 출력층 사이에 있는 모든 층
	- **옵티마이저:** 신경망의 가중치와 절편을 학습하기 위한 알고리즘 (경사하강법 알고리즘)

- 신경망 구성
	- **입력층 -> 은닉층 -> 출력층**
	- 각 층은 뉴런(유닛)으로 구성되어 있다.
	- 은닉층과 출력층에는 활성화 함수가 사용된다.

- 활성화 함수
	- **은닉층: 렐루 함수**
	-> 은닉층에 시그모이드 함수를 사용하면 함수 양쪽 끝에서 변화율이 작기 때문에 Vanishing Gradient Problem이 발생할 확률이 높아 학습이 어려워진다.
	- **출력층: 시그모이드 함수(이진분류), 소프트맥스 함수(다중분류), 사용X (회귀)**

- 옵티마이저
	- **SGD**
	- **모멘텀:** 이전의 그레디언트를 가속도 처럼 사용하는 **모멘텀 최적화**를 사용
	- **네스테로프 모멘텀:** 모멘텀 최적화를 2번 반복
	- **Adagrad, RMSprop:** 최적점에 가까이 갈수록 학습률을 낮춰 안정적으로 최적점에 수렴하는 적응적 학습률 사용
	- **Adam:** 모멘텀 최적화와 RMSprop의 장점을 접목

---

## Chapter 07-3 신경망 모델 훈련

- **손실 곡선**을 사용하여 훈련 손실과 검증 손실을 비교함으로써 과적합이 일어나지 않게 한다.

- **드롭아웃:** 훈련 과정에서 층에 있는 일부 뉴런을 끄는 (일부 뉴런의 출력이 0으로 만들어주는) 알고리즘
	-> 특정 뉴런에 과대하게 의존하는 것을 막아주고 앙상블과 비슷하게 바라볼 수 있어 과적합을 방지한다.
---
