# 이론 요약
## 로지스틱 회귀

- 로지스틱 회귀는 선형 방정식을 통해 학습하는 분류 모델이다.
> ${z = a*( 특성1 ) + b*( 특성2 ) + ...}$
- 시그모이드 함수를 통해 z 값을 0 과 1 사이로 만든다. 
- ## 시그모이드 함수 :  ${\Phi ={\frac {1}{1+e^-z}}}$


## 로지스틱 회귀 다중 분류

- 로지스틱 회귀 함수는 이진 분류일 때 가장 강력한 성능을 발휘한다.
- 또한, 다중 분류일 때는 시그모이드 함수가 아닌 소프트맥스 함수를 사용한다.


## 소프트맥스 함수 : ${ y_i ={\frac {e^{z_i}}{\sum_{i=1}^K e^{z_i}}}}$
- 소프트맥스 함수는 시그모이드 함수의 일반화 버전이다.
- k 개의 클래스에 대해 확률을 계산하여 분류할 수 있다.


## 손실함수, loss function
- 어떤 문제에서 머신러닝 알고리즘이 얼마나 부정확한지 측정하는 기준
- 머신러닝은 손실 함수 결과가 최솟값이 나오도록 모델을 최적화하는 것이 목표가 된다.
- 로지스틱 손실함수는 '-log(예측확률)' 함수를 사용한다.


## 경사하강법

-  Z = a*X0 + b*X1 + c*X2 + d 라고 할 때, 적절한 a,b,c,d 값을 찾기위한 최적화 알고리즘이다. 
-  샘플 X = [ X0, X1, X2 ] 라고 하면 Z 는 a,b,c,d 에 대한 함수로 표현 가능하고 이 a,b,c,d 값으 바꿔가면서 경사가 최소값일 때를 찾는 알고리즘이다. 

#### 확률적 경사 하강법 (Stochastic gradient descent)
- 랜덤하게 하나의 샘플을 선택해 경사 하강법을 수행하는 방식

#### 미니 배치 경사 하강법
- 여러 개의 샘플을 사용해 경사 하강법을 수행하는 방식 

#### 배치 경사 하강법
- 전체 샘플을 사용해 경사 하강법을 수행하는 방식
<br/></br>

# 혼공머신_예제

## K-최근접 이웃 분류기의 확률 예측

- ['Species'] 를 7개의 클래스로 설정(target_data)
- 특성 5개: 길이, 높이, 두께, 대각선, 무게 (train_data)
- train_test_split 으로 훈련 데이터, 테스트 데이터 분리
- StandardScaler 클래스를 사용해 훈련 세트 및 테스트 세트 표준화 전처리

```python
import pandas as pd
fish = pd.read_csv('https://bit.ly/fish_csv_data')
# unqiue() 함수를 사용해서 어떤 종류의 생선이 있는지 확인
print(pd.unique(fish['Species']))

>>> ['Bream' 'Roach' 'Whitefish' 'Parkki' 'Perch' 'Pike' 'Smelt']

# input data 설정
fish_input = fish[['Weight', 'Length', 'Diagonal','Height','Width']].to_numpy()

# target data 설정
fish_target = fish['Species'].to_numpy()

# 훈련 데이터, 테스트 데이터 분리
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)

# 표준화 전처리
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

```python
# 모델 생성 및 훈련
from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier(n_neighbors=3)
kn.fit(train_scaled, train_target)
```
<br/></br>
## 로지스틱 회귀로 이진 분류 수행

- 데이터는 '도미'와 '빙어' 데이터를 사용.
- 시그모이드 함수의 출력 0.5보다 크면 양성 클래스, 0.5와 같거나 작으면 음성 클래스.

```python
# 전체 데이터에서 boolean indexing 을 통해 도미와 빙어 데이터만 추출
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]

# 로지스틱 회귀 모델 생성 및 훈련
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)
```

```python
# 처음 5개 샘플의 클래스를 분류 및 예측 확률 출력
print(lr.predict(train_bream_smelt[:5]))

>>> ['Bream' 'Smelt' 'Bream' 'Bream' 'Bream']

print(lr.predict_proba(train_bream_smelt[:5]))

>>> [0.99759855 0.00240145]
    [0.02735183 0.97264817]
    [0.99486072 0.00513928]
    [0.98584202 0.01415798]
    [0.99767269 0.00232731]
```
<br/></br>
## 로지스틱 회귀로 다중 분류 수행하기

- 반복 횟수 (max_iter) = 1000 으로 설정
- 계수의 제곱을 규제하는 L2 규제 적용

```python
# 다중 분류 모델 생성 및 훈련
lr = LogisticRegression(C=20, max_iter=1000)    
lr.fit(train_scaled, train_target)
```

```python
# 테스트 세트 처음 5개 샘플에 대한 분류, 예측 확률 출력
print(lr.predict(test_scaled[:5]))

>>> ['Perch' 'Smelt' 'Pike' 'Roach' 'Perch']

# 예측 확률 출력
proba = lr.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=3))

>>> [0.    0.014 0.841 0.    0.136 0.007 0.003]
    [0.    0.003 0.044 0.    0.007 0.946 0.   ]
    [0.    0.    0.034 0.935 0.015 0.016 0.   ]
    [0.011 0.034 0.306 0.007 0.567 0.    0.076]
    [0.    0.    0.904 0.002 0.089 0.002 0.001]
```

```python
# 처음 샘플 5개에 대한 z 값을 소프트맥스 함수에 통과시키기
decision = lr.decision_function(test_scaled[:5])

from scipy.special import softmax
proba = softmax(decision, axis=1)   # 열 기준, 행에 대해 소프트맥스 계산.
print(np.round(proba, decimals=3))

>>> [0.    0.014 0.841 0.    0.136 0.007 0.003]
    [0.    0.003 0.044 0.    0.007 0.946 0.   ]
    [0.    0.    0.034 0.935 0.015 0.016 0.   ]
    [0.011 0.034 0.306 0.007 0.567 0.    0.076]
    [0.    0.    0.904 0.002 0.089 0.002 0.001]
```
<br/></br>
## SGDClassifier

- 이진 분류의 경우, 이진 크로스엔트로피 손실 함수
- 다중 분류의 경우, 크로스엔트로피 손실 함수
- 경사 하강법 예제 데이터의 경우 앞선 7개 클래스에 대한 데이터를 그대로 사용한다.

```python
# SGDClassifier 로드
from sklearn.linear_model import SGDClassifier

# 손실함수: loss='log' 로지스틱, 에포크: max_iter=10
sc =SGDClassifier(loss='log', max_iter=10, random_state=42)
sc.fit(train_scaled, train_target)

# score 출력
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))

>>> 0.773109243697479           // score 가 다소 낮게 나오는 경향이 있다.
    0.775
```
<br/></br>
## epoch와 과대/과소적합

- 확률적 경사 하강법을 사용한 모델은 epoch 횟수에 따라 과대/과소적합 현상이 나타난다.
- epoch 횟수가 적으면 과소적합이 발생하며, epoch 가 너무 많으면 과대적합이 발생한다.
- 과대적합이 시작되기 전 훈련을 멈추는 것을 'early stopping' 이라고 한다.
- 예제에서는 np.unique() 함수로 7개 생선 클래스 목록(=classes) 을 만든다.
- 과대적합이 발생하는 지점을 찾기 위해 epoch 1회 마다 train_score, test_score를 리스트로 logging 한다.
- partial_fit() 메서드를 통해 모델을 훈련한다.

```python
# 모델 생성 및 훈련 준비
import numpy as np
sc = SGDClassifier(loss='log', random_state=42)
train_score = []
test_score = []
classes = np.unique(train_target)

# epochs= 300으로 설정하여 훈련
for _ in range(0,300):
  sc.partial_fit(train_scaled, train_target, classes=classes)
  train_score.append(sc.score(train_scaled, train_target))      # train_score logging
  test_score.append(sc.score(test_scaled, test_target))         # test_score logging

# matplotlib 그래프로 확인
import matplotlib.pyplot as plt
plt.plot(train_score)
plt.plot(test_score)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()
```