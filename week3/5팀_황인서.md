# Chapter 04 다양한 분류 알고리즘
### 04-1 로지스틱 회귀
- Keywords
  - **로지스틱 회귀**: 선형 방정식을 사용한 분류 알고리즘. 시그모이드 함수나 소프트맥스 함수를 통해 클래스 확률을 출력함. 다음과 같은 선형 방정식을 학습함.
<center> $z=a\times(Weight)+b\times(Length)+c\times(Height)+d\times(Width)+e$
  - **다중 분류**: 타깃 클래스가 2개 이상인 분류 문제
  - **시그모이드 함수**: 선형 방정식의 출력을 0과 1 사이의 값으로 압축함. 이진 분류에 사용됨. 0.5보다 크면 양성 클래스, 작으면 음성 클래스임.
<center> $\phi=\frac{1}{1+e^{-z}}$

  - **소프트맥스 함수**: 다중 분류에서 여러 선형 방정식의 출력 결과를 정규화하여 합이 1이 되도록 함.
<center> $e_{sum}=e^{z_1}+e^{z_2}+e^{z_3}+\dots+e^{z_n}$
<center> $s_1=\frac{e^{z_1}}{e_{sum}}, s_2=\frac{e^{z_2}}{e_{sum}},\dots,s_n=\frac{e^{z_n}}{e_{sum}}$
  - **불리언 인덱싱**: numpy 배열에서 True, False 값을 전달하여 행을 선택하는 방법. 아래 예시에서는 'A'와 'C'만 있는 배열이 출력됨.

    ```
    char_arr = np.array(['A', 'B', 'C' , 'D', 'E'])
    print(char_arr[[True, False, True, False, False)
    ```
- Packages and Functions
  - sckit-learn
    - **LogisticRegression**: 로지스틱 회귀를 위한 클래스
    - **predict_proba()**: 이진 분류 혹은 다중 분류의 예측 확률을 반환함.
    - **decision_function()**: LogisticRegression 모델로 z 값을 계산함.
  - pandas
    - **unique()**: 데이터에서 고유한 값을 추출함. 주로 데이터의 종류를 알고 싶을 때 사용함.
  - scipy
    - **expit()**: 시그모이드 함수

### 04-2 확률적 경사 하강법
- Keywords
  - **확률적 경사 하강법**: 훈련 세트에서 무작위로 샘플을 하나씩 선택해 손실 함수의 경사를 따라 최적의 모델을 찾는 알고리즘
  - **에포크**: 확률적 경사 하강법에서 훈련 세트를 한 번 모두 사용하는 과정
  - **미니배치 경사 하강법**: 샘플을 여러 개씩 사용하는 경사 하강법
  - **배치 경사 하강법**: 한 번에 전체 샘플을 사용하는 경사 하강법
  - **손실 함수**: 확률적 경사 하강법이 최적화할 대상. 모델의 성능이 얼마나 나쁜지 측정함. 대표적으로 아래 형태의 **로지스틱 손실 함수**가 있음.
<center> $\begin{cases}
타깃=1: -\log(예측 확률)\\
타깃=0: -\log(1-예측 확률)\\
\end{cases}$
- Packages and Functions
  - scikit-learn
    - **SGD-Classifier**: 확률적 경사 하강법을 사용한 분류 모델을 만듦.
    - **SGDRegressor**: 확률적 경사 하강법을 사용한 회귀 모델을 만듦.
    - **partial_fit()**: 한번 훈련한 모델을 이어서 훈련할 때 사용함.