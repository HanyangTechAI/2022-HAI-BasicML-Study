
## ▷ 다양한 분류 알고리즘
* * *
### ▶ 로지스틱 회귀
> 이름은 회귀이지만 분류 모델입니다. 
> ex) z = a × (Weight) + b × (Length) + c × (Diagonal) + d × (Height) + e × (Width) + f, a, b, c, d, e는 가중치이며 다중 회귀를 위한 선형 방정식과 동일합니다. 여기서 z의 값은 무관하나 확률이 되려면 0~1 또는 0~100%가 되어야합니다. 이를 도와주는 것이 시그모이드 함수입니다.
> #### ▶ 시그모이드 함수
> > 시그모이드 함수는 다음과 같이 계산합니다. φ = 1/ 1+ e^(−z) 이 함수는 z값이 양의 무한대에 가까울수록 1에, 음의 무한대에 가까울수록 0에 수렴하게 됩니다. 아쉽게도 이중분류만 가능한 시그모이드 함수 다중 분류는 어떻게 함수를 이용할까요?
> #### ▶ 소프트맥스 함수
> > 소프트맥스 함수는 여러 개의 선 형 방정식의 출력값을 0~1 사이로 압축하고 전체 합이 1이 되도록 만드는 함수를 말합니다.
### ▶ 확률적 경사 하강법
> 하나의 샘플을 훈련 세트에서 랜덤하게 골라 가장 가파른 길을 찾는 방법을 말합니다.
> #### ▶ 에포크
> > 훈련 세트를 한 번 모두 사용하는 과정을 말합니다.
> 다시 돌아가서 훈련 세트가 너무 많다면 에포크가 너무 커질 수 있습니다. 이럴 땐 하나의 샘플이 아닌 여러 개의 샘플을 이용하는 것이 좋습니다.
> #### ▶ 미니 배치 경사 하강법
> > 여러 개의 샘플을 이용하여 경사 하강법을 수행하는 방법입니다.
> #### ▶ 배치 경사 하강법
> > 모든 샘플을 이용하여 경사 하강법을 수행하는 방법입니다.
### ▶ 손실함수
> 어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지를 측정하는 기준입니다.
> #### ▶ 로지스틱 손실 함수
> > 양성 클래스(타깃 = 1)일 때 손실을 -log(예측 확률)로 계산하고 음성 클래스(타깃 = 0)일 때 손실은 -log(1-예측 확률)로 계산하는 함수입니다.
> > 다른 이름으로는 이진 크로스엔트로피 손실 함수라고도 합니다.
> > 이름 처럼 다중 분류에서는 크로스엔트로피 손실 함수를 사용할 수 있습니다.

> 결정계수는 R^2 = 1 - { (타깃 - 예측)^2의 합 / (타깃 - 평균)^2의 합 }으로 계산합니다. 즉, 1에 가까울수록 예측이 타겟에 가깝다고 해석할 수 있습니다.
### ▶ 에포크 과대적합과 과소적합
> 에포크 횟수가 너무 적으면 과소적합, 많으면 과대적합이 일어날 수 있습니다. 이런 일을 방지하고자 모델을 조기 종료합니다.
> #### ▶ 조기 종료
> > 훈련 세트 점수는 에포크가 진행될수록 꾸준히 증가하지만 테스트 세트 점수는 어느 순간 감소하기 시작합니다. 바로 이 지점 이 모델이 과대적합되기 시작하는 곳입니다. 따라서 이렇게 과대적합이 시작하기 전에 훈련을 멈추는 것을 말합니다.  


