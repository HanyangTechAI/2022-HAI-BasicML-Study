---

## 로지스틱 회귀

### 1\. 로지스틱 회귀란?

선형 방정식을 사용한 분류 알고리즘이다. 선형 회귀와 달리 시그모이드 함수나 소프트맥스 함수를 사용하여 클래스 확률을 출력할 수 있다.

#### 시그모이드 함수


선형 방정식의 출력을 0~1 사이의 값으로 압축한다. z가 음의 무한으로 갈 수록 0에 가까워지고 양의 무한으로 갈 때 1에 가까워진다. 이는 이진 분류를 위해 사용한다.

#### 소프트맥스 함수


다중 분류에서 여러 선형 방정식의 출력 결과를 정규화하여 합이 1이 되도록 만든다. 

---

### 2\. 로지스틱 회귀 예제 데이터 준비

#### 데이터 불러오기

```
import pandas as pd

fish = pd.read_csv('https://bit.ly/fish_csv_data')
fish.head()
# DF -> ndarray
fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
fish_target = fish['Species'].to_numpy()
```

#### 고유값 확인하기

unique() 메서드를 통해 특정 속성의 고유 값을 찾아보자. 

```
print(pd.unique(fish['Species']))
```

이번 예제에서는 물고기 종(Species)의 고유 값을 출력했다.

#### 데이터 셋 준비

```
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)
    
print(train_input.shape) #(119, 5)
print(test_input.shape) #(40, 5)
```

#### 데이터 전처리

```
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

---

### 3\. k-최근접 이웃 분류기 확률 예측

```
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier(n_neighbors=3)
kn.fit(train_scaled, train_target)

print(kn.score(train_scaled, train_target))
print(kn.score(test_scaled, test_target))

# 분류 값 확인
print(kn.classes_)
"""
['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']
"""

#테스트 데이터 5개 예측
print(kn.predict(test_scaled[:5]))
"""
['Perch' 'Smelt' 'Pike' 'Perch' 'Perch']
"""
```

#### 클래스별 확률 확인

predict\_proba 메서드를 이용해서 각 샘플의 클래스별 확률을 확인할 수 있다.

```
import numpy as np

proba = kn.predict_proba(test_scaled[:5])
print(proba)
#round(): 반올림 함수, deciamls 매개변수로 원하는 소수점까지 출력 가능
print(np.round(proba, decimals=4))
```

#### 최근접 이웃 클래스 확인

4번째 샘플의 최근접 이웃 클래스를 확인해보자. 

```
distances, indexes = kn.kneighbors(test_scaled[3:4])
print(train_target[indexes])
"""
[['Roach' 'Perch' 'Perch']]
"""
```

---

### 4\. 로지스틱 회귀

#### 시그모이드 함수 그리기

```
import numpy as np
import matplotlib.pyplot as plt

z = np.arange(-5, 5, 0.1)
phi = 1 / (1 + np.exp(-z))

plt.plot(z, phi)
plt.xlabel('z')
plt.ylabel('phi')
plt.show()
```

#### 로지스틱 회귀로 이진 분류

불리언 인덱싱으로 Bream , Smelt 종만의 데이터를 불러오자.

```
# boolean indexing
char_arr = np.array(['A', 'B', 'C', 'D', 'E'])
print(char_arr[[True, False, True, False, False]]) # ['A' 'C']

# boolean array
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')

#boolean indexing
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]
```

위 데이터를 바탕으로 로지스틱 회귀 훈련을 진행해보자.

```
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)

# 예측
print(lr.predict(train_bream_smelt[:5]))
"""
['Bream' 'Smelt' 'Bream' 'Bream' 'Bream']
"""
print(lr.predict_proba(train_bream_smelt[:5]))
"""
[[0.99759855 0.00240145]
 [0.02735183 0.97264817]
 [0.99486072 0.00513928]
 [0.98584202 0.01415798]
 [0.99767269 0.00232731]]
"""
print(lr.classes_)
"""
['Bream' 'Smelt']
"""
# 가중치 구하기(계수, 기울기)
print(lr.coef_, lr.intercept_)
"""
[[-0.4037798  -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132]
"""
```

위 계수로 z 값을 구한 뒤 시그모이드 함수에 대입해 확률을 구해보자.

decision\_function()으로 z값을 구할 수 있다.

```
decisions = lr.decision_function(train_bream_smelt[:5])
print(decisions
"""
[-6.02927744  3.57123907 -5.26568906 -4.24321775 -6.0607117 ]
"""
```

scipy의 expit() 으로 시그모이드 함수에 z값을 대입해보자.

```
from scipy.special import expit

print(expit(decisions))
"""
[0.00240145 0.97264817 0.00513928 0.01415798 0.00232731]
"""
```

---

#### 로지스틱 회귀로 다중 분류하기

다중분류는 타깃 클래스가 2개 이상인 분류이다. 로지스틱 회귀는 다중 분류를 위해 소프트맥스 함수를 사용한다.

C 값과 max\_iter 매개변수를 통해 규제 정도를 정할 수 있다.

```
lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_scaled, train_target)

#과대적합 / 과소적합 확인
print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))

# 테스트 샘플 확인
print(lr.predict(test_scaled[:5]))
"""
['Perch' 'Smelt' 'Pike' 'Roach' 'Perch']
"""

# 클래스 확인
print(lr.classes_) # ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']

# 테스트 샘플 확률 확인 (각 클래스별 확률 확인)
proba = lr.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=3))
"""
[[0.    0.014 0.841 0.    0.136 0.007 0.003]
 [0.    0.003 0.044 0.    0.007 0.946 0.   ]
 [0.    0.    0.034 0.935 0.015 0.016 0.   ]
 [0.011 0.034 0.306 0.007 0.567 0.    0.076]
 [0.    0.    0.904 0.002 0.089 0.002 0.001]]
"""

# 가중치 확인
print(lr.coef_.shape, lr.intercept_.shape)
print(lr.coef_, lr.intercept_)
"""
(7, 5) (7,)
[[-1.49002087 -1.02912886  2.59345551  7.70357682 -1.2007011 ]
 [ 0.19618235 -2.01068181 -3.77976834  6.50491489 -1.99482722]
 [ 3.56279745  6.34357182 -8.48971143 -5.75757348  3.79307308]
 [-0.10458098  3.60319431  3.93067812 -3.61736674 -1.75069691]
 [-1.40061442 -6.07503434  5.25969314 -0.87220069  1.86043659]
 [-1.38526214  1.49214574  1.39226167 -5.67734118 -4.40097523]
 [ 0.62149861 -2.32406685 -0.90660867  1.71599038  3.6936908 ]] [-0.09205179 -0.26290885  3.25101327 -0.14742956  2.65498283 -6.78782948
  1.38422358]
"""

# z 값 -> 소프트맥스 함수 대입해서 확률 구하기 (predict_proba 와 기능 동일)
decision = lr.decision_function(test_scaled[:5])
print(np.round(decision, decimals=2))

from scipy.special import softmax

proba = softmax(decision, axis=1)
print(np.round(proba, decimals=3))
"""
[[0.    0.014 0.841 0.    0.136 0.007 0.003]
 [0.    0.003 0.044 0.    0.007 0.946 0.   ]
 [0.    0.    0.034 0.935 0.015 0.016 0.   ]
 [0.011 0.034 0.306 0.007 0.567 0.    0.076]
 [0.    0.    0.904 0.002 0.089 0.002 0.001]]
"""
```

---

### **참고 자료**

**\[혼자 공부하는 머신러닝 + 딥러닝 4-1.ipynb\] : [https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/4-1.ipynb](https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/4-1.ipynb)**

---

## 확률적 강사 하강법

### 1\. 확률적 강사 하강법이란?

훈련 세트에서 샘플 하나씩 꺼내 손실 함수의 경사를 따라 최적의 모델을 찾는 알고리즘이다. 샘플을 하나씩 사용하지 않고 여러 개를 사용하면 미니배치 경사 하강법이 된다. 한 번에 전체 샘플을 사용하면 배치 경사 하강법이 된다.

#### 이전 방법들과의 차이는?

앞에서 배운 방법들은 이전에 훈련한 모델을 버리고 다시 새로운 모델을 훈련하는 방식이다. 만약 매번 데이터가 늘어나는 상황이라면 매번 새로운 모델을 만들어야 하는 것이다. 그래서 앞서 만든 모델에 새로운 훈련 데이터만 추가로 훈련하여 추가할 수 있는 방법을 **점진적 학습**이라고 한다. 

#### 손실 함수

손실함수는 어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지를 측정하는 기준이다. 이는 확률적 강사 하강법이 최적화할 대상이다. 대부분의 문제에 잘 맞는 손실 함수가  이미 정의되어 있다. 이진 분류에는 로지스틱 회귀 손실 함수를 사용한다. 다중 분류에는 크로스엔트로피 손실 함수를 사용한다. 회귀 문제에서는 mse를 사용한다. 손실 함수에서도 로지스틱 회귀(시그모이드, 소프트맥스)에서 처럼 연속적인 값을 얻는 방법이 있다. 대표적으로 로지스틱 손실 함수, 크로스엔트로피 손실 함수 등이 있다. 

---

### 2\. 확률적 강사 하강법 예제

#### 데이터 불러오기 , 셋 분리, 전처리

```
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

fish = pd.read_csv('https://bit.ly/fish_csv_data')
fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
fish_target = fish['Species'].to_numpy()

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)
    
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

#### SGDClassifier

SGDClassifier 객체를 만들 때는 2개의 매개변수(loss, max\_iter)를 넘겨준다. loss 는 손실함수 종류, max\_iter 는 에포크(전체 샘플을 모두 사용한 횟수) 횟수를 지정한다. loss 값으로는 hinge(default), svm 등 다양한 머신 러닝 알고리즘을 지원한다.  

```
from sklearn.linear_model import SGDClassifier
#loss = log -> 로지스틱 손실 함수
sc = SGDClassifier(loss='log', max_iter=10, random_state=42)
sc.fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
"""
0.773109243697479
0.775
"""
```

#### 추가 훈련

모델을 새로 만들지 않고 이어서 훈련할 때는 partial\_fit() 을 이용한다. 

```
sc.partial_fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
"""
0.8151260504201681
0.85
"""
```

정확도가 향상된 것을 확인할 수 있다. 따라서 모델을 여러 에포크에서 훈련할 필요성이 있다. 

#### 에포크 과대/과소적합

에포크를 계속해서 진행할수록 과대적합이 일어날 것이다. 이를 그래프로 확인해보자.

```
import numpy as np
import matplotlib.pyplot as plt


sc = SGDClassifier(loss='log', random_state=42)

train_score = []
test_score = []

classes = np.unique(train_target)

#에포크 횟수 0~299
for _ in range(0, 300):
    sc.partial_fit(train_scaled, train_target, classes=classes)
    
    train_score.append(sc.score(train_scaled, train_target))
    test_score.append(sc.score(test_scaled, test_target))
    
plt.plot(train_score)
plt.plot(test_score)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()
```

과대적합이 시작하기 전에 훈련을 멈추는 것을 조기 종료라 한다. SGDClassifier는 일정 에포크 동안 성능이 향상되지 않으면 더 훈련하지 않고 자동으로 멈춘다. 

---

### **참고 자료**

**\[혼자 공부하는 머신러닝 + 딥러닝 4.2.ipynb\] : [https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/4-2.ipynb](https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/4-2.ipynb)**