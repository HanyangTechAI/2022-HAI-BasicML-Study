+ 로지스틱 회귀
  선형성을 가진 분류 알고리즘.

+ 시그모이드 함수 (sigmoid function)
  z값이 음수인 경우에는 0에 수렴하고, 양수인 경우에는 1에 수렴하는 함수 그래프. 치역이 절대로 0과 1 사이의 범위를 벗어나지 않으므로, 이를 0%에서 100%의 확률로 환산할 수 있음.

+ 로지스틱 회귀를 이용한 다중 분류
  로지스틱 회귀를 이용한 이진 분류에서는 시그모이드 함수를 사용하지만, 로지스틱 회귀를 이용한 다중 분류에서는 소프트맥스 함수를 이용함.

+ 소프트맥스 함수 (softmax function)
  범주마다 z값을 하나씩 계산하여 가장 높은 z값의 범주를 반환함.

+ 옵티마이저: 확률적 경사 하강법
  - 경사 하강법 (Gradient Descent)
        함수 그래프의 기울기를 이용한 점진적 학습 알고리즘. 손실 함수의 그래프의 기울기가 점점 낮아지는 방향으로 가면서 손실 점수를 줄임.
    - 확률적 경사 하강법
        전체 훈련 샘플을 사용하지 않고 랜덤하게 하나의 샘플을 골라 경사 하강법을 진행. 하나의 샘플에서 경사 하강법이 진행 완료되면 다음 샘플로 랜덤하게 옮겨 다시 경사하강법 진행. 이런 식으로 전체 샘플에 대해 경사하강법 진행.
  - 미니배치 경사하강법 (Minibatch Gradient Descent)
        확률적 경사 하강법과 다르게 여러 개의 샘플을 무작위로 선택해 경사 하강법 진행. 
  - 배치 경사 하강법 (Batch Gradient Descent)
        전체 샘플에 대해 한 번에 경사하강법 진행. 가장 안정적인 방법이나, 컴퓨팅 파워가 많이 필요하고, 데이터의 크기에 따라 에러 발생 가능.
+ 손실 함수 (Loss Function)
  머신러닝 알고리즘의 학습 결과가 얼마나 잘못되었는지 판단. 손실 함수의 결과값이 낮을수록 좋음. 

+ 로지스틱 손실 함수 (Logistic Loss Function, 이진 크로스엔트로피 손실 함수)
    양성 클래스 (target = 1) 인 경우에는 -log(예측확률) 로 계산하고, 음성 클래스 (target = 0) 인 경우에는 -log(1-예측확률) 로 계산. 이렇게 된다면 예측 확률이 0에서 멀어질수록 손실값이 아주 큰 양수가 됨.