# 04 다양한 분류 알고리즘
> 럭키백의 확률 계산하기
---
## 4-1 로지스틱 회귀  
<br>

### 로지스틱 회귀 (*logistic regression*)
 **로지스틱 회귀**는 이름은 회귀이지만 분류 모델이다. 이 알고리즘은 선형 회귀와 동일하게 다음과 같은 선형 방정식을 학습한다.  
 $z = a × (weight) + b × (length) + c ×(height)$  
 <br>
 여기서 $a, b, c$는 가중치 혹은 계수이다. 하지만 $z$는 실수의 값을 가지고 이것이 확률이 되려면 0\~1의 값을 가져야 한다. 따라서 z를 **시그모이드 함수** (*sigmoid function*)로 변환하여 확률값을 얻는다.  
 $$ϕ = \frac{1}{1+\frac{1}{z}}$$  
 $z$가 무한하게 큰 음수일 경우 이 함수는 0에 가까워지고, $z$가 무한하게 큰 양수일 때는 1에 가까워지므로 0\~1사이의 값을 얻을 수 있다.  
 <br>
 로지스틱 회귀를 사용하여 다중분류를 할 수도 있다.  
 각 클래스마다 $z$값을 구한 후 **소프트맥스**(*softmax*)함수를 사용하여 7개의 $z$값을 총합이 1인 확률로 변환한다.  
 <br>
<br>
## 4-2 확률적 경사 하강법  
<br>

### 점진적 학습
 훈련 데이터가 한번에 준비되는 것이 아닌 조금씩 전달될 때, 앞서 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 더 훈련하는 방식을 **점진적 학습**또는 온라인 학습이라고 한다. 
<br>
<br>
### 확률적 경사 하강법 (*Stochastic Gradient Descent*)
**확률적 경사 하강법**은 대표적인 점진적 학습 알고리즘이다.  
확률적 경사 하강법에서 경사 하강법은 모델이 얼마나 엉터리인지 판단하는 기준인 **손실 함수**(*loss function*)라 하는 수치를 줄이는 방향으로 학습한다는 의미이다. 또한 확률적이란 말은 '무작위하게'라는 의미를 포함하여, 확률적 경사 하강법은 훈련을 할 때 전체 샘플을 사용하지 않고 딱 하나의 샘플을 훈련 세트에서 랜덤하게 골라 손실 함수가 줄어드는 가장 가파른 길을 찾는 방식으로 훈련을 진행한다.  
이때 전체 훈련 세트를 한 번 모두 사용하는 과정을 **에포크**(*epoch*)라 하고 경사 하강법은 일반적으로 수십, 수백 번 이상 에포크를 수행한다.  
<br>
1개씩 말고 무작위로 여러개의 샘플을 사용해 경사 하강법을 수행하는 방식을 **미니배치 경사 하강법**(*minibatch gradient descent*)라 하고, 한 번에 전체 샘플을 사용하는 방법을 **배치 경사 하강법**(*batch gradient descent*)라 한다.  
<br>

이진 분류에서 사용하는 손실 함수를 **로지스틱 손실 함수**(*logistic loss function*) 또는 **이진 크로스엔트로피 손실 함수**(*binary cross-entropy loss function*)이라 하고, 다중 분류에서 사용하는 손실 함수를 **크로스엔트로피 손실 함수**라고 한다.  
<br>

### 에포크와 과대/과소적합
경사 하강법을 사용한 모델은 에포크 횟수에 따라 과소적합이나 과대적합이 될 수 있다.  
에포크 횟수가 적으면 모델이 조금밖에 훈련하지 않아 훈련 세트와 테스트 세트에 잘 맞지 않는 과소적합된 모델이 될 가능성이 높고, 에포크 횟수가 너무 많으면 훈련 세트에 너무 잘 맞아 테스트 세트에는 정확도가 낮은 과대적합된 모델이 될 가능성이 높다.  
<br>
에포크가 진행됨에 따라 훈련 세트 점수는 꾸준히 증가하지만 테스트 세트 점수는 어느 순간 감소하기 시작하는데, 이 과대적합이 시작하는 지점 전에 훈련을 멈추는 것을 **조기종료**(*early stopping*)라고 한다.  
<br>
사이킷런에서 제공하는 **SGDClassifier** 모델은 *tol* 매개변수 값을 따라 일정 에포크 동안 성능이 향상되지 않으면 더 훈련하지 않고 자동으로 훈련을 종료한다.