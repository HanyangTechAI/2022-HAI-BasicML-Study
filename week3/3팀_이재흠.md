# 🖥️ 혼공머신 스터디 : 3장 요약
#### 스터디 3조 이재흠 (@rethinking21, rethinking21@gmail.com)

***
## 챕터 4 다양한 분류 알고리즘 🎁<br><br>

### 04-1. 로지스틱 회귀 🐍

###### 다중분류(multi-class classification)
타깃 데이터에 2개 이상의 클래스가 포함된 문제를 **다중분류(multi-class classification)** 라고 부릅니다.
이전에 배웠던 k-최근접 이웃 분류기를 통해 다중 분류를 수행 하실 수 있습니다.
```python
from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier(n_neighbors=3)
kn.fit(train_scaled. train_target)
proba = kn.predict_proba(test_scaled[4])
print(np.round(proba, decimals=4))
```
출력값에 보이는 결과는 

###### 로지스틱 회귀(logistic regression)

**로지스틱 회귀(logistic regression)** 는 분류 모델로,  선형회귀와 동일하게 선형방정식을 학습합니다. 
확률은 0에서 1 사이의 값이어야 하기 때문에, 선형회귀는 확률을 표현하는 데에는 적절하지 않습니다

![시그모이드 함수의 그래프](images\3팀_이재흠\04-1 logistic function.png)<br>
▲ [🖼️ 이미지 출저][1]
<br> **시그모이드 함수(sigmoid function)** 는 이를 해결해주는 함수입니다.
이 함수는 무한히 음수값이 될 때는 0에 가까워지고, 무한히 양수값이 될 때는 1에 가까워집니다.


Scikit-learn에서는 LogisticRegression 클래스를 이용하여 로지스틱 회귀모델을 이용할 수 있습니다.
```python
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_data, target_data)
```
여기서 max_iter 매개변수는 반복횟수를 의미하고, C 매개변수는 LogisticRegression 클래스의 규제를 제어합니다.

또한 Scipy의 expit이라는 함수를 통해 시그모이드 함수를 구현 할 수 있습니다.
```python
from scipy.special import expit
print(expit(decisions)) 
```

###### 불리언 인덱싱(boolean indexing)
넘파이 배열은 True, False 값을 전달하여 행을 선택 할 수 있습니다. 이를 **불리언 인덱싱(boolean indexing)** 이라고 부릅니다.

```python
import numpy as np
char_arr = np.array(['A', 'B', 'C', 'D', 'E'])
print(char_arr[[True, False, True, False, False]]) # ['A' 'C']
```

###### 소프트맥스 함수 (softmax function)

로지스틱 회귀로 다중 분류를 수행할 때는 **소프트맥스(softmax)** 함수를 사용하여 확률로 변환합니다.

![소프트맥스 함수](images\3팀_이재흠\04-1 softmax function.png)<br>

scipy에서의 함수 softmax를 이용하여 소프트맥스 함수를 사용 할 수 있습니다.

```python
from scipy.special import softmax
import numpy as np
proba = softmax(decision, axis=1)
print(np.round(proba, decimals=3))
```

### 04-2. 확률적 경사 하강법 ⛰️

###### 경사하강법
**경사하강법** 은 함수 값이 낮아지는 방향으로 독립 변수 값을 변형해 최소 함수 값을 갖도록 하는 방법을 말합니다. 보통 산에서 내려오는 것으로 비유를 많이 합니다.
<br>경사하강법에서 조심해야 하는 부분은 step size입니다. 이 값이 너무 작을 경우 학습을 여러번 해도 온전히 학습이 안되는 문제가 발생 할 수 있으며, 반대로 값이 너무 클경우 제대로 학습이 되지 않는 문제가 발생합니다.
<br>![편안한 stepsize](images\3팀_이재흠\04-2 GD step size 01.gif)<br>
<br>![너무 커서 불편한](images\3팀_이재흠\04-2 GD step size 02.gif)<br>
▲[🖼️ 이미지 출처][2]
<br><br>경사하강법에는 종류가 여러가지가 있습니다.

먼저, 전체 데이터를 한번에 돌려서 경사하강법을 수행하는 **배치 경사 하강법(Batch Gradient Descent)** 가 있습니다. 
이 방법은 가장 안정적이지기도 하지만, 전체데이터를 사용하기에 컴퓨터 자원을 많이 소모한다는 단점이 있습니다.

이와 달리 전체데이터중 한개를 무작위로 골라서 학습시키는 **확률적 경사 하강법(Stochastic Gradient Descent, SGD)** 이 있습니다. 
이 경우 학습속도가 빠르다는 장점이 있지만, 데이터가 말 그대로 확률적이기에 최소비용에 도달했는지 판단하는데 어려움이 있다는 단점이 존재합니다.

전체 데이터 중에 여러개의 데이터를 뽑아와 학습시키는 방법도 있습니다. 이를 **미니배치 경사 하강법(minibatch Gradient Descent)** 라고 부릅니다.
SGD의 노이즈를 줄이면서, 속도 또한 빠르다는 장점이 있기에 실전에서 많이 쓰이는 방법입니다.

<br>![경사학습법을 그림으로 나타낸 것](images\3팀_이재흠\04-2 manyGDs.png)<br>

훈련세트를 한번 다 사용 하는 것을 **에포크(epoch)** 라고 합니다. 
또, **배치(batch)** 는 모델의 가중치를 한번 업데이트시킬 때 사용되는 샘플들의 묶음을 의미합니다. 
배치와 에포크를 조절해가며 학습의 효율을 조절해야 합니다.

에포크를 많이 수행하게 되면 어느새 훈련/테스트 세트의 정확도가 낮아지는 현상이 발생합니다. 이는 과대적합이 된 경우로서 우리는 이 경우가 오지 전에 훈련을 종료해야 합니다.
이를 **조기 종료(early stopping)** 라고 합니다.

[❕ 경사하강법에 대한 자세한 내용][3]

Scikit-learn에서 확률적 경사 하강법을 제공하는 대표적인 분류용 클래스는 SGDClassifier가 있습니다.
```python
from sklearn.linear_model import SGDClassifier
sc = SGDClassifier(loss='log', max_iter=10, random_state=42)
sc.fit(train_scaled, train_target)
```
여기서 max_iter는 수행할 에포크 횟수를 뜻합니다. loss는 추후의 나올 손실함수를 무엇으로 계산할 것인지를 정해주는 매개변수입니다.

###### 손실함수(loss function)

**크로스엔트로피 손실 함수(cross-entropy loss function)** 분류 모델이 얼마나 잘 수행되는지 측정하기 위해 사용되는 지표입니다. 
분류 과정에서 실제값과 예측값이 정확할수록 점점 0에 가까워지게 됩니다.

**로지스틱 손실함수(logistic loss function)** 는 다중 분류를 위한 손실 함수인 크로스 엔트로피(cross entropy) 손실 함수를 이진 분류 버전으로 만든 것입니다.
그래서 **이진 크로스엔트로피 손실 함수(binary cross-entropy loss function)** 이라고 불리기도 합니다.

[❕ 로지스틱 손실 함수에 대한 더 자세한 내용][4]

SGDClassifier 에서의 loss 매개변수의 기본값은 힌지손실입니다. **힌지 손실(hinge loss)** 는 **서포트 벡터 머신** 이라는 또 다른 머신러닝 알고리즘을 위한 손실함수입니다.


[1]: https://velog.io/@yuns_u/Logistic-Regression
[2]: https://hackernoon.com/life-is-gradient-descent-880c60ac1be8
[3]: https://angeloyeo.github.io/2020/08/16/gradient_descent.html
[4]: https://ukb1og.tistory.com/22