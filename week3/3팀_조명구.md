04-1 로지스틱 회귀
=================

>럭키백에서 각 생선들이 나올 확률 구하기

앞에서 공부한 k-최근접 이웃으로 가까운 이웃의 비율을 계산해주는 모델을 이용하겠습니다.

데이터 준비
----------
```python
import pandas as pd
fish  = pd.read_csv('http://bit.ly/fish_csv_data')
fish.head()
```
![](2022-04-04-16-09-30.png)

```python
print(pd.unique(fish['Species']))
```
![](2022-04-04-16-11-23.png)
```pd.unique()``` 클래스로 데이터 프레임의 Species 열에 어떤 데이터가 있는지 출력하여 생선의 종류를 확인하였습니다.

```python
fish_input = fish[['weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()

fish_target = fish['Species'].to_numpy()

from sklearn.model_selection import train_split
train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```
fish_target는 각 인덱스에 해당하는 생선 종류에 대한 데이터를 넘파이 배열화시킨 것입니다. 
데이터를 훈련 세트와 테스트 세트로 나누어주었고, 데이터 전처리를 통해 표준점수화 하였습니다.

k-최근접 이웃 분류기의 확률 예측
----------
>확률이 0.5 이상이면 양성 클래스로, 0.5 미만이면 음성 클래스로 판단한다.

```python
import numpy as np
proba = kn.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=4))
```
```predict_proba()```로 각 클래스에 대해 모델이 예상한 확률을 나타낼 수 있습니다. 근데 이웃 3개에 대해 판단하므로, 확률이 1, 0.6666..., 0.3333..., 0 4개만 나타나서 좀 어색합니다.

그래서 **로지스틱 회귀**를 이용하여 선형적인 확률 분포를 만들어보려 합니다. **시그모이드 함수**를 사용하면 가능합니다.

![](2022-04-06-13-10-01.png)

로지스틱 회귀로 이진 분류
-----------
```python
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt == train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]
```
```bream_smelt_indexes``` 배열은 도미와 빙어일 경우 ```True``` 이고 그 외는 모두 ```False``` 값이 들어가 있습니다. 이 인덱스 배열을 이용하면 도미와 빙어 데이터만 골라낼 수 있습니다.

이제 사이킷런에서 제공하는 로지스틱 회귀 메서드를 이용하여 모델 훈련 및 결과 확인을 해보겠습니다.

```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)

print(lr.predict_proba(train_bream_smelt[:5]))
```

```
[[0.99759855 0.00240145]
 [0.02735183 0.97264817]
 [0.99486072 0.00513928]
 [0.98584202 0.01415798]
 [0.99767269 0.00232731]]
 ```
 첫 번째 열이 음성클래스(Bream), 두 번째 열이 양성클래스(Smelt)에 대한 확률입니다.


로지스틱 회귀로 다중 분류
-------------
```python
lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_scaled, train_target)
```
```C```는 값이 작을수록 규제가 커지는 매개변수고, ```max_iter```가 반복 횟수를 지정해준느 매개변수입니다(기본값 100).

```python
proba = lr.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=3))
```
```
[[0.    0.014 0.841 0.    0.136 0.007 0.003]
 [0.    0.003 0.044 0.    0.007 0.946 0.   ]
 [0.    0.    0.034 0.935 0.015 0.016 0.   ]
 [0.011 0.034 0.306 0.007 0.567 0.    0.076]
 [0.    0.    0.904 0.002 0.089 0.002 0.001]]
 ```
 테스트 세트의 처음 5개 샘플에 대한 예측 확률입니다. 가장 높은 확률에 대응하는 생선을 예측합니다.

 데이터가 5개의 특성을 사용하므로 선형방정식의 계수가 5개입니다. 식은 7개입니다. 한 데이터에 대해 7개의 z값을 모두 도출해 **소프트맥스 함수**를 이용하여 7개의 z값을 확률로 변환합니다. 모델은 가장 높은 함수값을 가진 z값에 해당하는 생선으로 예측하게 되는 것입니다.

 확률적 경사 하강법
 ===========
 데이터가 조금씩 공급될 때, 지속적인 학습을 위해 사용하는 방법입니다. 이를 위해서 앞서 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 훈련해야 합니다. 이를 **점진적 학습**이라고 합니다. 대표적인 점진적 학습 알고리즘은 **확률적 경사 하강법**입니다.

 >훈련 세트의 데이터를 모두 사용하면 새로운 데이터를 훈련세트의 데이터에 채워준다.

 여기서 훈련세트를 한 번 모두 사용하는 과정을 에포크라고 합니다.

 ![](2022-04-06-13-40-02.png)

 로지스틱 손실 함수
 ------
 ![](2022-04-06-13-43-11.png)
여기서 양성 클래스 (타깃 = 1)일 때 예측 확률이 낮을수록 손실 함수의 크기가 매우 커집니다. 음성 클래스 (타깃 =0)에서는 반대로 예측 확률이 높을수록 손실 함수의 크기가 매우 커집니다. 이처럼 잘못된 예측을 할수록 손실 함수의 크기가 커지는 것을 확인할 수 있습니다. 그렇다면 손실 함수의 크기를 낮추는 것이 확률적 경사 하강법에서의 목표라고 볼 수 있겠습니다.

SGDClassifier
-------------
```python
import pandas as pd

fish = pd.read_csv('https://bit.ly/fish_csv_data')

fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
fish_target = fish['Species'].to_numpy()

from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```
데이터를 훈련 세트와 테스트 세트로 분류하고 전처리까지 마쳤습니다.

사이킷런에서 확률적 경사 하강법을 제공하는 대표적인 분류용 클래스는 **SGDClassifier**입니다.

```python
from sklearn.linear_model import SGDClassifier

sc = SGDClassifier(loss='log', max_iter=10, random_state=42)
sc.fit(train_scaled, train_target)
```
지금 에포크 횟수가 10으로 설정되어있는데(```max_iter```), 에포크 횟수가 부족하여 모델의 점수가 낮게 나옵니다.
```python
sc.partial_fit(train_scaled, train_target)
```
해당 클래스를 사용하면 1 에포크씩 훈련할 수 있고, 점수가 좀 오릅니다. 그럼 어디까지 반복해야 할지 정해주는 기준이 필요하겠습니다.

에포크의 과대/과소적합
-----
에포크의 반복횟수가 많을수록 훈련세트를 과도하게 학습하여 **과대적합**이 일어날 것이고, 에포크를 적게 반복하면 **과소적합**이 일어날 것입니다. 이 현상을 막기 위해 그래프를 통해 적절한 에포크 횟수를 찾을 것입니다.

```python
import numpy as np

sc = SGDClassifier(loss='log', random_state=42)

train_score = []
test_score = []

classes = np.unique(train_target)

for _ in range(0, 300):
    sc.partial_fit(train_scaled, train_target, classes=classes)
    
    train_score.append(sc.score(train_scaled, train_target))
    test_score.append(sc.score(test_scaled, test_target))

import matplotlib.pyplot as plt

plt.plot(train_score)
plt.plot(test_score)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()
```
```train_score```와 ```test_score```에 에포크마다 훈련 세트와 테스트 세트에 대한 점수가 기록됩니다.

에포크를 300번 반복하였습니다. 

**matplotlib**을 통해 그린 그래프는 다음과 같습니다.

![](2022-04-06-14-12-11.png)

그래프를 통해 훈련 세트 점수, 테스트 세트 점수 모두 적당히 높고, 차이가 그리 나지 않고, 훈련 세트 점수가 조금 더 높은 에포크는 100 점도로 보입니다.

```python
sc = SGDClassifier(loss='log', max_iter=100, tol=None, random_state=42)
sc.fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```
```
0.957983193277311
0.925
```

최종적으로 적절한 에포크 100을 설정하여 모델을 학습시킨 결과입니다. 점수가 잘 나오는 것으로 확인됩니다.