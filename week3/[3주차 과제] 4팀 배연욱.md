# [3주차 과제] 4팀 배연욱

---

### Chapter 4. 다양한 분류 알고리즘
#### 04-1 로지스틱 회귀

---

* k-최근접 이웃은 주변 이웃의 클래스 비율을 확률로 출력한다. 

**로지스틱 회귀**는 선형 방정식을 사용한 분류 알고리즘이다. 선형 회귀와 달리 시그모이드 함수나 소프트맥스 함수를 사용하여 클래스 확률을 출력할 수 있다. 분류 알고리즘임에도 로지스틱 '회귀'라고 이름 붙여진 것은 그 기저 기술이 선형 회귀와 거의 같기 때문이다.

**다중 분류**는 타깃 클래스가 2개 이상인 분류 문제이다. 로지스틱 회귀는 다중 분류를 위해 소프트맥스 함수를 사용하여 클래스를 예측한다.

**시그모이드 함수**는 선형 방정식의 출력을 0과 1 사이의 값으로 압축하며 이진 분류를 위해 사용한다.
![시그모이드 함수](/Users/yeonukpae/Desktop/2022-HAI-BasicML-Study/week3/sigmoid function.png)

**소프트맥스 함수**는 다중 분류에서 여러 선형 방정식의 출력 결과를 정규화하여 합이 1이 되도록 만든다.
![소프트맥스 함수](/Users/yeonukpae/Desktop/2022-HAI-BasicML-Study/week3/softmax function.png)

---

#### 04-2 확률적 경사 하강법

**점진적 학습**은 앞서 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 더 훈련하는 방식이다. 온라인 학습이라고 불리기도 한다. 대표적인 점진적 학습 알고리즘이 확률적 경사 하강법이다.

**확률적 경사 하강법**은 훈련 세트에서 샘플을 하나씩 꺼내 손실 함수의 경사를 따라 최적의 모델을 찾는 알고리즘이다. 샘플을 하나씩 사용하지 않고 여러 개를 사용하면 **미니배치 경사 하강법**이 된다. 한 번에 전체 샘플을 사용하면 **배치 경사 하강법**이 된다.

* 신경망 모델은 확률적 경사 하강법이나 미니배치 경사 하강법을 사용한다.

**손실 함수**는 확률적 경사 하강법이 최적화할 대상이다. 대부분의 문제에 잘 맞는 손실 함수가 이미 정의되어 있다. 이진 분류에는 로지스틱 회귀(또는 이진 크로스엔트로피) 손실 함수를 사용한다. 다중 분류에는 크로스엔트로피 손실 함수를 사용한다. 회귀 문제에는 평균 제곱 오차 손실 함수를 사용한다. 손실 함수는 미분 가능해야 한다.

* 엄밀히 말하면 손실 함수는 샘플 하나에 대한 손실을 정의하고 비용 함수는 훈련 세트에 있는 모든 샘플에 대한 손실 함수의 합을 말한다.
*  회귀의 손실 함수로는 평균 절댓값 오차(타깃에서 예측을 뺀 절댓값을 모든 샘플에 평균한 값) 또는 평균 제곱 오차(타깃에서 예측을 뺀 값을 제곱한 다음 모든 샘플에 평균한 값)를 많이 사용한다. 이 값이 작을수록, 즉 오차가 작을수록 좋은 모델이다. 

**힌지 손실**은 **서포트 벡터 머신**이라 불리는 다른 머신러닝 알고리즘을 위한 손실 함수이다.

**에포크**는 확률적 경사 하강법에서 전체 샘플을 모두 사용하는 한 번 반복을 의미한다. 일반적으로 경사 하강법 알고리즘은 수십에서 수백 번의 에포크를 반복한다.

**조기 종료**는 과대적합이 시작하기 전에 훈련을 멈추는 것이다.