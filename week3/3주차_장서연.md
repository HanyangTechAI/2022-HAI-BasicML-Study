# k-최근접 이웃 회귀
## 구현
* 데이터 불러오기: pd.read_csv()
* 넘파이로 변환: .to_numpy()
* 데이터 분리하기: train_test_split()
* 데이터 표준화: StandardScaler()
* 모델 만들기: KneighborsClassfier()


# 로지스틱 회귀
## 로지스틱 함수
$y=1/(1+e^(-x)) (x∈R, 0<y<1)$
* 치역이 0(0%)과 1(100%) 사이
* 이중 분류(True/False)에서 사용
* 0.5보다 크면 양성 클래스 / 0.5보다 작으면 음성 클래스 (scikitlearn에서는 0.5인 경우 음성으로 판단)
### 로지스틱 함수 구현
* 데이터 가공
* 불리언 인덱싱
* 모델 만들기: Logistic Regression()
* 모델 학습하기: lr.fit()
* 모델 평가하기: lr.predict(), predict_proba()

## 소프트맥스 함수
$s_n=e^(z_n)/(∑e^z) (z∈R,  ∑s = 1)$
* 모든 z에 대해 계산한 s를 모두 더하면 1
* 다중 분류에서 사용
* from scipy.special import softmax로 호출

# 경사 하강법
## 확률적 경사 하강법
* 훈련 세트에서 랜덤하게 샘플 하나를 골라 손실 함수를 줄임
* 에포크(epoch): 훈련 세트를 한번 모두 사용하는 것
    * 에포크 횟수가 적음 >>> 과소 적합
    *  에포크 횟수가 많음 >>> 과대 적합, 조기 종료를 통해 과대적합 방지
## 미니배치 경사 하강법
* 샘플을 하나씩 사용하지 않고 여러 개를 사용
## 배치 경사 하강법
* 한 번에 전체 샘플을 사용


# 손실함수
## 로지스틱 손실 함수
* 타깃이 양성 클래스인 경우 >> -log(p) (p: 예측 확률)
* 타깃이 음성 클래스인 경우 >> -log(1-p)
* 이진 분류: 이진 크로스엔트로피 손실 함수,
* 다중 분류: 크로스엔트로피 손실 함수
## 평균 제곱/절댓값 오차
* 평균 절댓값 오차: 타깃에서 예측을 뺀 절댓값을 모든 샘플에 평균한 값
* 평균 제곱 오차: 타깃에서 뺸 값을 제곱한 다음 모든 샘플에 평균한 값
    * 회귀 문제에서 많이 사용
## 힌지
* SGDClassifier() 함수 loss 매개변수의 기본값
* (SGDClassifier(loss, max_iter, random_state)
    * loss = ‘log’인 경우 로지스틱 회귀
    * max_iter = epoch 횟수)