
# Ch4 스터디

###4-1. 로지스틱 회귀
* K-최근접 이웃을 통한 확률 계산
> - K-최근접 이웃 분류기로 클래스 비율을 통해 확률을 계산할 수 있다.
> - 타깃 데이터에 2개 이상의 클래스가 포함된 문제인 '다중분류' 문제가 더 복잡해 질수록 확률이 어색해진다.
* 로지스틱 회귀 : 선형방정식을 학습하는 분류 모델. 

 사이킷런의 LogisticRegression 클래스를 사용한다.
 > - max_iter : 반복 횟수를 지정해준다. 횟수가 적으면 경고가 발생한다.
 > - C : 계수의 제곱을 얼마나 규제할 지를 지정해준다. 이를 L2규제라고 하며, 릿지 회귀에서의 alpha 매개변수와 비슷한 역할이다.
 값이 작을 수록 규제가 커진다.
> - lf.coef_, lr.interceot로 로지스틱 회귀 모델이 학습한 방정식을 알 수 있다.

* 불리언 인덱싱 : 넘파이 배열에 True, False 값을 전달하여 원하는 행만을 선택할 수 있다.

* 로지스틱 회귀로 이진 분류하기 :
 
 확률을 나타내기 위해, 값을 0~1 사이의 값으로 바꾸어주는 시그모이드 함수(로지스틱 함수)를 사용한다. 
$$f(x) = {1 \over 1+e^{-z}}$$
z가 무한하게 큰 음수일 경우 0에 가까워지고, z가 무한하게 큰 양수가 될 때는 1에 가까워진다. 함수의 출력이 0.5보다 크면 양성 클래스, 작으면 음성 클래스이다.

* 로지스틱 회귀로 다중 분류하기 : 

 다중분류는 클래스마다 결과값을 하나씩 계산한다. 이 때 소프트맥스 함수를 사용한다. 소프트맥스 함수는 여러 개의 선형 방정식의 출력값을 0~1로 압축하고, 전체의 합이 1이 되도록 한다.

###4-2. 확률적 경사 하강법
새로운 데이터가 계속해서 전달되는 경우, 데이터가 늘어날수록 훈련에 필요한 자원이 매우 늘어나고, 이전 데이터를 버린다면 중요한 데이터를 버릴 가능성도 있다.

새로운 데이터에 대해서만 조금씩 더 훈련하는 방법을 점진적 학습이라고 하고, 대표적으로 확률적 경사 하강법이 있다.

* 경사 하강법 :

 가장 가파른 길을 찾아 조금씩 내려와 원하는 지점에 도달하는 것이 목표이다. 이 때 가장 가파른 길은 훈련 세트에서 랜덤하게 하나의 샘플을 선택하여 찾는다.

 > - 에포크 : 샘플을 하나하나 선택하면서 내려오다 훈련세트를 다 사용하게 되었을 때, 샘플을 다시 채워넣고 경사를 내려가는 사이클
 > - 미니배치 경사 하강법 : 샘플을 한 번에 여러 개 선택하는 것
 것
 > - 배치 경사 하강법 : 전체 데이터를 샘플로 사용하는 것

* 손실 함수 : 경사 하강법에서 내려오는 길이 되는 함수이다.
 
 머신러닝 알고리즘이 얼마나 엉터리인지를 측정하는 기준이며, 값이 작을수록 좋다.
 비연속적인 값들을 연속적이게 만들어주어야(미분이 가능해야) 값을 하강시키면서 찾을 수 있다.
 
 > - 로지스틱 손실 함수(이진 크로스엔트로피 손실 함수) : 샘플의 예측값을 양성 클래스와 음성 클래스로 구분하여 각각 -log를 씌워 연속적으로 만들어준다.
 > - 크로스엔트로피 손실 함수 : 다중 분류에서 사용하는 손실 함수
 > - 회귀에서는 평균 제곱 오차를 사용한다.

* 에포크와 과대/과소 적합
 
 적은 에포크 횟수로 훈련한 모델은 과소적합, 많은 에포크 횟수로 훈련한 모델은 과대적합된 모델일 가능성이 높다.
 > - 조기 종료 : 과대적합이 시작하기 전에 훈련을 멈추는 것.


